"""
Data Observability - Snowflake Data Quality Monitoring Platform
==============================================================
A comprehensive Streamlit application for monitoring Snowflake data health.

Features:
- Business KPI Monitoring: Track custom metrics with anomaly detection
- Pipeline Health: Monitor Snowpipe ingestion status and errors
- Data Freshness: Track table update frequency and staleness
- AI Assistant: Natural language queries powered by Snowflake Cortex

Prerequisites:
- Snowflake account with ACCOUNT_USAGE access
- Stored procedures for data refresh (see setup documentation)
- Cortex LLM access for AI features

Version: 2.0
"""

import streamlit as st
import pandas as pd
import numpy as np
from datetime import date, datetime, timedelta
from snowflake.snowpark.context import get_active_session
import altair as alt
import json
import yaml
import os
import time

# --- CONFIG ---
CONFIG_DATABASE = "DATA_QUALITY_MONITORING_DB"
CONFIG_SCHEMA = "OBSERVABILITY"

# Pipe monitoring configuration
CONFIG_TABLE = "PIPE_MONITOR_CONFIG"
CONFIG_TABLE_FQN = f"{CONFIG_DATABASE}.{CONFIG_SCHEMA}.{CONFIG_TABLE}"

# Data freshness tables (populated by REFRESH_DATA_FRESHNESS_TABLES procedure)
TABLE_METRICS_TABLE = "DATA_FRESHNESS_TABLE_METRICS"
TABLE_METRICS_TABLE_FQN = f"{CONFIG_DATABASE}.{CONFIG_SCHEMA}.{TABLE_METRICS_TABLE}"
VOLUME_DAILY_TABLE = "DATA_FRESHNESS_DAILY_VOLUME"
VOLUME_DAILY_TABLE_FQN = f"{CONFIG_DATABASE}.{CONFIG_SCHEMA}.{VOLUME_DAILY_TABLE}"

# Pipe health tables (populated by REFRESH_PIPE_HEALTH_METRICS procedure)
PIPE_HEALTH_METRICS_TABLE = "PIPE_HEALTH_METRICS"
PIPE_HEALTH_METRICS_FQN = f"{CONFIG_DATABASE}.{CONFIG_SCHEMA}.{PIPE_HEALTH_METRICS_TABLE}"
PIPE_HEALTH_DAILY_TABLE = "PIPE_HEALTH_HISTORY"
PIPE_HEALTH_DAILY_FQN = f"{CONFIG_DATABASE}.{CONFIG_SCHEMA}.{PIPE_HEALTH_DAILY_TABLE}"

# Global alert configuration table
ALERT_CONFIG_TABLE = "ALERT_INTEGRATION_CONFIG"
ALERT_CONFIG_FQN = f"{CONFIG_DATABASE}.{CONFIG_SCHEMA}.{ALERT_CONFIG_TABLE}"

# KPI monitoring tables (populated by REFRESH_KPI_METRICS procedure)
KPI_CONFIG_TABLE = "KPI_CONFIG"
KPI_CONFIG_FQN = f"{CONFIG_DATABASE}.{CONFIG_SCHEMA}.{KPI_CONFIG_TABLE}"
KPI_DAILY_TABLE = "KPI_DAILY_METRICS"
KPI_DAILY_FQN = f"{CONFIG_DATABASE}.{CONFIG_SCHEMA}.{KPI_DAILY_TABLE}"
KPI_SUMMARY_TABLE = "KPI_HEALTH_SUMMARY"
KPI_SUMMARY_FQN = f"{CONFIG_DATABASE}.{CONFIG_SCHEMA}.{KPI_SUMMARY_TABLE}"

# YAML configuration file for KPI definitions (optional)
KPI_YAML_FILE = "kpi_definitions.yaml"

# Table monitoring configuration (for Data Freshness page)
TABLE_MONITOR_CONFIG_TABLE = "TABLE_MONITOR_CONFIG"
TABLE_MONITOR_CONFIG_FQN = f"{CONFIG_DATABASE}.{CONFIG_SCHEMA}.{TABLE_MONITOR_CONFIG_TABLE}"
SCHEMA_THRESHOLD_CONFIG_TABLE = "SCHEMA_THRESHOLD_CONFIG"
SCHEMA_THRESHOLD_CONFIG_FQN = f"{CONFIG_DATABASE}.{CONFIG_SCHEMA}.{SCHEMA_THRESHOLD_CONFIG_TABLE}"

# --- FORMATTING ---
def format_metric(value, precision=1):
    """Format large numbers with K/M/B suffix (e.g., 1500 -> 1.5K)."""
    if pd.isna(value) or value is None:
        return "‚Äî"
    value = float(value)
    if abs(value) >= 1e9: return f"{value/1e9:.{precision}f}B"
    if abs(value) >= 1e6: return f"{value/1e6:.{precision}f}M"
    if abs(value) >= 1e3: return f"{value/1e3:.{precision}f}K"
    return f"{value:,.0f}"

def format_thousands(value):
    """Format number with thousands separators."""
    if pd.isna(value) or value is None: return "‚Äî"
    try: return f"{int(value):,}"
    except (ValueError, TypeError): return "‚Äî"

def format_pct(value):
    """Format as percentage."""
    return "‚Äî" if pd.isna(value) or value is None else f"{value:.0f}%"

def format_timestamp_est(value, fmt="%Y-%m-%d %H:%M"):
    """Convert UTC timestamp to US Eastern Time."""
    if pd.isna(value) or value is None: return "‚Äî"
    try:
        ts = pd.to_datetime(value)
        if ts.tzinfo is None: ts = ts.tz_localize('UTC')
        return ts.tz_convert('America/New_York').strftime(fmt) + " EST"
    except: return "‚Äî"

# --- YAML KPI CONFIG ---
def load_kpi_yaml_config() -> list:
    """Load KPI definitions from YAML file if it exists."""
    import sys
    
    # Build list of paths to try
    possible_paths = [
        KPI_YAML_FILE,  # Current directory (relative)
        f"./{KPI_YAML_FILE}",  # Explicit current directory
        os.path.join("/home/udf", KPI_YAML_FILE),  # SiS default directory
    ]
    
    # Add script directory if available
    if '__file__' in dir():
        script_dir = os.path.dirname(os.path.abspath(__file__))
        possible_paths.insert(0, os.path.join(script_dir, KPI_YAML_FILE))
    
    # Add paths from sys.path (where Python looks for modules)
    for p in sys.path:
        if p and os.path.isdir(p):
            possible_paths.append(os.path.join(p, KPI_YAML_FILE))
    
    # Try each path
    for yaml_path in possible_paths:
        try:
            if os.path.exists(yaml_path):
                with open(yaml_path, 'r') as f:
                    content = f.read()
                    if content.strip():
                        config = yaml.safe_load(content)
                        return config.get('kpis', []) if config else []
        except Exception:
            continue
    
    return []

def sync_kpis_from_yaml(session, kpi_list: list) -> dict:
    """Sync KPI definitions from YAML to database. Returns sync status."""
    results = {"added": 0, "skipped": 0, "errors": []}
    for kpi in kpi_list:
        try:
            name = kpi.get('name', '').replace("'", "''")
            if not name: continue
            
            # Check if already exists
            existing = session.sql(f"SELECT 1 FROM {KPI_CONFIG_FQN} WHERE KPI_NAME = '{name}'").collect()
            if existing:
                results["skipped"] += 1
                continue
            
            # Insert new KPI
            desc = (kpi.get('description', '') or '').replace("'", "''")
            sql = (kpi.get('sql', '') or '').replace("'", "''")
            mode = kpi.get('mode', 'THRESHOLD')
            threshold = kpi.get('threshold_pct', 20)
            fixed_val = kpi.get('fixed_value', 'NULL')
            fixed_val = fixed_val if fixed_val != 'NULL' else 'NULL'
            baseline = kpi.get('baseline_days', 30)
            offset = kpi.get('date_offset', 1)
            dashboard_url = (kpi.get('dashboard_url', '') or '').replace("'", "''")
            
            insert_sql = f"""
                INSERT INTO {KPI_CONFIG_FQN} (
                    KPI_NAME, KPI_DESCRIPTION, METRIC_SQL, EXPECTED_MODE,
                    THRESHOLD_PCT, FIXED_EXPECTED_VALUE, BASELINE_DAYS, DATE_OFFSET, 
                    DASHBOARD_URL, IS_MONITORED
                ) VALUES (
                    '{name}', '{desc}', '{sql}', '{mode}',
                    {threshold}, {fixed_val}, {baseline}, {offset}, 
                    '{dashboard_url}', TRUE
                )
            """
            session.sql(insert_sql).collect()
            results["added"] += 1
        except Exception as e:
            results["errors"].append(f"{kpi.get('name', 'Unknown')}: {str(e)[:50]}")
    return results

# --- ALERTING ---
def create_alerts_tracking_table(session, alert_type: str):
    """Create the alerts tracking table for deduplication."""
    if alert_type == "freshness":
        table_name = "DATA_FRESHNESS_ALERTS_SENT"
        ddl = f"""
            CREATE TABLE IF NOT EXISTS {CONFIG_DATABASE}.{CONFIG_SCHEMA}.{table_name} (
                ALERT_ID VARCHAR(100) NOT NULL PRIMARY KEY,
                ALERT_TYPE VARCHAR(20) DEFAULT 'ALL',
                ALERT_DATE DATE NOT NULL,
                NOTIFICATION_TIME TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP(),
                TOTAL_ISSUES_COUNT NUMBER,
                SCHEMAS_AFFECTED VARCHAR(4000),
                TABLES_AFFECTED VARCHAR(4000),
                MESSAGE_SENT VARCHAR(4000)
            )
        """
    elif alert_type == "kpi":
        table_name = "KPI_ALERTS_SENT"
        ddl = f"""
            CREATE TABLE IF NOT EXISTS {CONFIG_DATABASE}.{CONFIG_SCHEMA}.{table_name} (
                ALERT_ID VARCHAR(100) NOT NULL PRIMARY KEY,
                ALERT_TYPE VARCHAR(20) DEFAULT 'ALL',
                ALERT_DATE DATE NOT NULL,
                NOTIFICATION_TIME TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP(),
                TOTAL_ISSUES_COUNT NUMBER,
                KPIS_AFFECTED VARCHAR(4000),
                ANOMALY_TYPES VARCHAR(500),
                MESSAGE_SENT VARCHAR(4000)
            )
        """
    elif alert_type == "pipeline":
        table_name = "PIPE_HEALTH_ALERTS_SENT"
        ddl = f"""
            CREATE TABLE IF NOT EXISTS {CONFIG_DATABASE}.{CONFIG_SCHEMA}.{table_name} (
                ALERT_ID VARCHAR(100) NOT NULL PRIMARY KEY,
                ALERT_TYPE VARCHAR(20) DEFAULT 'ALL',
                ALERT_DATE DATE NOT NULL,
                NOTIFICATION_TIME TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP(),
                TOTAL_ISSUES_COUNT NUMBER,
                ISSUE_TYPES VARCHAR(500),
                PIPES_AFFECTED VARCHAR(4000),
                MESSAGE_SENT VARCHAR(4000)
            )
        """
    else:
        return False
    
    try:
        session.sql(ddl).collect()
        return True
    except:
        return False

def create_data_freshness_alert_procedure(session, critical_integration: str, warning_integration: str):
    """Create the data freshness alert procedure (parameterized version)."""
    procedure_sql = f'''
CREATE OR REPLACE PROCEDURE {CONFIG_DATABASE}.{CONFIG_SCHEMA}.SEND_DATA_FRESHNESS_ALERT(
    P_CRITICAL_INTEGRATION VARCHAR DEFAULT 'data_freshness_slack_critical_int',
    P_WARNING_INTEGRATION VARCHAR DEFAULT 'data_freshness_slack_warning_int'
)
RETURNS VARCHAR
LANGUAGE PYTHON
RUNTIME_VERSION = '3.9'
PACKAGES = ('snowflake-snowpark-python')
HANDLER = 'send_alert'
EXECUTE AS OWNER
AS $$
import snowflake.snowpark as snowpark
from datetime import datetime, date
import json

def send_alert(session, P_CRITICAL_INTEGRATION, P_WARNING_INTEGRATION):
    TARGET_DB = "{CONFIG_DATABASE}"
    TARGET_SCHEMA = "{CONFIG_SCHEMA}"
    CRITICAL_INTEGRATION = P_CRITICAL_INTEGRATION
    WARNING_INTEGRATION = P_WARNING_INTEGRATION
    DEFAULT_WARN_HOURS = 24
    DEFAULT_ALERT_HOURS = 48
    BASELINE_DAYS = 30
    
    # --- 1. Refresh data first ---
    try:
        schema_config = session.sql(f"SELECT DATABASE_NAME, SCHEMA_NAME FROM {{TARGET_DB}}.{{TARGET_SCHEMA}}.SCHEMA_THRESHOLD_CONFIG WHERE IS_MONITORED = TRUE").collect()
        if schema_config:
            db_schemas = {{}}
            for row in schema_config:
                db = row["DATABASE_NAME"]
                schema = row["SCHEMA_NAME"]
                if db not in db_schemas:
                    db_schemas[db] = []
                db_schemas[db].append(schema)
            config_json = json.dumps(db_schemas)
            session.sql(f"CALL {{TARGET_DB}}.{{TARGET_SCHEMA}}.REFRESH_DATA_FRESHNESS_TABLES('{{config_json}}', {{BASELINE_DAYS}})").collect()
    except:
        pass
    
    # --- 2. Check if alerts already sent today ---
    today_str = date.today().isoformat()
    critical_alert_id = f"DATA_FRESHNESS_CRITICAL_{{today_str}}"
    warning_alert_id = f"DATA_FRESHNESS_WARNING_{{today_str}}"
    
    try:
        critical_already_sent = session.sql(f"SELECT COUNT(*) FROM {{TARGET_DB}}.{{TARGET_SCHEMA}}.DATA_FRESHNESS_ALERTS_SENT WHERE ALERT_ID = '{{critical_alert_id}}'").collect()[0][0] > 0
    except:
        critical_already_sent = False
    
    try:
        warning_already_sent = session.sql(f"SELECT COUNT(*) FROM {{TARGET_DB}}.{{TARGET_SCHEMA}}.DATA_FRESHNESS_ALERTS_SENT WHERE ALERT_ID = '{{warning_alert_id}}'").collect()[0][0] > 0
    except:
        warning_already_sent = False
    
    if critical_already_sent and warning_already_sent:
        return f"Both alerts already sent today ({{today_str}}). Skipping."
    
    # --- 3. Query for issues ---
    issues_query = f"""
    WITH TABLE_THRESHOLDS AS (
        SELECT 
            m.DATABASE_NAME, m.SCHEMA_NAME, m.TABLE_NAME, m.FQN,
            DATEDIFF('hour', GREATEST(COALESCE(m.LAST_MODIFIED_DATE, '1900-01-01'::TIMESTAMP), COALESCE(m.LAST_ALTERED, '1900-01-01'::TIMESTAMP)), CURRENT_TIMESTAMP()) AS HOURS_SINCE_UPDATE,
            COALESCE(tc.WARN_THRESHOLD_MINUTES, sc.WARN_THRESHOLD_MINUTES, {{DEFAULT_WARN_HOURS * 60}}) / 60 AS WARN_THRESHOLD_HOURS,
            COALESCE(tc.ALERT_THRESHOLD_MINUTES, sc.ALERT_THRESHOLD_MINUTES, {{DEFAULT_ALERT_HOURS * 60}}) / 60 AS ALERT_THRESHOLD_HOURS
        FROM {{TARGET_DB}}.{{TARGET_SCHEMA}}.DATA_FRESHNESS_TABLE_METRICS m
        LEFT JOIN {{TARGET_DB}}.{{TARGET_SCHEMA}}.TABLE_MONITOR_CONFIG tc ON m.FQN = tc.TABLE_FQN
        LEFT JOIN {{TARGET_DB}}.{{TARGET_SCHEMA}}.SCHEMA_THRESHOLD_CONFIG sc ON m.DATABASE_NAME = sc.DATABASE_NAME AND m.SCHEMA_NAME = sc.SCHEMA_NAME
        WHERE COALESCE(tc.IS_MONITORED, FALSE) = TRUE  -- Only monitor explicitly enabled tables
    )
    SELECT DATABASE_NAME, SCHEMA_NAME, TABLE_NAME, FQN, HOURS_SINCE_UPDATE, WARN_THRESHOLD_HOURS, ALERT_THRESHOLD_HOURS,
        CASE WHEN HOURS_SINCE_UPDATE >= ALERT_THRESHOLD_HOURS THEN 'CRITICAL' WHEN HOURS_SINCE_UPDATE >= WARN_THRESHOLD_HOURS THEN 'WARNING' ELSE NULL END AS ALERT_LEVEL
    FROM TABLE_THRESHOLDS WHERE HOURS_SINCE_UPDATE >= WARN_THRESHOLD_HOURS
    ORDER BY HOURS_SINCE_UPDATE DESC
    """
    
    try:
        issues = session.sql(issues_query).collect()
    except Exception as e:
        return f"ERROR: {{str(e)}}"
    
    if len(issues) == 0:
        return "Data refreshed. No freshness issues detected."
    
    critical_issues = [row for row in issues if row["ALERT_LEVEL"] == "CRITICAL"]
    warning_issues = [row for row in issues if row["ALERT_LEVEL"] == "WARNING"]
    results = []
    
    def build_message(issue_list, alert_type):
        total = len(issue_list)
        schema_counts = {{}}
        for row in issue_list:
            schema = row["SCHEMA_NAME"]
            schema_counts[schema] = schema_counts.get(schema, 0) + 1
        schema_summary = ", ".join([f"{{s}}({{c}})" for s, c in list(schema_counts.items())[:5]])
        msg = f"*{{total}} table(s) {{('CRITICALLY stale' if alert_type == 'CRITICAL' else 'need attention')}}* | *Schemas:* {{schema_summary}}"
        top_items = []
        for row in issue_list[:5]:
            table = row["TABLE_NAME"][:25]
            hours = int(row["HOURS_SINCE_UPDATE"])
            top_items.append(f"{{table}}: {{hours}}h")
        msg += " | *Tables:* " + ", ".join(top_items)
        if total > 5:
            msg += f" (+{{total - 5}} more)"
        return msg, schema_counts
    
    if critical_issues and not critical_already_sent:
        critical_message, critical_schemas = build_message(critical_issues, "CRITICAL")
        try:
            escape_sql = critical_message.replace("'", "''")
            sanitized = session.sql(f"SELECT SNOWFLAKE.NOTIFICATION.SANITIZE_WEBHOOK_CONTENT('{{escape_sql}}')").collect()[0][0]
            session.sql(f"CALL SYSTEM$SEND_SNOWFLAKE_NOTIFICATION(SNOWFLAKE.NOTIFICATION.TEXT_PLAIN('{{sanitized}}'), SNOWFLAKE.NOTIFICATION.INTEGRATION('{{CRITICAL_INTEGRATION}}'))").collect()
            session.sql(f"INSERT INTO {{TARGET_DB}}.{{TARGET_SCHEMA}}.DATA_FRESHNESS_ALERTS_SENT (ALERT_ID, ALERT_TYPE, ALERT_DATE, TOTAL_ISSUES_COUNT, SCHEMAS_AFFECTED, MESSAGE_SENT) VALUES ('{{critical_alert_id}}', 'CRITICAL', CURRENT_DATE(), {{len(critical_issues)}}, '{{', '.join(critical_schemas.keys())[:4000]}}', '{{critical_message[:4000].replace(chr(39), chr(39)+chr(39))}}')").collect()
            results.append(f"CRITICAL: {{len(critical_issues)}} table(s)")
        except Exception as e:
            results.append(f"CRITICAL failed: {{str(e)[:50]}}")
    
    if warning_issues and not warning_already_sent:
        warning_message, warning_schemas = build_message(warning_issues, "WARNING")
        try:
            escape_sql = warning_message.replace("'", "''")
            sanitized = session.sql(f"SELECT SNOWFLAKE.NOTIFICATION.SANITIZE_WEBHOOK_CONTENT('{{escape_sql}}')").collect()[0][0]
            session.sql(f"CALL SYSTEM$SEND_SNOWFLAKE_NOTIFICATION(SNOWFLAKE.NOTIFICATION.TEXT_PLAIN('{{sanitized}}'), SNOWFLAKE.NOTIFICATION.INTEGRATION('{{WARNING_INTEGRATION}}'))").collect()
            session.sql(f"INSERT INTO {{TARGET_DB}}.{{TARGET_SCHEMA}}.DATA_FRESHNESS_ALERTS_SENT (ALERT_ID, ALERT_TYPE, ALERT_DATE, TOTAL_ISSUES_COUNT, SCHEMAS_AFFECTED, MESSAGE_SENT) VALUES ('{{warning_alert_id}}', 'WARNING', CURRENT_DATE(), {{len(warning_issues)}}, '{{', '.join(warning_schemas.keys())[:4000]}}', '{{warning_message[:4000].replace(chr(39), chr(39)+chr(39))}}')").collect()
            results.append(f"WARNING: {{len(warning_issues)}} table(s)")
        except Exception as e:
            results.append(f"WARNING failed: {{str(e)[:50]}}")
    
    return " | ".join(results) if results else "No new alerts to send"
$$
'''
    try:
        session.sql(procedure_sql).collect()
        return True
    except Exception as e:
        return str(e)

def create_kpi_alert_procedure(session, critical_integration: str, warning_integration: str):
    """Create the KPI alert procedure (parameterized version)."""
    procedure_sql = f'''
CREATE OR REPLACE PROCEDURE {CONFIG_DATABASE}.{CONFIG_SCHEMA}.SEND_KPI_ALERT(
    P_CRITICAL_INTEGRATION VARCHAR DEFAULT 'kpi_slack_critical_int',
    P_WARNING_INTEGRATION VARCHAR DEFAULT 'kpi_slack_warning_int'
)
RETURNS VARCHAR
LANGUAGE PYTHON
RUNTIME_VERSION = '3.9'
PACKAGES = ('snowflake-snowpark-python')
HANDLER = 'send_alert'
EXECUTE AS OWNER
AS $$
import snowflake.snowpark as snowpark
from datetime import datetime, date, timedelta

def send_alert(session, P_CRITICAL_INTEGRATION, P_WARNING_INTEGRATION):
    TARGET_DB = "{CONFIG_DATABASE}"
    TARGET_SCHEMA = "{CONFIG_SCHEMA}"
    CRITICAL_INTEGRATION = P_CRITICAL_INTEGRATION
    WARNING_INTEGRATION = P_WARNING_INTEGRATION
    CRITICAL_DEVIATION = 50
    WARNING_DEVIATION = 25
    LOOKBACK_DAYS = 7
    
    # --- 1. Refresh KPI metrics first ---
    try:
        session.sql(f"CALL {{TARGET_DB}}.{{TARGET_SCHEMA}}.REFRESH_KPI_METRICS('{{TARGET_DB}}', '{{TARGET_SCHEMA}}', {{LOOKBACK_DAYS}})").collect()
    except:
        pass
    
    # --- 2. Check if alerts already sent today ---
    today_str = date.today().isoformat()
    critical_alert_id = f"KPI_CRITICAL_{{today_str}}"
    warning_alert_id = f"KPI_WARNING_{{today_str}}"
    
    try:
        critical_already_sent = session.sql(f"SELECT COUNT(*) FROM {{TARGET_DB}}.{{TARGET_SCHEMA}}.KPI_ALERTS_SENT WHERE ALERT_ID = '{{critical_alert_id}}'").collect()[0][0] > 0
    except:
        critical_already_sent = False
    
    try:
        warning_already_sent = session.sql(f"SELECT COUNT(*) FROM {{TARGET_DB}}.{{TARGET_SCHEMA}}.KPI_ALERTS_SENT WHERE ALERT_ID = '{{warning_alert_id}}'").collect()[0][0] > 0
    except:
        warning_already_sent = False
    
    if critical_already_sent and warning_already_sent:
        return f"Both alerts already sent today ({{today_str}}). Skipping."
    
    # --- 3. Check for anomalies ---
    yesterday = (date.today() - timedelta(days=1)).isoformat()
    
    issues_query = f"""
    WITH KPI_BASELINE AS (
        SELECT KPI_NAME, AVG(METRIC_VALUE) AS BASELINE_AVG, STDDEV(METRIC_VALUE) AS BASELINE_STDDEV, COUNT(*) AS BASELINE_DAYS
        FROM {{TARGET_DB}}.{{TARGET_SCHEMA}}.KPI_DAILY_METRICS
        WHERE METRIC_DATE >= DATEADD('day', -30, CURRENT_DATE()) AND METRIC_DATE < CURRENT_DATE() - 1
        GROUP BY KPI_NAME HAVING COUNT(*) >= 7
    ),
    YESTERDAY_METRICS AS (
        SELECT m.KPI_NAME, m.METRIC_VALUE, c.DISPLAY_NAME, c.ALERT_ON_ANOMALY, c.IS_ENABLED
        FROM {{TARGET_DB}}.{{TARGET_SCHEMA}}.KPI_DAILY_METRICS m
        JOIN {{TARGET_DB}}.{{TARGET_SCHEMA}}.KPI_CONFIG c ON m.KPI_NAME = c.KPI_NAME
        WHERE m.METRIC_DATE = '{{yesterday}}' AND c.IS_ENABLED = TRUE AND c.ALERT_ON_ANOMALY = TRUE
    )
    SELECT y.KPI_NAME, y.DISPLAY_NAME, y.METRIC_VALUE AS YESTERDAY_VALUE, b.BASELINE_AVG,
        CASE WHEN b.BASELINE_AVG = 0 THEN CASE WHEN y.METRIC_VALUE = 0 THEN 0 ELSE 100 END
             ELSE ABS((y.METRIC_VALUE - b.BASELINE_AVG) / b.BASELINE_AVG) * 100 END AS DEVIATION_PCT,
        CASE WHEN y.METRIC_VALUE > b.BASELINE_AVG THEN 'HIGHER' WHEN y.METRIC_VALUE < b.BASELINE_AVG THEN 'LOWER' ELSE 'SAME' END AS DIRECTION,
        CASE WHEN ABS((y.METRIC_VALUE - b.BASELINE_AVG) / NULLIF(b.BASELINE_AVG, 0)) * 100 >= {{CRITICAL_DEVIATION}} THEN 'CRITICAL'
             WHEN ABS((y.METRIC_VALUE - b.BASELINE_AVG) / NULLIF(b.BASELINE_AVG, 0)) * 100 >= {{WARNING_DEVIATION}} THEN 'WARNING'
             WHEN y.METRIC_VALUE > b.BASELINE_AVG + (3 * b.BASELINE_STDDEV) THEN 'CRITICAL'
             WHEN y.METRIC_VALUE < b.BASELINE_AVG - (3 * b.BASELINE_STDDEV) THEN 'CRITICAL'
             WHEN y.METRIC_VALUE > b.BASELINE_AVG + (2 * b.BASELINE_STDDEV) THEN 'WARNING'
             WHEN y.METRIC_VALUE < b.BASELINE_AVG - (2 * b.BASELINE_STDDEV) THEN 'WARNING' ELSE NULL END AS ALERT_LEVEL
    FROM YESTERDAY_METRICS y JOIN KPI_BASELINE b ON y.KPI_NAME = b.KPI_NAME
    WHERE ABS((y.METRIC_VALUE - b.BASELINE_AVG) / NULLIF(b.BASELINE_AVG, 0)) * 100 >= {{WARNING_DEVIATION}}
       OR y.METRIC_VALUE > b.BASELINE_AVG + (2 * b.BASELINE_STDDEV) OR y.METRIC_VALUE < b.BASELINE_AVG - (2 * b.BASELINE_STDDEV)
    ORDER BY DEVIATION_PCT DESC
    """
    
    try:
        issues = session.sql(issues_query).collect()
    except Exception as e:
        return f"ERROR: {{str(e)}}"
    
    if len(issues) == 0:
        return f"No KPI anomalies detected for {{yesterday}}."
    
    critical_issues = [row for row in issues if row["ALERT_LEVEL"] == "CRITICAL"]
    warning_issues = [row for row in issues if row["ALERT_LEVEL"] == "WARNING"]
    results = []
    
    def build_message(issue_list, alert_type):
        total = len(issue_list)
        msg = f"*{{total}} {{('CRITICAL KPI anomaly(s)' if alert_type == 'CRITICAL' else 'KPI warning(s)')}} detected*"
        top_items = []
        for row in issue_list[:5]:
            kpi = (row["DISPLAY_NAME"] or row["KPI_NAME"])[:30]
            direction = "üìà" if row["DIRECTION"] == "HIGHER" else "üìâ" if row["DIRECTION"] == "LOWER" else "‚û°Ô∏è"
            dev = row["DEVIATION_PCT"]
            top_items.append(f"{{direction}} {{kpi}}: {{dev:.0f}}%")
        msg += " | " + " | ".join(top_items)
        if total > 5:
            msg += f" (+{{total - 5}} more)"
        return msg
    
    if critical_issues and not critical_already_sent:
        critical_message = build_message(critical_issues, "CRITICAL")
        try:
            escape_sql = critical_message.replace("'", "''")
            sanitized = session.sql(f"SELECT SNOWFLAKE.NOTIFICATION.SANITIZE_WEBHOOK_CONTENT('{{escape_sql}}')").collect()[0][0]
            session.sql(f"CALL SYSTEM$SEND_SNOWFLAKE_NOTIFICATION(SNOWFLAKE.NOTIFICATION.TEXT_PLAIN('{{sanitized}}'), SNOWFLAKE.NOTIFICATION.INTEGRATION('{{CRITICAL_INTEGRATION}}'))").collect()
            kpis = ", ".join([row["KPI_NAME"] for row in critical_issues[:20]])
            session.sql(f"INSERT INTO {{TARGET_DB}}.{{TARGET_SCHEMA}}.KPI_ALERTS_SENT (ALERT_ID, ALERT_TYPE, ALERT_DATE, TOTAL_ISSUES_COUNT, KPIS_AFFECTED, MESSAGE_SENT) VALUES ('{{critical_alert_id}}', 'CRITICAL', CURRENT_DATE(), {{len(critical_issues)}}, '{{kpis[:4000]}}', '{{critical_message[:4000].replace(chr(39), chr(39)+chr(39))}}')").collect()
            results.append(f"CRITICAL: {{len(critical_issues)}} KPI(s)")
        except Exception as e:
            results.append(f"CRITICAL failed: {{str(e)[:50]}}")
    
    if warning_issues and not warning_already_sent:
        warning_message = build_message(warning_issues, "WARNING")
        try:
            escape_sql = warning_message.replace("'", "''")
            sanitized = session.sql(f"SELECT SNOWFLAKE.NOTIFICATION.SANITIZE_WEBHOOK_CONTENT('{{escape_sql}}')").collect()[0][0]
            session.sql(f"CALL SYSTEM$SEND_SNOWFLAKE_NOTIFICATION(SNOWFLAKE.NOTIFICATION.TEXT_PLAIN('{{sanitized}}'), SNOWFLAKE.NOTIFICATION.INTEGRATION('{{WARNING_INTEGRATION}}'))").collect()
            kpis = ", ".join([row["KPI_NAME"] for row in warning_issues[:20]])
            session.sql(f"INSERT INTO {{TARGET_DB}}.{{TARGET_SCHEMA}}.KPI_ALERTS_SENT (ALERT_ID, ALERT_TYPE, ALERT_DATE, TOTAL_ISSUES_COUNT, KPIS_AFFECTED, MESSAGE_SENT) VALUES ('{{warning_alert_id}}', 'WARNING', CURRENT_DATE(), {{len(warning_issues)}}, '{{kpis[:4000]}}', '{{warning_message[:4000].replace(chr(39), chr(39)+chr(39))}}')").collect()
            results.append(f"WARNING: {{len(warning_issues)}} KPI(s)")
        except Exception as e:
            results.append(f"WARNING failed: {{str(e)[:50]}}")
    
    return " | ".join(results) if results else "No new alerts"
$$
'''
    try:
        session.sql(procedure_sql).collect()
        return True
    except Exception as e:
        return str(e)

def create_pipe_health_alert_procedure(session, critical_integration: str, warning_integration: str):
    """Create the pipe health alert procedure (parameterized version)."""
    procedure_sql = f'''
CREATE OR REPLACE PROCEDURE {CONFIG_DATABASE}.{CONFIG_SCHEMA}.SEND_PIPE_HEALTH_ALERT(
    P_CRITICAL_INTEGRATION VARCHAR DEFAULT 'pipe_health_slack_critical_int',
    P_WARNING_INTEGRATION VARCHAR DEFAULT 'pipe_health_slack_warning_int'
)
RETURNS VARCHAR
LANGUAGE PYTHON
RUNTIME_VERSION = '3.9'
PACKAGES = ('snowflake-snowpark-python')
HANDLER = 'send_alert'
EXECUTE AS OWNER
AS $$
import snowflake.snowpark as snowpark
from datetime import datetime, date

def send_alert(session, P_CRITICAL_INTEGRATION, P_WARNING_INTEGRATION):
    TARGET_DB = "{CONFIG_DATABASE}"
    TARGET_SCHEMA = "{CONFIG_SCHEMA}"
    CRITICAL_INTEGRATION = P_CRITICAL_INTEGRATION
    WARNING_INTEGRATION = P_WARNING_INTEGRATION
    
    today_str = date.today().isoformat()
    critical_alert_id = f"PIPE_HEALTH_CRITICAL_{{today_str}}"
    warning_alert_id = f"PIPE_HEALTH_WARNING_{{today_str}}"
    
    try:
        critical_already_sent = session.sql(f"SELECT COUNT(*) FROM {{TARGET_DB}}.{{TARGET_SCHEMA}}.PIPE_HEALTH_ALERTS_SENT WHERE ALERT_ID = '{{critical_alert_id}}'").collect()[0][0] > 0
    except:
        critical_already_sent = False
    
    try:
        warning_already_sent = session.sql(f"SELECT COUNT(*) FROM {{TARGET_DB}}.{{TARGET_SCHEMA}}.PIPE_HEALTH_ALERTS_SENT WHERE ALERT_ID = '{{warning_alert_id}}'").collect()[0][0] > 0
    except:
        warning_already_sent = False
    
    if critical_already_sent and warning_already_sent:
        return f"Both alerts already sent today ({{today_str}}). Skipping."
    
    issues_query = f"""
    WITH PIPE_ALERTS AS (
        SELECT H.PIPE_NAME, H.DATABASE_NAME, H.SCHEMA_NAME, H.YESTERDAY_DATE, COALESCE(H.YESTERDAY_FILES, 0) AS YESTERDAY_FILES,
            COALESCE(H.YESTERDAY_ERRORS, 0) AS YESTERDAY_ERRORS, H.EXPECTED_FILES, COALESCE(H.TODAY_FILES, 0) AS TODAY_FILES,
            C.RUNS_DAILY, C.ALERT_ON_MISSING, C.ALERT_ON_VOLUME_DROP, C.VOLUME_THRESHOLD_PCT,
            CASE WHEN H.YESTERDAY_DATE IS NULL THEN 'NO_DATA'
                 WHEN C.RUNS_DAILY = TRUE AND H.YESTERDAY_DATE < CURRENT_DATE() - 1 AND C.ALERT_ON_MISSING = TRUE THEN 'STALE_DATA'
                 WHEN C.RUNS_DAILY = TRUE AND C.ALERT_ON_MISSING = TRUE AND COALESCE(H.TODAY_FILES, 0) = 0 AND H.EXPECTED_FILES > 0 THEN 'MISSING_TODAY'
                 WHEN C.ALERT_ON_VOLUME_DROP = TRUE AND H.EXPECTED_FILES > 0 AND COALESCE(H.YESTERDAY_FILES, 0) <= (C.VOLUME_THRESHOLD_PCT / 100.0) * H.EXPECTED_FILES THEN 'LOW_FILES'
                 WHEN COALESCE(H.YESTERDAY_ERRORS, 0) > 0 THEN 'ERRORS' ELSE NULL END AS ISSUE_TYPE,
            CASE WHEN H.YESTERDAY_DATE IS NULL THEN 5 WHEN H.YESTERDAY_DATE < CURRENT_DATE() - 1 THEN 5
                 WHEN C.RUNS_DAILY = TRUE AND COALESCE(H.TODAY_FILES, 0) = 0 AND H.EXPECTED_FILES > 0 THEN 5
                 WHEN COALESCE(H.YESTERDAY_ERRORS, 0) > 0 THEN 4 ELSE 2 END AS SEVERITY
        FROM {{TARGET_DB}}.{{TARGET_SCHEMA}}.PIPE_HEALTH_METRICS H
        JOIN {{TARGET_DB}}.{{TARGET_SCHEMA}}.PIPE_MONITOR_CONFIG C ON H.PIPE_NAME = C.PIPE_NAME
        WHERE C.IS_MONITORED = TRUE
    )
    SELECT PIPE_NAME, DATABASE_NAME, SCHEMA_NAME, ISSUE_TYPE, SEVERITY, YESTERDAY_DATE, YESTERDAY_FILES, EXPECTED_FILES, YESTERDAY_ERRORS, TODAY_FILES
    FROM PIPE_ALERTS WHERE ISSUE_TYPE IS NOT NULL ORDER BY SEVERITY DESC, ISSUE_TYPE, PIPE_NAME
    """
    
    try:
        issues = session.sql(issues_query).collect()
    except Exception as e:
        return f"ERROR: {{str(e)}}"
    
    if len(issues) == 0:
        return "No pipe health issues detected."
    
    critical_issues = [row for row in issues if row["SEVERITY"] == 5]
    warning_issues = [row for row in issues if row["SEVERITY"] < 5]
    results = []
    
    def build_message(issue_list, alert_type):
        total = len(issue_list)
        issue_counts = {{}}
        for row in issue_list:
            itype = row["ISSUE_TYPE"]
            issue_counts[itype] = issue_counts.get(itype, 0) + 1
        issue_summary = ", ".join([f"{{v}} {{k}}" for k, v in issue_counts.items()])
        msg = f"*{{total}} {{('CRITICAL issue(s)' if alert_type == 'CRITICAL' else 'warning(s)')}}:* {{issue_summary}}"
        top_items = []
        for row in issue_list[:5]:
            pipe_short = row["PIPE_NAME"].split(".")[-1][:20]
            itype = row["ISSUE_TYPE"]
            top_items.append(f"{{pipe_short}}:{{itype}}")
        msg += " | *Pipes:* " + ", ".join(top_items)
        if total > 5:
            msg += f" (+{{total - 5}} more)"
        return msg, issue_counts
    
    if critical_issues and not critical_already_sent:
        critical_message, critical_counts = build_message(critical_issues, "CRITICAL")
        try:
            escape_sql = critical_message.replace("'", "''")
            sanitized = session.sql(f"SELECT SNOWFLAKE.NOTIFICATION.SANITIZE_WEBHOOK_CONTENT('{{escape_sql}}')").collect()[0][0]
            session.sql(f"CALL SYSTEM$SEND_SNOWFLAKE_NOTIFICATION(SNOWFLAKE.NOTIFICATION.TEXT_PLAIN('{{sanitized}}'), SNOWFLAKE.NOTIFICATION.INTEGRATION('{{CRITICAL_INTEGRATION}}'))").collect()
            pipes = ", ".join([row["PIPE_NAME"] for row in critical_issues[:20]])
            session.sql(f"INSERT INTO {{TARGET_DB}}.{{TARGET_SCHEMA}}.PIPE_HEALTH_ALERTS_SENT (ALERT_ID, ALERT_TYPE, ALERT_DATE, TOTAL_ISSUES_COUNT, ISSUE_TYPES, PIPES_AFFECTED, MESSAGE_SENT) VALUES ('{{critical_alert_id}}', 'CRITICAL', CURRENT_DATE(), {{len(critical_issues)}}, '{{', '.join(critical_counts.keys())[:500]}}', '{{pipes[:4000]}}', '{{critical_message[:4000].replace(chr(39), chr(39)+chr(39))}}')").collect()
            results.append(f"CRITICAL: {{len(critical_issues)}} pipe(s)")
        except Exception as e:
            results.append(f"CRITICAL failed: {{str(e)[:50]}}")
    
    if warning_issues and not warning_already_sent:
        warning_message, warning_counts = build_message(warning_issues, "WARNING")
        try:
            escape_sql = warning_message.replace("'", "''")
            sanitized = session.sql(f"SELECT SNOWFLAKE.NOTIFICATION.SANITIZE_WEBHOOK_CONTENT('{{escape_sql}}')").collect()[0][0]
            session.sql(f"CALL SYSTEM$SEND_SNOWFLAKE_NOTIFICATION(SNOWFLAKE.NOTIFICATION.TEXT_PLAIN('{{sanitized}}'), SNOWFLAKE.NOTIFICATION.INTEGRATION('{{WARNING_INTEGRATION}}'))").collect()
            pipes = ", ".join([row["PIPE_NAME"] for row in warning_issues[:20]])
            session.sql(f"INSERT INTO {{TARGET_DB}}.{{TARGET_SCHEMA}}.PIPE_HEALTH_ALERTS_SENT (ALERT_ID, ALERT_TYPE, ALERT_DATE, TOTAL_ISSUES_COUNT, ISSUE_TYPES, PIPES_AFFECTED, MESSAGE_SENT) VALUES ('{{warning_alert_id}}', 'WARNING', CURRENT_DATE(), {{len(warning_issues)}}, '{{', '.join(warning_counts.keys())[:500]}}', '{{pipes[:4000]}}', '{{warning_message[:4000].replace(chr(39), chr(39)+chr(39))}}')").collect()
            results.append(f"WARNING: {{len(warning_issues)}} pipe(s)")
        except Exception as e:
            results.append(f"WARNING failed: {{str(e)[:50]}}")
    
    return " | ".join(results) if results else "No new alerts"
$$
'''
    try:
        session.sql(procedure_sql).collect()
        return True
    except Exception as e:
        return str(e)

# --- PAGE CONFIG ---
st.set_page_config(
    page_title="Data Observability",
    page_icon="üìä",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- GLOBAL STYLES ---
st.markdown("""
<style>
    /* =========================================================================
       DESIGN SYSTEM - Premium Dark Theme
       ========================================================================= */
    :root {
        /* Brand Colors */
        --primary: #6366F1;
        --primary-dark: #4F46E5;
        --primary-light: #818CF8;
        --primary-glow: rgba(99, 102, 241, 0.3);
        
        /* Semantic Colors */
        --success: #10B981;
        --success-light: rgba(16, 185, 129, 0.15);
        --warning: #F59E0B;
        --warning-light: rgba(245, 158, 11, 0.15);
        --danger: #EF4444;
        --danger-light: rgba(239, 68, 68, 0.15);
        --info: #3B82F6;
        --info-light: rgba(59, 130, 246, 0.15);
        
        /* Neutral Palette */
        --slate-50: #F8FAFC;
        --slate-100: #F1F5F9;
        --slate-200: #E2E8F0;
        --slate-300: #CBD5E1;
        --slate-400: #94A3B8;
        --slate-500: #64748B;
        --slate-600: #475569;
        --slate-700: #334155;
        --slate-800: #1E293B;
        --slate-900: #0F172A;
        
        /* Gradients */
        --gradient-hero: linear-gradient(135deg, #0F172A 0%, #1E293B 50%, #334155 100%);
        --gradient-primary: linear-gradient(135deg, #6366F1 0%, #8B5CF6 100%);
        --gradient-success: linear-gradient(135deg, #10B981 0%, #34D399 100%);
        --gradient-card: linear-gradient(145deg, #FFFFFF 0%, #F8FAFC 100%);
        
        /* Shadows */
        --shadow-sm: 0 1px 2px rgba(0, 0, 0, 0.05);
        --shadow-md: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06);
        --shadow-lg: 0 10px 15px -3px rgba(0, 0, 0, 0.1), 0 4px 6px -2px rgba(0, 0, 0, 0.05);
        --shadow-xl: 0 20px 25px -5px rgba(0, 0, 0, 0.1), 0 10px 10px -5px rgba(0, 0, 0, 0.04);
        --shadow-glow: 0 0 20px var(--primary-glow);
        
        /* Border Radius */
        --radius-sm: 6px;
        --radius-md: 10px;
        --radius-lg: 16px;
        --radius-xl: 24px;
        
        /* Transitions */
        --transition-fast: 150ms cubic-bezier(0.4, 0, 0.2, 1);
        --transition-base: 200ms cubic-bezier(0.4, 0, 0.2, 1);
        --transition-slow: 300ms cubic-bezier(0.4, 0, 0.2, 1);
    }
    
    /* =========================================================================
       GLOBAL LAYOUT
       ========================================================================= */
    .main { background: var(--slate-50); }
    .main .block-container { 
        padding: 1.5rem 2rem 3rem 2rem; 
        max-width: 1400px;
    }
    
    /* Hide Streamlit branding */
    #MainMenu, footer, header { visibility: hidden; }
    
    /* =========================================================================
       PAGE HEADER - Hero Style
       ========================================================================= */
    .page-header {
        background: var(--gradient-hero);
        padding: 2rem 2.5rem;
        border-radius: var(--radius-xl);
        margin-bottom: 2rem;
        color: white;
        box-shadow: var(--shadow-xl);
        position: relative;
        overflow: hidden;
    }
    .page-header::before {
        content: '';
        position: absolute;
        top: 0;
        right: 0;
        width: 300px;
        height: 300px;
        background: radial-gradient(circle, var(--primary-glow) 0%, transparent 70%);
        transform: translate(30%, -30%);
    }
    .page-header h1 { 
        margin: 0; 
        font-size: 2rem; 
        font-weight: 700; 
        color: white;
        position: relative;
        z-index: 1;
    }
    .page-header p { 
        margin: 0.5rem 0 0 0; 
        opacity: 0.85; 
        font-size: 1rem;
        position: relative;
        z-index: 1;
    }
    
    /* =========================================================================
       SECTION HEADERS
       ========================================================================= */
    .section-header {
        display: flex;
        align-items: center;
        gap: 0.75rem;
        margin: 2.5rem 0 1.5rem 0;
        padding-bottom: 0.75rem;
        border-bottom: 2px solid var(--slate-200);
    }
    .section-header h3 { 
        margin: 0; 
        font-size: 1.25rem; 
        font-weight: 600; 
        color: var(--slate-700);
        letter-spacing: -0.02em;
    }
    
    /* =========================================================================
       STATUS BADGES
       ========================================================================= */
    .status-badge { 
        display: inline-flex; 
        align-items: center;
        gap: 0.35rem;
        padding: 0.4rem 0.85rem; 
        border-radius: 9999px; 
        font-size: 0.8rem; 
        font-weight: 600;
        letter-spacing: 0.01em;
    }
    .status-ok { background: var(--success-light); color: #065F46; }
    .status-warning { background: var(--warning-light); color: #92400E; }
    .status-error { background: var(--danger-light); color: #991B1B; }
    .status-info { background: var(--info-light); color: #1E40AF; }
    
    /* =========================================================================
       ALERT BOXES
       ========================================================================= */
    .alert-box { 
        padding: 1rem 1.25rem; 
        border-radius: var(--radius-md); 
        margin: 1rem 0; 
        display: flex; 
        align-items: flex-start;
        gap: 0.75rem;
        backdrop-filter: blur(8px);
    }
    .alert-success { background: var(--success-light); border-left: 4px solid var(--success); }
    .alert-warning { background: var(--warning-light); border-left: 4px solid var(--warning); }
    .alert-error { background: var(--danger-light); border-left: 4px solid var(--danger); }
    .alert-info { background: var(--info-light); border-left: 4px solid var(--info); }
    
    /* =========================================================================
       STREAMLIT COMPONENT OVERRIDES
       ========================================================================= */
    
    /* Buttons */
    .stButton > button {
        border-radius: var(--radius-md) !important;
        font-weight: 500 !important;
        transition: all var(--transition-base) !important;
        border: 1px solid var(--slate-200) !important;
    }
    .stButton > button:hover {
        transform: translateY(-1px) !important;
        box-shadow: var(--shadow-md) !important;
    }
    .stButton > button[kind="primary"] {
        background: var(--gradient-primary) !important;
        border: none !important;
        color: white !important;
    }
    .stButton > button[kind="primary"]:hover {
        box-shadow: var(--shadow-glow), var(--shadow-lg) !important;
    }
    
    /* Tabs */
    .stTabs [data-baseweb="tab-list"] {
        gap: 0.25rem;
        background: var(--slate-100);
        padding: 0.35rem;
        border-radius: var(--radius-lg);
        border: 1px solid var(--slate-200);
    }
    .stTabs [data-baseweb="tab"] {
        border-radius: var(--radius-md);
        padding: 0.65rem 1.25rem;
        font-weight: 500;
        font-size: 0.9rem;
        transition: all var(--transition-fast);
        color: var(--slate-600);
    }
    .stTabs [data-baseweb="tab"]:hover {
        background: rgba(99, 102, 241, 0.08);
        color: var(--primary);
    }
    .stTabs [aria-selected="true"] {
        background: white !important;
        color: var(--primary) !important;
        box-shadow: var(--shadow-sm) !important;
    }
    
    /* Metrics */
    [data-testid="stMetricValue"] { 
        font-size: 2.25rem !important; 
        font-weight: 700 !important;
        letter-spacing: -0.02em !important;
        color: var(--slate-800) !important;
    }
    [data-testid="stMetricLabel"] { 
        font-weight: 500 !important; 
        text-transform: uppercase !important; 
        font-size: 0.75rem !important;
        letter-spacing: 0.05em !important;
        color: var(--slate-500) !important;
    }
    [data-testid="stMetricDelta"] {
        font-weight: 600 !important;
    }
    
    /* Data Tables */
    .stDataFrame { 
        border-radius: var(--radius-lg) !important;
        border: 1px solid var(--slate-200) !important;
        overflow: hidden !important;
    }
    .stDataFrame [data-testid="stDataFrameResizable"] {
        border-radius: var(--radius-lg) !important;
    }
    
    /* Expanders */
    .streamlit-expanderHeader {
        font-weight: 500 !important;
        font-size: 0.95rem !important;
        color: var(--slate-700) !important;
        background: var(--slate-50) !important;
        border-radius: var(--radius-md) !important;
    }
    
    /* Selectbox & Inputs */
    .stSelectbox > div > div,
    .stMultiSelect > div > div,
    .stTextInput > div > div > input,
    .stTextArea > div > div > textarea {
        border-radius: var(--radius-md) !important;
        border-color: var(--slate-200) !important;
        transition: all var(--transition-fast) !important;
    }
    .stSelectbox > div > div:focus-within,
    .stMultiSelect > div > div:focus-within,
    .stTextInput > div > div > input:focus,
    .stTextArea > div > div > textarea:focus {
        border-color: var(--primary) !important;
        box-shadow: 0 0 0 3px var(--primary-glow) !important;
    }
    
    /* =========================================================================
       ANIMATIONS
       ========================================================================= */
    @keyframes fadeIn {
        from { opacity: 0; transform: translateY(10px); }
        to { opacity: 1; transform: translateY(0); }
    }
    @keyframes slideIn {
        from { opacity: 0; transform: translateX(-10px); }
        to { opacity: 1; transform: translateX(0); }
    }
    @keyframes pulse {
        0%, 100% { opacity: 1; }
        50% { opacity: 0.7; }
    }
    
    .animate-in { animation: fadeIn 0.4s ease-out; }
    .animate-slide { animation: slideIn 0.3s ease-out; }
    .animate-pulse { animation: pulse 2s infinite; }
    
    /* =========================================================================
       CUSTOM SCROLLBAR
       ========================================================================= */
    ::-webkit-scrollbar {
        width: 8px;
        height: 8px;
    }
    ::-webkit-scrollbar-track {
        background: var(--slate-100);
        border-radius: 4px;
    }
    ::-webkit-scrollbar-thumb {
        background: var(--slate-300);
        border-radius: 4px;
    }
    ::-webkit-scrollbar-thumb:hover {
        background: var(--slate-400);
    }
</style>
""", unsafe_allow_html=True)

# --- UI COMPONENTS ---
def render_page_header(icon: str, title: str, subtitle: str):
    """Render page header with gradient background."""
    st.markdown(f'<div class="page-header animate-in"><h1>{icon} {title}</h1><p>{subtitle}</p></div>', unsafe_allow_html=True)

def render_section_header(icon: str, title: str):
    """Render section divider with icon."""
    st.markdown(f'<div class="section-header"><span style="font-size:1.5rem;">{icon}</span><h3>{title}</h3></div>', unsafe_allow_html=True)

def render_status_badge(status: str, text: str) -> str:
    """Return HTML for status badge. Status: ok, warning, error, info."""
    return f'<span class="status-badge status-{status}">{text}</span>'

def render_alert(alert_type: str, message: str, icon: str = "‚ÑπÔ∏è"):
    """Render styled alert box. Types: success, warning, error, info."""
    st.markdown(f'<div class="alert-box alert-{alert_type} animate-in"><span style="font-size:1.25rem;">{icon}</span><div>{message}</div></div>', unsafe_allow_html=True)

# --- DATABASE ---
session = get_active_session()

@st.cache_data(ttl=300)
def run_query(sql: str) -> pd.DataFrame:
    """Execute SQL and return DataFrame (cached 5 min)."""
    return session.sql(sql).to_pandas()

def run_ddl(sql: str):
    """Execute DDL/DML statement."""
    session.sql(sql).collect()

@st.cache_data(ttl=600)
def list_db_schema_options() -> pd.DataFrame:
    """Get available databases and schemas from account metadata."""
    return session.sql("""
        SELECT DISTINCT TABLE_CATALOG AS DATABASE_NAME, TABLE_SCHEMA AS SCHEMA_NAME
        FROM SNOWFLAKE.ACCOUNT_USAGE.TABLE_STORAGE_METRICS ORDER BY 1, 2
    """).to_pandas()

@st.cache_data(ttl=600)
def list_table_prefixes(databases: list, schemas: list) -> list:
    """Extract unique table prefixes from selected databases/schemas."""
    if not databases or not schemas: return []
    db_list = ",".join([f"'{d.replace(chr(39), chr(39)+chr(39))}'" for d in databases])
    schema_list = ",".join([f"'{s.replace(chr(39), chr(39)+chr(39))}'" for s in schemas])
    df = session.sql(f"""
        SELECT DISTINCT TABLE_NAME FROM SNOWFLAKE.ACCOUNT_USAGE.TABLES
        WHERE TABLE_CATALOG IN ({db_list}) AND TABLE_SCHEMA IN ({schema_list}) AND DELETED IS NULL
    """).to_pandas()
    if df.empty: return []
    df["PREFIX"] = df["TABLE_NAME"].astype(str).str.extract(r'^([A-Za-z0-9]+)', expand=False).fillna(df["TABLE_NAME"]).str.upper()
    return sorted(df["PREFIX"].dropna().unique().tolist())

@st.cache_data(ttl=600)
def list_all_pipes() -> pd.DataFrame:
    """Fetch all Snowpipes from account."""
    return session.sql("""
        SELECT DISTINCT PIPE_CATALOG AS DATABASE_NAME, PIPE_SCHEMA AS SCHEMA_NAME, PIPE_NAME,
            PIPE_CATALOG || '.' || PIPE_SCHEMA || '.' || PIPE_NAME AS FULL_PIPE_NAME
        FROM SNOWFLAKE.ACCOUNT_USAGE.PIPES WHERE DELETED IS NULL ORDER BY 1, 2, 3
    """).to_pandas()

# --- PIPE MONITORING CONFIG ---
def ensure_config_table_exists():
    """Create pipe configuration table if not exists."""
    run_ddl(f"""
        CREATE TABLE IF NOT EXISTS {CONFIG_TABLE_FQN} (
            PIPE_NAME STRING NOT NULL PRIMARY KEY, DATABASE_NAME STRING, SCHEMA_NAME STRING,
            IS_MONITORED BOOLEAN DEFAULT TRUE, RUNS_DAILY BOOLEAN DEFAULT TRUE,
            ALERT_ON_MISSING BOOLEAN DEFAULT TRUE, ALERT_ON_VOLUME_DROP BOOLEAN DEFAULT TRUE,
            VOLUME_THRESHOLD_PCT NUMBER DEFAULT 50, NOTES STRING,
            CREATED_AT TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP(),
            UPDATED_AT TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP())
    """)
    try: run_ddl(f"ALTER TABLE {CONFIG_TABLE_FQN} ADD COLUMN IF NOT EXISTS RUNS_DAILY BOOLEAN DEFAULT TRUE")
    except: pass

def get_configured_pipes() -> pd.DataFrame:
    """Get all configured pipes."""
    ensure_config_table_exists()
    return run_query(f"SELECT * FROM {CONFIG_TABLE_FQN} ORDER BY PIPE_NAME")

def add_pipe_to_config(pipe_name: str, database_name: str, schema_name: str, notes: str = ""):
    """Add pipe to monitoring configuration."""
    safe = lambda s: (s or "").replace("'", "''")
    run_ddl(f"""
        INSERT INTO {CONFIG_TABLE_FQN} (PIPE_NAME, DATABASE_NAME, SCHEMA_NAME, NOTES, UPDATED_AT)
        SELECT '{safe(pipe_name)}', '{safe(database_name)}', '{safe(schema_name)}', '{safe(notes)}', CURRENT_TIMESTAMP()
        WHERE NOT EXISTS (SELECT 1 FROM {CONFIG_TABLE_FQN} WHERE PIPE_NAME = '{safe(pipe_name)}')
    """)

def remove_pipe_from_config(pipe_name: str):
    """Remove pipe from monitoring configuration."""
    run_ddl(f"DELETE FROM {CONFIG_TABLE_FQN} WHERE PIPE_NAME = '{pipe_name.replace(chr(39), chr(39)+chr(39))}'")

def update_pipe_config(pipe_name: str, is_monitored: bool, runs_daily: bool, alert_missing: bool, alert_volume: bool, threshold: int, notes: str):
    """Update pipe monitoring settings."""
    safe_pipe, safe_notes = pipe_name.replace("'", "''"), (notes or "").replace("'", "''")
    run_ddl(f"""
        UPDATE {CONFIG_TABLE_FQN} SET IS_MONITORED={is_monitored}, RUNS_DAILY={runs_daily},
        ALERT_ON_MISSING={alert_missing}, ALERT_ON_VOLUME_DROP={alert_volume},
        VOLUME_THRESHOLD_PCT={threshold}, NOTES='{safe_notes}', UPDATED_AT=CURRENT_TIMESTAMP()
        WHERE PIPE_NAME='{safe_pipe}'
    """)

# --- TABLE MONITORING CONFIG ---
def ensure_table_monitor_config_exists():
    """Create table monitoring configuration tables if not exist."""
    # Table-level config with critical flag
    run_ddl(f"""
        CREATE TABLE IF NOT EXISTS {TABLE_MONITOR_CONFIG_FQN} (
            TABLE_FQN STRING NOT NULL PRIMARY KEY,
            DATABASE_NAME STRING,
            SCHEMA_NAME STRING,
            TABLE_NAME STRING,
            IS_MONITORED BOOLEAN DEFAULT FALSE,      -- New tables NOT monitored by default
            WARN_THRESHOLD_MINUTES NUMBER,           -- Table-specific warn threshold (overrides schema)
            ALERT_THRESHOLD_MINUTES NUMBER,          -- Table-specific alert threshold (overrides schema)
            NOTES STRING,
            CREATED_AT TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP(),
            UPDATED_AT TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()
        )
    """)
    # Add new columns if upgrading from old schema
    try:
        run_ddl(f"ALTER TABLE {TABLE_MONITOR_CONFIG_FQN} ADD COLUMN IF NOT EXISTS WARN_THRESHOLD_MINUTES NUMBER")
        run_ddl(f"ALTER TABLE {TABLE_MONITOR_CONFIG_FQN} ADD COLUMN IF NOT EXISTS ALERT_THRESHOLD_MINUTES NUMBER")
    except:
        pass  # Columns already exist
    # Schema-level threshold config (Warn + Alert thresholds in MINUTES)
    run_ddl(f"""
        CREATE TABLE IF NOT EXISTS {SCHEMA_THRESHOLD_CONFIG_FQN} (
            SCHEMA_KEY STRING NOT NULL PRIMARY KEY,
            DATABASE_NAME STRING,
            SCHEMA_NAME STRING,
            WARN_THRESHOLD_MINUTES NUMBER DEFAULT 1440,    -- 24 hours = WARNING
            ALERT_THRESHOLD_MINUTES NUMBER DEFAULT 2880,   -- 48 hours = ALERT
            DEFAULT_VOLUME_DROP_PCT NUMBER DEFAULT 50,
            DEFAULT_VOLUME_SPIKE_PCT NUMBER DEFAULT 200,
            IS_MONITORED BOOLEAN DEFAULT TRUE,
            NOTES STRING,
            CREATED_AT TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP(),
            UPDATED_AT TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()
        )
    """)
    # Migration is no longer needed - columns are in CREATE TABLE
    pass

def get_table_monitor_config() -> pd.DataFrame:
    """Get all table monitoring configurations."""
    ensure_table_monitor_config_exists()
    return run_query(f"SELECT * FROM {TABLE_MONITOR_CONFIG_FQN} ORDER BY TABLE_FQN")

def get_schema_threshold_config() -> pd.DataFrame:
    """Get all schema threshold configurations."""
    ensure_table_monitor_config_exists()
    return run_query(f"SELECT * FROM {SCHEMA_THRESHOLD_CONFIG_FQN} ORDER BY DATABASE_NAME, SCHEMA_NAME")

def upsert_table_monitor_config(table_fqn: str, database_name: str, schema_name: str, table_name: str, 
                                 is_monitored: bool,
                                 warn_threshold_minutes: int = None, alert_threshold_minutes: int = None,
                                 notes: str = ""):
    """Add or update table monitoring configuration.
    
    Args:
        is_monitored: Whether to monitor this table
        warn_threshold_minutes: Table-specific warn threshold in minutes (NULL = use schema default)
        alert_threshold_minutes: Table-specific alert threshold in minutes (NULL = use schema default)
    """
    safe = lambda s: (s or "").replace("'", "''")
    
    # Handle NULL values for thresholds
    warn_val = "NULL" if warn_threshold_minutes is None else str(int(warn_threshold_minutes))
    alert_val = "NULL" if alert_threshold_minutes is None else str(int(alert_threshold_minutes))
    
    run_ddl(f"""
        MERGE INTO {TABLE_MONITOR_CONFIG_FQN} t
        USING (SELECT '{safe(table_fqn)}' AS TABLE_FQN) s ON t.TABLE_FQN = s.TABLE_FQN
        WHEN MATCHED THEN UPDATE SET
            IS_MONITORED = {is_monitored},
            WARN_THRESHOLD_MINUTES = {warn_val},
            ALERT_THRESHOLD_MINUTES = {alert_val},
            NOTES = '{safe(notes)}',
            UPDATED_AT = CURRENT_TIMESTAMP()
        WHEN NOT MATCHED THEN INSERT (
            TABLE_FQN, DATABASE_NAME, SCHEMA_NAME, TABLE_NAME, IS_MONITORED,
            WARN_THRESHOLD_MINUTES, ALERT_THRESHOLD_MINUTES, NOTES
        ) VALUES (
            '{safe(table_fqn)}', '{safe(database_name)}', '{safe(schema_name)}', '{safe(table_name)}',
            {is_monitored}, {warn_val}, {alert_val}, '{safe(notes)}'
        )
    """)

def bulk_update_table_monitoring(table_fqns: list, is_monitored: bool):
    """Bulk update monitoring status for multiple tables."""
    if not table_fqns:
        return
    fqn_list = ",".join([f"'{fqn.replace(chr(39), chr(39)+chr(39))}'" for fqn in table_fqns])
    run_ddl(f"""
        UPDATE {TABLE_MONITOR_CONFIG_FQN}
        SET IS_MONITORED = {is_monitored}, UPDATED_AT = CURRENT_TIMESTAMP()
        WHERE TABLE_FQN IN ({fqn_list})
    """)

def upsert_schema_threshold(database_name: str, schema_name: str, 
                            warn_threshold_minutes: int = 1440, alert_threshold_minutes: int = 2880,
                            volume_drop_pct: int = 50, volume_spike_pct: int = 200, 
                            is_monitored: bool = True, notes: str = "",
                            critical_integration: str = None, warning_integration: str = None):
    """Add or update schema threshold configuration.
    
    Args:
        warn_threshold_minutes: Minutes before WARNING (default 1440 = 24 hours)
        alert_threshold_minutes: Minutes before ALERT (default 2880 = 48 hours)
        critical_integration: Notification integration for critical alerts (None = use default)
        warning_integration: Notification integration for warning alerts (None = use default)
        Note: alert_threshold_minutes must be >= warn_threshold_minutes
    """
    # Validate: alert threshold must be >= warn threshold
    if alert_threshold_minutes < warn_threshold_minutes:
        alert_threshold_minutes = warn_threshold_minutes
    
    safe = lambda s: (s or "").replace("'", "''")
    schema_key = f"{database_name}.{schema_name}"
    
    # Handle NULL values for integrations
    crit_int_sql = f"'{safe(critical_integration)}'" if critical_integration else "NULL"
    warn_int_sql = f"'{safe(warning_integration)}'" if warning_integration else "NULL"
    
    run_ddl(f"""
        MERGE INTO {SCHEMA_THRESHOLD_CONFIG_FQN} t
        USING (SELECT '{safe(schema_key)}' AS SCHEMA_KEY) s ON t.SCHEMA_KEY = s.SCHEMA_KEY
        WHEN MATCHED THEN UPDATE SET
            WARN_THRESHOLD_MINUTES = {warn_threshold_minutes},
            ALERT_THRESHOLD_MINUTES = {alert_threshold_minutes},
            DEFAULT_VOLUME_DROP_PCT = {volume_drop_pct},
            DEFAULT_VOLUME_SPIKE_PCT = {volume_spike_pct},
            IS_MONITORED = {is_monitored},
            CRITICAL_INTEGRATION = {crit_int_sql},
            WARNING_INTEGRATION = {warn_int_sql},
            NOTES = '{safe(notes)}',
            UPDATED_AT = CURRENT_TIMESTAMP()
        WHEN NOT MATCHED THEN INSERT (
            SCHEMA_KEY, DATABASE_NAME, SCHEMA_NAME, WARN_THRESHOLD_MINUTES, ALERT_THRESHOLD_MINUTES,
            DEFAULT_VOLUME_DROP_PCT, DEFAULT_VOLUME_SPIKE_PCT, IS_MONITORED, CRITICAL_INTEGRATION, WARNING_INTEGRATION, NOTES
        ) VALUES (
            '{safe(schema_key)}', '{safe(database_name)}', '{safe(schema_name)}',
            {warn_threshold_minutes}, {alert_threshold_minutes}, {volume_drop_pct}, {volume_spike_pct}, {is_monitored}, 
            {crit_int_sql}, {warn_int_sql}, '{safe(notes)}'
        )
    """)

@st.cache_data(ttl=300)
def get_notification_integrations():
    """Get list of available webhook notification integrations."""
    try:
        # Must use session directly (not cached run_query) so LAST_QUERY_ID works
        session.sql("SHOW NOTIFICATION INTEGRATIONS").collect()
        df = session.sql('SELECT "name", "type" FROM TABLE(RESULT_SCAN(LAST_QUERY_ID()))').to_pandas()
        if not df.empty:
            # Filter for WEBHOOK type only
            webhook_df = df[df["type"].str.upper() == "WEBHOOK"]
            if not webhook_df.empty:
                return sorted(webhook_df["name"].tolist())
    except:
        pass
    
    return []

@st.cache_data(ttl=300)
def get_all_notification_integrations_details():
    """Get all notification integrations with their details (for admin panel)."""
    try:
        session.sql("SHOW NOTIFICATION INTEGRATIONS").collect()
        df = session.sql('SELECT "name", "type", "enabled", "comment" FROM TABLE(RESULT_SCAN(LAST_QUERY_ID()))').to_pandas()
        if not df.empty:
            # Filter for WEBHOOK type only
            webhook_df = df[df["type"].str.upper() == "WEBHOOK"]
            return webhook_df
    except:
        pass
    return pd.DataFrame()

def ensure_alert_config_table_exists():
    """Create alert integration config table if not exists."""
    run_ddl(f"""
        CREATE TABLE IF NOT EXISTS {ALERT_CONFIG_FQN} (
            ALERT_TYPE VARCHAR(50) PRIMARY KEY,
            CRITICAL_INTEGRATION VARCHAR(255),
            WARNING_INTEGRATION VARCHAR(255),
            UPDATED_AT TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()
        )
    """)

def get_alert_integration_config(alert_type: str) -> dict:
    """Get integration config for a specific alert type (e.g., 'PIPE_HEALTH', 'KPI')."""
    try:
        df = run_query(f"""
            SELECT CRITICAL_INTEGRATION, WARNING_INTEGRATION 
            FROM {ALERT_CONFIG_FQN} 
            WHERE ALERT_TYPE = '{alert_type}'
        """)
        if not df.empty:
            return {
                "critical": df.iloc[0].get("CRITICAL_INTEGRATION") or "",
                "warning": df.iloc[0].get("WARNING_INTEGRATION") or ""
            }
    except:
        pass
    return {"critical": "", "warning": ""}

def save_alert_integration_config(alert_type: str, critical_integration: str, warning_integration: str):
    """Save integration config for a specific alert type."""
    ensure_alert_config_table_exists()
    crit_val = f"'{critical_integration}'" if critical_integration else "NULL"
    warn_val = f"'{warning_integration}'" if warning_integration else "NULL"
    run_ddl(f"""
        MERGE INTO {ALERT_CONFIG_FQN} T
        USING (SELECT '{alert_type}' AS ALERT_TYPE) S ON T.ALERT_TYPE = S.ALERT_TYPE
        WHEN MATCHED THEN UPDATE SET 
            CRITICAL_INTEGRATION = {crit_val},
            WARNING_INTEGRATION = {warn_val},
            UPDATED_AT = CURRENT_TIMESTAMP()
        WHEN NOT MATCHED THEN INSERT (ALERT_TYPE, CRITICAL_INTEGRATION, WARNING_INTEGRATION)
            VALUES ('{alert_type}', {crit_val}, {warn_val})
    """)

def get_schema_thresholds_map() -> dict:
    """Get schema thresholds as a dictionary for quick lookup."""
    df = get_schema_threshold_config()
    if df.empty:
        return {}
    result = {}
    for _, row in df.iterrows():
        key = f"{row['DATABASE_NAME']}.{row['SCHEMA_NAME']}"
        # Support both old (hours) and new (minutes) column names
        warn_mins = row.get("WARN_THRESHOLD_MINUTES")
        alert_mins = row.get("ALERT_THRESHOLD_MINUTES")
        old_hours = row.get("DEFAULT_FRESHNESS_HOURS")
        
        # Use new columns if available, otherwise convert from old hours column
        if warn_mins is not None and pd.notna(warn_mins):
            warn_threshold = int(warn_mins)
        elif old_hours is not None and pd.notna(old_hours):
            warn_threshold = int(old_hours) * 60
        else:
            warn_threshold = 1440  # 24 hours default
            
        if alert_mins is not None and pd.notna(alert_mins):
            alert_threshold = int(alert_mins)
        elif old_hours is not None and pd.notna(old_hours):
            alert_threshold = int(old_hours) * 60 * 2  # Default alert = 2x warn
        else:
            alert_threshold = 2880  # 48 hours default
        
        result[key] = {
            "warn_threshold_minutes": warn_threshold,
            "alert_threshold_minutes": alert_threshold,
            # Keep freshness_hours for backward compatibility (derived from warn threshold)
            "freshness_hours": warn_threshold // 60,
            "is_monitored": bool(row.get("IS_MONITORED", True)),
            "notes": row.get("NOTES", "")
        }
    return result

def get_table_config_map() -> dict:
    """Get table configurations as a dictionary for quick lookup."""
    df = get_table_monitor_config()
    if df.empty:
        return {}
    result = {}
    for _, row in df.iterrows():
        # Get table-specific thresholds (may be NULL to inherit from schema)
        warn_mins = row.get("WARN_THRESHOLD_MINUTES")
        alert_mins = row.get("ALERT_THRESHOLD_MINUTES")
        
        result[row["TABLE_FQN"]] = {
            "is_monitored": bool(row.get("IS_MONITORED", False)),  # Default to False for new tables
            "warn_threshold_minutes": int(warn_mins) if warn_mins is not None and pd.notna(warn_mins) else None,
            "alert_threshold_minutes": int(alert_mins) if alert_mins is not None and pd.notna(alert_mins) else None,
            "notes": row.get("NOTES", ""),
        }
    return result

def get_effective_table_thresholds(table_fqn: str, schema_key: str, 
                                    table_config_map: dict, schema_thresholds_map: dict) -> dict:
    """Get effective thresholds for a table: table-level first, then schema-level.
    
    Table-specific thresholds (if set) override schema defaults.
    """
    table_cfg = table_config_map.get(table_fqn, {})
    schema_cfg = schema_thresholds_map.get(schema_key, {})
    
    # Check table-level thresholds first, fall back to schema
    table_warn = table_cfg.get("warn_threshold_minutes")
    table_alert = table_cfg.get("alert_threshold_minutes")
    
    # Get effective thresholds: table > schema > default
    warn_mins = table_warn if table_warn else schema_cfg.get("warn_threshold_minutes", 1440)  # 24h default
    alert_mins = table_alert if table_alert else schema_cfg.get("alert_threshold_minutes", 2880)  # 48h default
    
    return {
        "warn_threshold_minutes": warn_mins,
        "alert_threshold_minutes": alert_mins,
        "has_table_override": table_warn is not None or table_alert is not None,
    }

def sync_tables_to_config(tables_df: pd.DataFrame, schema_thresholds: dict):
    """Sync tables from metrics to config table. New tables are NOT monitored by default."""
    ensure_table_monitor_config_exists()
    existing = get_table_config_map()
    
    for _, row in tables_df.iterrows():
        fqn = row.get("FQN") or f"{row['DATABASE_NAME']}.{row['SCHEMA_NAME']}.{row['TABLE_NAME']}"
        if fqn not in existing:
            # New tables are NOT monitored by default - user must explicitly enable
            upsert_table_monitor_config(
                table_fqn=fqn, 
                database_name=row['DATABASE_NAME'], 
                schema_name=row['SCHEMA_NAME'], 
                table_name=row['TABLE_NAME'],
                is_monitored=False,  # Default: NOT monitored
                warn_threshold_minutes=None,  # NULL = inherit from schema
                alert_threshold_minutes=None  # NULL = inherit from schema
            )

# --- VALIDATION HELPERS ---
def check_metrics_table_exists() -> dict:
    """Check if freshness metrics table exists and get metadata."""
    result = {"exists": False, "last_refresh": None, "row_count": 0}
    try:
        df = run_query(f"SELECT COUNT(*) AS ROW_COUNT, MAX(REFRESHED_AT) AS LAST_REFRESH FROM {TABLE_METRICS_TABLE_FQN}")
        if not df.empty and df.iloc[0]["ROW_COUNT"] > 0:
            result = {"exists": True, "row_count": int(df.iloc[0]["ROW_COUNT"]), "last_refresh": df.iloc[0]["LAST_REFRESH"]}
    except: pass
    return result

def get_available_filters() -> dict:
    """Get filter options from freshness metrics table AND schema threshold config."""
    result = {"databases": [], "schemas": [], "db_schema_pairs": []}
    
    # Get schemas from metrics table (already have data)
    metrics_dbs = set()
    metrics_schemas = set()
    metrics_pairs = set()
    try:
        df = run_query(f"SELECT DISTINCT DATABASE_NAME, SCHEMA_NAME FROM {TABLE_METRICS_TABLE_FQN} ORDER BY 1, 2")
        if not df.empty:
            for _, row in df.iterrows():
                db, sch = row["DATABASE_NAME"], row["SCHEMA_NAME"]
                if db and sch:
                    metrics_dbs.add(db)
                    metrics_schemas.add(sch)
                    metrics_pairs.add((db, sch))
    except: pass
    
    # Also get schemas from threshold config (configured but may not have data yet)
    config_dbs = set()
    config_schemas = set()
    config_pairs = set()
    try:
        cfg_df = run_query(f"SELECT DISTINCT DATABASE_NAME, SCHEMA_NAME FROM {SCHEMA_THRESHOLD_CONFIG_FQN} WHERE IS_MONITORED = TRUE")
        if not cfg_df.empty:
            for _, row in cfg_df.iterrows():
                db, sch = row["DATABASE_NAME"], row["SCHEMA_NAME"]
                if db and sch:
                    config_dbs.add(db)
                    config_schemas.add(sch)
                    config_pairs.add((db, sch))
    except: pass
    
    # Combine both sources
    result["databases"] = sorted(metrics_dbs | config_dbs)
    result["schemas"] = sorted(metrics_schemas | config_schemas)
    result["db_schema_pairs"] = sorted(metrics_pairs | config_pairs)
    
    return result

def check_pipe_health_table_exists() -> dict:
    """Check if pipe health metrics table exists and get metadata."""
    result = {"exists": False, "last_refresh": None, "row_count": 0}
    try:
        df = run_query(f"SELECT COUNT(*) AS ROW_COUNT, MAX(REFRESHED_AT) AS LAST_REFRESH FROM {PIPE_HEALTH_METRICS_FQN}")
        if not df.empty and df.iloc[0]["ROW_COUNT"] > 0:
            result = {"exists": True, "row_count": int(df.iloc[0]["ROW_COUNT"]), "last_refresh": df.iloc[0]["LAST_REFRESH"]}
    except: pass
    return result

# -----------------------------------------------------------------------------
# SIDEBAR NAVIGATION - Clean & Modern
# -----------------------------------------------------------------------------
with st.sidebar:
    # Modern header with gradient effect
    st.markdown("""
    <style>
        [data-testid="stSidebar"] {
            background: linear-gradient(180deg, #0F172A 0%, #1E293B 100%);
        }
        [data-testid="stSidebar"] * {
            color: #E2E8F0 !important;
        }
        [data-testid="stSidebar"] .stButton button {
            background: transparent !important;
            border: 1px solid rgba(148, 163, 184, 0.2) !important;
            color: #CBD5E1 !important;
            font-weight: 500 !important;
            transition: all 0.2s ease !important;
        }
        [data-testid="stSidebar"] .stButton button:hover {
            background: rgba(99, 102, 241, 0.15) !important;
            border-color: rgba(99, 102, 241, 0.4) !important;
            color: #FFFFFF !important;
        }
        [data-testid="stSidebar"] .stButton button[kind="primary"] {
            background: linear-gradient(135deg, #6366F1 0%, #8B5CF6 100%) !important;
            border: none !important;
            color: #FFFFFF !important;
            box-shadow: 0 4px 15px -3px rgba(99, 102, 241, 0.4) !important;
        }
        .nav-section-title {
            font-size: 0.7rem;
            text-transform: uppercase;
            letter-spacing: 1px;
            color: #64748B !important;
            margin: 1rem 0 0.5rem 0;
            padding-left: 0.25rem;
        }
    </style>
    <div style="padding: 1rem 0 1.5rem 0; text-align: center;">
        <div style="font-size: 2rem; margin-bottom: 0.25rem;">üìä</div>
        <div style="font-size: 1.1rem; font-weight: 700; color: #FFFFFF !important; letter-spacing: 0.5px;">Data Observability</div>
        <div style="font-size: 0.7rem; color: #64748B !important; margin-top: 0.25rem;">Monitor ‚Ä¢ Analyze ‚Ä¢ Alert</div>
    </div>
    """, unsafe_allow_html=True)
    
    if "selected_page" not in st.session_state:
        st.session_state.selected_page = "home"
    
    if "kpi_page_tab" not in st.session_state:
        st.session_state.kpi_page_tab = "monitor"
    
    def nav_button(label, page_key, key):
        is_active = st.session_state.selected_page == page_key
        if st.button(label, key=key, use_container_width=True, type="primary" if is_active else "secondary"):
            st.session_state.selected_page = page_key
            st.rerun()
    
    # Main Navigation
    st.markdown('<div class="nav-section-title">Overview</div>', unsafe_allow_html=True)
    nav_button("üè†  Dashboard", "home", "nav_home")
    
    st.markdown('<div class="nav-section-title">Monitoring</div>', unsafe_allow_html=True)
    nav_button("üìä  Data Freshness", "data_freshness", "nav_freshness")
    nav_button("‚ö°  Pipelines", "pipelines", "nav_pipelines")
    nav_button("üìà  KPI Metrics", "kpi_monitor", "nav_kpi_monitor")
    
    st.markdown('<div class="nav-section-title">Tools</div>', unsafe_allow_html=True)
    nav_button("ü§ñ  AI Assistant", "ai_assistant", "nav_ai_assistant")
    
    # Admin at bottom with separator
    st.markdown("""<div style="margin-top: 1.5rem; padding-top: 1rem; border-top: 1px solid rgba(100,116,139,0.3);"></div>""", unsafe_allow_html=True)
    nav_button("‚öôÔ∏è  Settings", "tasks", "nav_tasks")
    
    # Page routing - must match the if page_choice == conditions below
    page_mapping = {
        "home": "üè† Home", 
        "pipelines": "üîß Pipelines",
        "data_freshness": "üìã Data Freshness", 
        "kpi_monitor": "üìà KPI Metrics",
        "ai_assistant": "ü§ñ AI Assistant", 
        "tasks": "‚è∞ Tasks"
    }
    page_choice = page_mapping.get(st.session_state.selected_page, "üè† Home")

# --- HOME PAGE ---
if page_choice == "üè† Home":
    # Home page premium styles
    st.markdown("""<style>
        /* Hero Section - Glassmorphism Effect */
        .home-hero { 
            background: linear-gradient(135deg, #0F172A 0%, #1E293B 40%, #334155 100%);
            border-radius: 24px; 
            padding: 3rem 2.5rem; 
            margin-bottom: 2rem; 
            text-align: center; 
            box-shadow: 0 25px 50px -12px rgba(0,0,0,0.25);
            position: relative;
            overflow: hidden;
        }
        .home-hero::before {
            content: '';
            position: absolute;
            top: -50%;
            right: -20%;
            width: 400px;
            height: 400px;
            background: radial-gradient(circle, rgba(99,102,241,0.2) 0%, transparent 70%);
            animation: pulse-glow 4s ease-in-out infinite;
        }
        .home-hero::after {
            content: '';
            position: absolute;
            bottom: -30%;
            left: -10%;
            width: 300px;
            height: 300px;
            background: radial-gradient(circle, rgba(139,92,246,0.15) 0%, transparent 70%);
            animation: pulse-glow 4s ease-in-out infinite 2s;
        }
        @keyframes pulse-glow {
            0%, 100% { transform: scale(1); opacity: 0.6; }
            50% { transform: scale(1.1); opacity: 0.8; }
        }
        .home-hero-badge { 
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            background: rgba(255,255,255,0.1); 
            backdrop-filter: blur(10px);
            padding: 0.5rem 1.25rem; 
            border-radius: 50px; 
            font-size: 0.85rem; 
            color: rgba(255,255,255,0.95); 
            margin-bottom: 1.25rem; 
            border: 1px solid rgba(255,255,255,0.15);
            position: relative;
            z-index: 1;
        }
        .home-hero-title { 
            font-size: 3rem; 
            font-weight: 800; 
            color: #FFFFFF !important; 
            margin: 0 0 0.75rem 0; 
            letter-spacing: -0.02em;
            position: relative;
            z-index: 1;
        }
        .home-hero-subtitle { 
            font-size: 1.15rem !important; 
            color: rgba(255,255,255,0.75) !important; 
            margin: 0 auto 2rem auto !important;
            max-width: 600px;
            text-align: center !important;
            position: relative;
            z-index: 1;
            display: block !important;
            width: 100%;
        }
        .home-hero p {
            text-align: center !important;
            margin-left: auto !important;
            margin-right: auto !important;
        }
        .home-hero-stats { 
            display: flex; 
            justify-content: center; 
            gap: 4rem;
            position: relative;
            z-index: 1;
        }
        .home-hero-stat { 
            text-align: center;
            padding: 1rem 1.5rem;
            background: rgba(255,255,255,0.05);
            border-radius: 16px;
            border: 1px solid rgba(255,255,255,0.1);
        }
        .home-hero-stat-value { 
            font-size: 2rem; 
            font-weight: 700; 
            color: #FFFFFF;
            display: block;
        }
        .home-hero-stat-label { 
            font-size: 0.75rem; 
            color: rgba(255,255,255,0.6); 
            text-transform: uppercase; 
            letter-spacing: 1px;
            margin-top: 0.25rem;
        }
        
        /* Monitoring Cards - Glass Effect */
        .home-card { 
            background: linear-gradient(145deg, rgba(255,255,255,0.95) 0%, rgba(248,250,252,0.9) 100%);
            border-radius: 20px; 
            padding: 1.75rem; 
            box-shadow: 0 10px 40px -10px rgba(0,0,0,0.1), 0 1px 3px rgba(0,0,0,0.05);
            border: 1px solid rgba(226,232,240,0.8);
            height: 100%;
            transition: all 0.3s ease;
            position: relative;
            overflow: hidden;
        }
        .home-card:hover {
            transform: translateY(-4px);
            box-shadow: 0 20px 50px -15px rgba(0,0,0,0.15), 0 1px 3px rgba(0,0,0,0.05);
        }
        .home-card::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            right: 0;
            height: 4px;
            border-radius: 20px 20px 0 0;
        }
        .home-card.kpi-card::before { background: linear-gradient(90deg, #6366F1, #8B5CF6); }
        .home-card.pipe-card::before { background: linear-gradient(90deg, #10B981, #34D399); }
        .home-card.fresh-card::before { background: linear-gradient(90deg, #F59E0B, #FBBF24); }
        
        .home-card-header { 
            display: flex; 
            align-items: center; 
            gap: 0.875rem; 
            margin-bottom: 1.25rem; 
        }
        .home-card-icon { 
            width: 48px; 
            height: 48px; 
            border-radius: 14px; 
            display: flex; 
            align-items: center; 
            justify-content: center; 
            font-size: 1.35rem;
            box-shadow: 0 4px 12px -2px rgba(0,0,0,0.1);
        }
        .home-card-icon.kpi { background: linear-gradient(135deg, #EEF2FF, #E0E7FF); }
        .home-card-icon.pipe { background: linear-gradient(135deg, #ECFDF5, #D1FAE5); }
        .home-card-icon.fresh { background: linear-gradient(135deg, #FFFBEB, #FEF3C7); }
        
        .home-card-title { 
            font-size: 1.05rem; 
            font-weight: 600; 
            color: #1E293B; 
            margin: 0;
            letter-spacing: -0.01em;
        }
        .home-card-subtitle { 
            font-size: 0.8rem; 
            color: #64748B; 
            margin: 0.15rem 0 0 0; 
        }
        .home-card-score { 
            font-size: 2.75rem; 
            font-weight: 700; 
            margin: 0.75rem 0;
            letter-spacing: -0.02em;
        }
        .home-card-score.kpi { color: #6366F1; }
        .home-card-score.pipe { color: #10B981; }
        .home-card-score.fresh { color: #F59E0B; }
        
        .home-card-meta { 
            font-size: 0.9rem; 
            color: #64748B;
            display: flex;
            gap: 1rem;
            flex-wrap: wrap;
        }
        .home-card-meta span { 
            font-weight: 600; 
            color: #334155;
        }
        .home-card-refresh { 
            font-size: 0.8rem; 
            color: #94A3B8; 
            margin-top: 1rem; 
            padding-top: 1rem; 
            border-top: 1px solid #E2E8F0;
            display: flex;
            align-items: center;
            gap: 0.35rem;
        }
        
        /* Status Bar */
        .home-status-bar { 
            background: linear-gradient(145deg, #FFFFFF 0%, #F8FAFC 100%);
            border-radius: 16px; 
            padding: 1.25rem 1.75rem; 
            margin: 2rem 0; 
            border: 1px solid #E2E8F0; 
            display: flex; 
            justify-content: space-around; 
            align-items: center; 
            flex-wrap: wrap; 
            gap: 1.5rem;
            box-shadow: 0 4px 15px -5px rgba(0,0,0,0.05);
        }
        .home-status-item { 
            display: flex; 
            align-items: center; 
            gap: 0.6rem; 
        }
        .home-status-dot {
            width: 10px;
            height: 10px;
            border-radius: 50%;
            animation: pulse 2s infinite;
        }
        .home-status-dot.healthy { background: #10B981; }
        .home-status-dot.warning { background: #F59E0B; }
        .home-status-dot.error { background: #EF4444; }
        @keyframes pulse {
            0%, 100% { opacity: 1; box-shadow: 0 0 0 0 currentColor; }
            50% { opacity: 0.7; box-shadow: 0 0 0 4px transparent; }
        }
        .home-status-label { 
            font-size: 0.85rem; 
            color: #64748B; 
        }
        .home-status-value { 
            font-size: 0.95rem; 
            font-weight: 600; 
            color: #1E293B; 
        }
    </style>""", unsafe_allow_html=True)
    
    # Gather stats from all monitoring sources
    kpi_health_pct, kpi_total, kpi_anomalies = 0, 0, 0
    pipe_health_pct, pipe_total, pipe_errors = 0, 0, 0
    fresh_health_pct, fresh_total, fresh_stale = 0, 0, 0
    
    try:
        kpi_stats = run_query(f"SELECT COUNT(*) AS total, SUM(CASE WHEN STATUS='OK' THEN 1 ELSE 0 END) AS healthy, SUM(CASE WHEN IS_ANOMALY=TRUE THEN 1 ELSE 0 END) AS anomalies FROM {KPI_SUMMARY_FQN}")
        if not kpi_stats.empty and kpi_stats.iloc[0]['TOTAL'] > 0:
            r = kpi_stats.iloc[0]
            kpi_total, kpi_anomalies = int(r['TOTAL']), int(r['ANOMALIES'])
            kpi_health_pct = int((r['HEALTHY'] / r['TOTAL']) * 100) if r['TOTAL'] > 0 else 0
    except: pass
    
    try:
        # Count all anomalies: no data, stale, missing today, low volume, errors
        # Same logic as Pipelines page apply_config function
        pipe_stats = run_query(f"""
            SELECT 
                COUNT(*) AS total,
                SUM(CASE WHEN m.YESTERDAY_FILES > 0 THEN 1 ELSE 0 END) AS active,
                SUM(CASE 
                    -- No data at all
                    WHEN m.YESTERDAY_DATE IS NULL THEN 1
                    -- Stale: yesterday_date is more than 1 day old
                    WHEN c.RUNS_DAILY = TRUE AND c.ALERT_ON_MISSING = TRUE 
                         AND m.YESTERDAY_DATE < CURRENT_DATE() - 1 THEN 1
                    -- Missing today: had data yesterday but nothing today (past expected time)
                    WHEN c.RUNS_DAILY = TRUE AND c.ALERT_ON_MISSING = TRUE 
                         AND m.YESTERDAY_DATE >= CURRENT_DATE() - 1
                         AND COALESCE(m.TODAY_FILES, 0) = 0 
                         AND COALESCE(m.EXPECTED_FILES, 0) > 0 THEN 1
                    -- Errors yesterday
                    WHEN COALESCE(m.YESTERDAY_ERRORS, 0) > 0 THEN 1
                    -- Low volume (files below threshold)
                    WHEN c.ALERT_ON_VOLUME_DROP = TRUE 
                         AND COALESCE(m.EXPECTED_FILES, 0) > 0 
                         AND COALESCE(m.YESTERDAY_FILES, 0) < (COALESCE(c.VOLUME_THRESHOLD_PCT, 50) / 100.0) * m.EXPECTED_FILES THEN 1
                    -- Low volume (rows per file below threshold)
                    WHEN c.ALERT_ON_VOLUME_DROP = TRUE 
                         AND COALESCE(m.EXPECTED_ROWS_PER_FILE, 0) > 0 
                         AND COALESCE(m.YESTERDAY_ROWS_PER_FILE, 0) < (COALESCE(c.VOLUME_THRESHOLD_PCT, 50) / 100.0) * m.EXPECTED_ROWS_PER_FILE THEN 1
                    ELSE 0 
                END) AS issues
            FROM {PIPE_HEALTH_METRICS_FQN} m 
            INNER JOIN {CONFIG_TABLE_FQN} c 
                ON m.PIPE_NAME = c.DATABASE_NAME || '.' || c.SCHEMA_NAME || '.' || c.PIPE_NAME
            WHERE c.IS_MONITORED = TRUE
        """)
        if not pipe_stats.empty and pipe_stats.iloc[0]['TOTAL'] > 0:
            r = pipe_stats.iloc[0]
            pipe_total, pipe_errors = int(r['TOTAL']), int(r['ISSUES'])
            healthy_pipes = pipe_total - pipe_errors
            pipe_health_pct = int((healthy_pipes / pipe_total) * 100) if pipe_total > 0 else 0
            pipe_health_pct = max(0, pipe_health_pct)
    except: pass
    
    try:
        fresh_stats = run_query(f"SELECT COUNT(*) AS total, SUM(CASE WHEN HOURS_SINCE_WRITE<24 THEN 1 ELSE 0 END) AS fresh, SUM(CASE WHEN HOURS_SINCE_WRITE>=48 OR HOURS_SINCE_WRITE IS NULL THEN 1 ELSE 0 END) AS stale, MAX(REFRESHED_AT) AS last_refresh FROM {TABLE_METRICS_TABLE_FQN}")
        if not fresh_stats.empty and fresh_stats.iloc[0]['TOTAL'] > 0:
            r = fresh_stats.iloc[0]
            fresh_total, fresh_stale = int(r['TOTAL']), int(r['STALE'])
            fresh_health_pct = int((r['FRESH'] / r['TOTAL']) * 100) if r['TOTAL'] > 0 else 0
            fresh_last_refresh = r['LAST_REFRESH']
    except: pass
    
    # Get last refresh timestamps for each monitoring type
    kpi_last_refresh, pipe_last_refresh, fresh_last_refresh = None, None, None
    
    try:
        kpi_refresh = run_query(f"SELECT MAX(REFRESHED_AT) AS last_refresh FROM {KPI_SUMMARY_FQN}")
        if not kpi_refresh.empty and kpi_refresh.iloc[0]['LAST_REFRESH'] is not None:
            kpi_last_refresh = kpi_refresh.iloc[0]['LAST_REFRESH']
    except: pass
    
    try:
        pipe_refresh = run_query(f"SELECT MAX(REFRESHED_AT) AS last_refresh FROM {PIPE_HEALTH_METRICS_FQN}")
        if not pipe_refresh.empty and pipe_refresh.iloc[0]['LAST_REFRESH'] is not None:
            pipe_last_refresh = pipe_refresh.iloc[0]['LAST_REFRESH']
    except: pass
    
    try:
        fresh_refresh = run_query(f"SELECT MAX(REFRESHED_AT) AS last_refresh FROM {TABLE_METRICS_TABLE_FQN}")
        if not fresh_refresh.empty and fresh_refresh.iloc[0]['LAST_REFRESH'] is not None:
            fresh_last_refresh = fresh_refresh.iloc[0]['LAST_REFRESH']
    except: pass
    
    # Format timestamps for display
    def format_refresh_time(ts):
        if ts is None:
            return "No data"
        try:
            if isinstance(ts, str):
                ts = pd.to_datetime(ts)
            return ts.strftime("%m/%d %H:%M")
        except:
            return "‚Äî"
    
    kpi_refresh_str = format_refresh_time(kpi_last_refresh)
    pipe_refresh_str = format_refresh_time(pipe_last_refresh)
    fresh_refresh_str = format_refresh_time(fresh_last_refresh)
    
    health_scores = [s for s in [kpi_health_pct, pipe_health_pct, fresh_health_pct] if s > 0]
    overall_health = int(sum(health_scores) / len(health_scores)) if health_scores else 0
    total_issues = kpi_anomalies + pipe_errors + fresh_stale
    
    # Determine overall system status
    system_status = "healthy" if total_issues == 0 else ("warning" if total_issues < 5 else "error")
    status_text = "All systems operational" if total_issues == 0 else f"{total_issues} issue{'s' if total_issues != 1 else ''} detected"
    
    # Hero Section
    st.markdown(f"""<div class="home-hero">
        <div class="home-hero-badge">
            <span style="width:8px;height:8px;background:{'#10B981' if system_status == 'healthy' else '#F59E0B' if system_status == 'warning' else '#EF4444'};border-radius:50%;display:inline-block;animation:pulse 2s infinite;"></span>
            {status_text}
        </div>
        <h1 class="home-hero-title">Data Observability</h1>
        <p class="home-hero-subtitle">Monitor, analyze, and maintain the health of your entire data ecosystem in real-time</p>
        <div class="home-hero-stats">
            <div class="home-hero-stat">
                <span class="home-hero-stat-value">{kpi_total + pipe_total + fresh_total:,}</span>
                <div class="home-hero-stat-label">Assets Monitored</div>
            </div>
            <div class="home-hero-stat">
                <span class="home-hero-stat-value">{overall_health}%</span>
                <div class="home-hero-stat-label">Overall Health</div>
            </div>
            <div class="home-hero-stat">
                <span class="home-hero-stat-value" style="color: {'#10B981' if total_issues == 0 else '#FBBF24'}">{total_issues}</span>
                <div class="home-hero-stat-label">Active Issues</div>
            </div>
        </div>
    </div>
    """, unsafe_allow_html=True)
    
    # =========================================================================
    # HEALTH SCORE CARDS (Using Streamlit columns)
    # =========================================================================
    col1, col2, col3 = st.columns(3)
    
    with col1:
        kpi_status_color = '#10B981' if kpi_anomalies == 0 else '#EF4444'
        st.markdown(f"""
        <div class="home-card kpi-card">
            <div class="home-card-header">
                <div class="home-card-icon kpi">üìà</div>
                <div>
                    <div class="home-card-title">Business KPIs</div>
                    <div class="home-card-subtitle">Performance metrics & anomalies</div>
                </div>
            </div>
            <div class="home-card-score kpi">{kpi_health_pct}%</div>
            <div class="home-card-meta">
                <div><span>{kpi_total}</span> KPIs tracked</div>
                <div><span style="color: {kpi_status_color}">{kpi_anomalies}</span> anomalies</div>
            </div>
            <div class="home-card-refresh">üïê {kpi_refresh_str}</div>
        </div>
        """, unsafe_allow_html=True)
        if st.button("View KPI Metrics ‚Üí", key="home_kpi", use_container_width=True, type="primary"):
            st.session_state.selected_page = "kpi_monitor"
            st.rerun()
    
    with col2:
        pipe_status_color = '#10B981' if pipe_errors == 0 else '#EF4444'
        st.markdown(f"""
        <div class="home-card pipe-card">
            <div class="home-card-header">
                <div class="home-card-icon pipe">‚ö°</div>
                <div>
                    <div class="home-card-title">Data Pipelines</div>
                    <div class="home-card-subtitle">Snowpipe ingestion health</div>
                </div>
            </div>
            <div class="home-card-score pipe">{pipe_health_pct}%</div>
            <div class="home-card-meta">
                <div><span>{pipe_total}</span> pipes monitored</div>
                <div><span style="color: {pipe_status_color}">{pipe_errors}</span> with issues</div>
            </div>
            <div class="home-card-refresh">üïê {pipe_refresh_str}</div>
        </div>
        """, unsafe_allow_html=True)
        if st.button("View Pipelines ‚Üí", key="home_pipes", use_container_width=True, type="primary"):
            st.session_state.selected_page = "pipelines"
            st.rerun()
    
    with col3:
        fresh_status_color = '#10B981' if fresh_stale == 0 else '#F59E0B'
        st.markdown(f"""
        <div class="home-card fresh-card">
            <div class="home-card-header">
                <div class="home-card-icon fresh">üìä</div>
                <div>
                    <div class="home-card-title">Data Freshness</div>
                    <div class="home-card-subtitle">Table update monitoring</div>
                </div>
            </div>
            <div class="home-card-score fresh">{fresh_health_pct}%</div>
            <div class="home-card-meta">
                <div><span>{fresh_total}</span> tables tracked</div>
                <div><span style="color: {fresh_status_color}">{fresh_stale}</span> stale</div>
            </div>
            <div class="home-card-refresh">üïê {fresh_refresh_str}</div>
        </div>
        """, unsafe_allow_html=True)
        if st.button("View Freshness ‚Üí", key="home_fresh", use_container_width=True, type="primary"):
            st.session_state.selected_page = "data_freshness"
            st.rerun()
    
    # =========================================================================
    # STATUS BAR - System Health Summary
    # =========================================================================
    health_dot = "healthy" if overall_health >= 90 else "warning" if overall_health >= 70 else "error"
    issues_dot = "healthy" if total_issues == 0 else "warning" if total_issues < 5 else "error"
    
    st.markdown(f"""
    <div class="home-status-bar">
        <div class="home-status-item">
            <div class="home-status-dot {health_dot}"></div>
            <div>
                <div class="home-status-label">System Health</div>
                <div class="home-status-value">{overall_health}% Healthy</div>
            </div>
        </div>
        <div class="home-status-item">
            <div class="home-status-dot {issues_dot}"></div>
            <div>
                <div class="home-status-label">Active Issues</div>
                <div class="home-status-value">{total_issues} item{'s' if total_issues != 1 else ''}</div>
            </div>
        </div>
        <div class="home-status-item">
            <div class="home-status-dot healthy"></div>
            <div>
                <div class="home-status-label">KPIs</div>
                <div class="home-status-value">{kpi_total} monitored</div>
            </div>
        </div>
        <div class="home-status-item">
            <div class="home-status-dot healthy"></div>
            <div>
                <div class="home-status-label">Pipelines</div>
                <div class="home-status-value">{pipe_total} active</div>
            </div>
        </div>
    </div>
    """, unsafe_allow_html=True)
    
    # =========================================================================
    # GETTING STARTED GUIDE
    # =========================================================================
    st.markdown("")
    
    with st.expander("üìñ **Getting Started Guide** - New to Data Observability? Start here!", expanded=False):
        st.markdown("""
        ### üéØ What is Data Observability?
        
        Data Observability is your **central command center** for monitoring the health of your Snowflake data platform. 
        It helps you catch data issues before they impact your business.
        
        ---
        
        ### üöÄ Quick Start Guide
        
        | Step | Feature | Description |
        |------|---------|-------------|
        | 1Ô∏è‚É£ | **Business KPIs** | Track metrics from your data warehouse. Add SQL queries for key numbers (revenue, user counts, etc.) |
        | 2Ô∏è‚É£ | **Pipeline Monitoring** | Monitor Snowpipe ingestion. Track file loads and errors |
        | 3Ô∏è‚É£ | **Data Freshness** | Ensure tables are updated on schedule. Configure thresholds per schema or table |
        | 4Ô∏è‚É£ | **AI Assistant** | Get instant answers about data health. Ask questions or request reports |
        
        ---
        
        ### üí° Pro Tips
        
        - **Start with KPIs**: Add 2-3 critical business metrics to track first
        - **Use AI**: Click "Ask AI" anytime to get intelligent insights about issues
        - **Check daily**: The home page gives you a quick health overview at a glance
        - **Set alerts**: Configure threshold alerts to catch problems early
        """)

# =============================================================================
# DATA FRESHNESS PAGE - Monitor table update frequency and staleness
# =============================================================================
if page_choice == "üìã Data Freshness":
    render_page_header(
        "üìã",
        "Data Freshness Monitor",
        "Track when your tables were last updated and detect volume anomalies automatically"
    )
    
    # Check if materialized table exists
    mat_status = check_metrics_table_exists()
    
    if not mat_status["exists"]:
        st.error(f"""
        ### ‚ö†Ô∏è Materialized Table Not Found
        
        The table `{TABLE_METRICS_TABLE_FQN}` does not exist or is empty.
        
        **To populate the data, run the scheduled task:**
        
        ```sql
        EXECUTE TASK {CONFIG_DATABASE}.{CONFIG_SCHEMA}.DATA_FRESHNESS_DAILY_TASK;
        ```
        
        Or check if the task is running:
        ```sql
        SHOW TASKS LIKE 'DATA_FRESHNESS_DAILY_TASK' IN SCHEMA {CONFIG_DATABASE}.{CONFIG_SCHEMA};
        ```
        
        Contact your administrator if you need help setting this up.
        """)
        st.stop()
    
    # Ensure config tables exist
    ensure_table_monitor_config_exists()
    
    # Load configuration data
    schema_thresholds = get_schema_thresholds_map()
    table_config = get_table_config_map()
    
    # Get available schemas for filtering
    available_filters = get_available_filters()
    
    # Show last refresh time
    if mat_status["last_refresh"]:
        refresh_time = pd.to_datetime(mat_status["last_refresh"])
        # Ensure refresh_time is timezone-naive for comparison
        if refresh_time.tzinfo is not None:
            refresh_time = refresh_time.tz_convert(None)
        hours_ago = (pd.Timestamp.utcnow().tz_localize(None) - refresh_time).total_seconds() / 3600
        if hours_ago > 24:
            st.warning(f"‚ö†Ô∏è Data last refreshed **{hours_ago:.0f} hours ago** ({refresh_time:%Y-%m-%d %H:%M} UTC). Consider running the refresh task.")
        else:
            st.caption(f"üìä Data refreshed {hours_ago:.1f} hours ago ({refresh_time:%Y-%m-%d %H:%M} UTC) ‚Ä¢ {mat_status['row_count']:,} tables monitored")
    
    # Info section
    with st.expander("‚ÑπÔ∏è **How It Works** - Understanding freshness & volume tracking", expanded=False):
        st.markdown("""
        ### Data Sources
        
        This page combines multiple Snowflake metadata views for comprehensive monitoring:
        
        | Source | Purpose | What It Captures |
        |--------|---------|------------------|
        | `ACCOUNT_USAGE.ACCESS_HISTORY` | DML tracking | INSERT, UPDATE, DELETE, MERGE with row counts |
        | `ACCOUNT_USAGE.QUERY_HISTORY` | Query tracking | COPY INTO, CTAS, INSERT operations with rows produced |
        | `ACCOUNT_USAGE.TABLES` | Current metadata | Current row counts, size, creation date |
        
        **Why multiple sources?**
        - `ACCESS_HISTORY` captures DML but may miss some COPY INTO/CTAS operations
        - `QUERY_HISTORY` captures COPY INTO and CTAS with `ROWS_PRODUCED`
        - Combining both gives the most complete picture of data loading activity
        
        ### Freshness Logic
        
        | Status | Meaning |
        |--------|---------|
        | ‚úÖ **Fresh** | Table was modified within the threshold |
        | üîµ **Recreated** | Table was created recently (CTAS pattern) |
        | üî¥ **Stale** | Table hasn't been modified within the threshold |
        | ‚ö™ **No History** | No modification records found in the lookback period |
        
        ### Volume Anomaly Detection
        
        The system detects volume issues across all ingestion patterns:
        
        | Anomaly Type | Description | Detection Method |
        |--------------|-------------|------------------|
        | üî¥ **Empty Table** | Table has 0 rows when previously had data | Current rows = 0 AND baseline > 0 |
        | üü† **Volume Drop** | Significant decrease from baseline | Current < threshold % of baseline |
        | üü° **Low Growth** | Append-only table not growing | Daily delta < expected for streaming/CDC |
        | üü£ **Volume Spike** | Unexpected increase (possible duplicates) | Current > 200% of baseline |
        | ‚ö™ **Insufficient History** | Not enough data for baseline | < 7 days of historical data |
        
        ### Ingestion Pattern Support
        
        | Pattern | How We Track It |
        |---------|-----------------|
        | **Batch/ETL** | Daily row count changes via TABLE_STORAGE_METRICS |
        | **Streaming/CDC** | Continuous inserts tracked via ACCESS_HISTORY aggregates |
        | **CTAS (Replace)** | Full table swap detection via creation date vs baseline |
        | **Incremental** | Row count growth rate analysis |
        
        ### Baseline Calculation
        
        - **Method**: Median of daily row counts over configurable window (default 14 days)
        - **Why Median**: Robust against outliers and weekend/holiday variations
        - **Day-of-Week Adjustment**: Optional seasonal adjustment for weekday patterns
        
        ### Important Notes
        
        - **Latency**: TABLE_STORAGE_METRICS updates daily; ACCESS_HISTORY lags ~3 hours
        - **History Depth**: TABLE_STORAGE_METRICS retains 365 days of history
        - **CTAS Detection**: Recently recreated tables use post-creation baseline
        
        ### Required Permissions
        
        ```sql
        GRANT IMPORTED PRIVILEGES ON DATABASE SNOWFLAKE TO ROLE your_role;
        ```
        """)
    
    st.markdown("---")
    
    # Sidebar controls
    with st.sidebar:
        st.markdown("### üîç Filters")
        
        # Search box for table name
        search_text = st.text_input("üîé Search tables", placeholder="Filter by table name...", help="Filter tables by name")
        
        # Schema filter
        if available_filters["schemas"]:
            selected_schemas = st.multiselect(
                "üìÅ Filter by Schema",
                options=available_filters["schemas"],
                default=[],
                help="Select schemas to show (empty = all)",
                key="schema_filter"
            )
        else:
            selected_schemas = []
        
        # Database filter
        if available_filters["databases"]:
            selected_databases = st.multiselect(
                "üóÑÔ∏è Filter by Database",
                options=available_filters["databases"],
                default=[],
                help="Select databases to show (empty = all)",
                key="db_filter"
            )
        else:
            selected_databases = []
        
        # Monitored only filter
        show_monitored_only = st.checkbox("Monitored tables only", value=False, help="Show only tables marked as monitored")
        
        st.markdown("---")
        st.markdown("### ‚è±Ô∏è Freshness Settings")
        
        freshness_hours = st.slider("Default stale threshold (hours)", 1, 168, 24, help="Flag tables not updated within this time")
        
        st.markdown("---")
        st.markdown("### üîç Display Options")
        
        show_issues_only = st.checkbox("Show issues only", value=True, help="Filter to tables with freshness issues")
        include_size = st.checkbox("Show size metrics", value=False)
    
    # =========================================================================
    # QUERY DATA FROM MATERIALIZED TABLE
    # =========================================================================
    dq_sql = f"""
    SELECT 
        DATABASE_NAME, SCHEMA_NAME, TABLE_NAME, FQN,
        CURRENT_ROWS, CURRENT_BYTES, TABLE_CREATED, LAST_ALTERED,
        LAST_MODIFIED_DATE AS LAST_MODIFIED,
        HOURS_SINCE_WRITE, TODAY_ROWS_MODIFIED, RECENT_INSERTS, ACTIVE_DAYS, DATA_SOURCES,
        HISTORY_DAYS, BASELINE_DAILY_INSERTS, MEDIAN_DAILY_INSERTS,
        AVG_DAILY_GROWTH, MEDIAN_DAILY_GROWTH, BASELINE_TOTAL_INSERTS, BASELINE_NET_CHANGE,
        GROWTH_DAYS, SHRINK_DAYS, CHURN_DAYS, QUIET_DAYS,
        TODAY_INSERTS, TODAY_NET_CHANGE, YESTERDAY_INSERTS, YESTERDAY_NET_CHANGE,
        PCT_OF_BASELINE,
        INGEST_PATTERN, TABLE_AGE_DAYS, DAYS_SINCE_ALTERED, BASELINE_ROWS,
        REFRESHED_AT
    FROM {TABLE_METRICS_TABLE_FQN}
    ORDER BY HOURS_SINCE_WRITE DESC NULLS LAST
    """
    
    dq_df = run_query(dq_sql)
    
    if dq_df.empty:
        st.warning("No data in materialized table. Please run the refresh task.")
        st.stop()
    
    # Apply filters
    # Database filter
    if selected_databases:
        dq_df = dq_df[dq_df["DATABASE_NAME"].isin(selected_databases)]
    
    # Schema filter
    if selected_schemas:
        dq_df = dq_df[dq_df["SCHEMA_NAME"].isin(selected_schemas)]
    
    # Apply search filter if provided
    if search_text:
        search_upper = search_text.upper()
        dq_df = dq_df[
            dq_df["TABLE_NAME"].str.upper().str.contains(search_upper, na=False) |
            dq_df["DATABASE_NAME"].str.upper().str.contains(search_upper, na=False) |
            dq_df["SCHEMA_NAME"].str.upper().str.contains(search_upper, na=False)
        ]
    
    if dq_df.empty:
        st.info("No tables match the current filters. Try adjusting your filter criteria.")
        st.stop()
    
    # Add FQN if not present
    if "FQN" not in dq_df.columns or dq_df["FQN"].isna().all():
        dq_df["FQN"] = dq_df["DATABASE_NAME"] + "." + dq_df["SCHEMA_NAME"] + "." + dq_df["TABLE_NAME"]
    
    # Add monitoring config to dataframe - default to FALSE for tables not yet in config
    dq_df["IS_MONITORED"] = dq_df["FQN"].apply(lambda x: table_config.get(x, {}).get("is_monitored", False))
    
    # Get effective threshold for each table (table-level overrides schema-level)
    def get_effective_threshold_minutes(row):
        """Get effective freshness threshold: table-level first, then schema-level, then default."""
        fqn = row["FQN"]
        schema_key = f"{row['DATABASE_NAME']}.{row['SCHEMA_NAME']}"
        
        # Check table-level threshold first
        table_cfg = table_config.get(fqn, {})
        if table_cfg.get("warn_threshold_minutes"):
            return table_cfg.get("warn_threshold_minutes")
        
        # Fall back to schema-level threshold
        schema_cfg = schema_thresholds.get(schema_key, {})
        if schema_cfg.get("warn_threshold_minutes"):
            return schema_cfg.get("warn_threshold_minutes")
        
        # Default: convert hours slider to minutes
        return freshness_hours * 60
    
    dq_df["CONFIG_THRESHOLD_MINUTES"] = dq_df.apply(get_effective_threshold_minutes, axis=1)
    dq_df["CONFIG_FRESHNESS_HOURS"] = dq_df["CONFIG_THRESHOLD_MINUTES"] / 60
    
    # Apply monitored-only filter if enabled
    if show_monitored_only:
        dq_df = dq_df[dq_df["IS_MONITORED"] == True]
        if dq_df.empty:
            st.info("No monitored tables found. Configure monitoring in the Setup view.")
            st.stop()
    
    # Process data - Type conversions
    # Convert timestamps and ensure they are timezone-naive for comparison
    last_mod = pd.to_datetime(dq_df["LAST_MODIFIED"])
    if last_mod.dt.tz is not None:
        last_mod = last_mod.dt.tz_convert(None)
    dq_df["LAST_MODIFIED"] = last_mod
    
    table_created = pd.to_datetime(dq_df["TABLE_CREATED"])
    if table_created.dt.tz is not None:
        table_created = table_created.dt.tz_convert(None)
    dq_df["TABLE_CREATED"] = table_created
    
    # Current timestamp for calculations
    now_naive = pd.Timestamp.utcnow().tz_localize(None)
    
    # Calculate hours since write DYNAMICALLY for accurate freshness
    # The materialized HOURS_SINCE_WRITE is stale (calculated at refresh time)
    # 
    # Use the most recent of:
    # 1. LAST_MODIFIED (date only from daily volume - represents DML activity date)
    # 2. LAST_ALTERED (timestamp from Snowflake metadata - more precise but may include DDL)
    #
    # LAST_ALTERED is a full timestamp, LAST_MODIFIED is just a date (midnight)
    
    # Convert LAST_ALTERED to timezone-naive if it exists
    if "LAST_ALTERED" in dq_df.columns:
        last_altered = pd.to_datetime(dq_df["LAST_ALTERED"])
        if last_altered.dt.tz is not None:
            last_altered = last_altered.dt.tz_convert(None)
        dq_df["_LAST_ALTERED_TS"] = last_altered
    else:
        dq_df["_LAST_ALTERED_TS"] = pd.NaT
    
    # Calculate hours since LAST_MODIFIED (date only - will be hours since midnight of that date)
    hours_since_last_mod = ((now_naive - dq_df["LAST_MODIFIED"]).dt.total_seconds() / 3600)
    
    # Calculate hours since LAST_ALTERED (precise timestamp)
    hours_since_last_altered = ((now_naive - dq_df["_LAST_ALTERED_TS"]).dt.total_seconds() / 3600)
    
    # Use the SMALLER of the two (more recent activity) - this catches both DML and DDL
    # If LAST_MODIFIED is today, hours_since_last_mod will be hours since midnight (could be 0-24)
    # If LAST_ALTERED is more recent, use that instead
    dq_df["HOURS_SINCE_WRITE"] = pd.concat([hours_since_last_mod, hours_since_last_altered], axis=1).min(axis=1).round(1)
    
    # Handle NaN values - use a large number to indicate no known activity
    dq_df["HOURS_SINCE_WRITE"] = dq_df["HOURS_SINCE_WRITE"].fillna(9999)
    
    # Calculate hours since created
    dq_df["HOURS_SINCE_CREATED"] = ((now_naive - dq_df["TABLE_CREATED"]).dt.total_seconds() / 3600).round(1)
    
    # Numeric conversions for volume columns
    volume_cols = ["CURRENT_ROWS", "BASELINE_ROWS", "BASELINE_DAILY_INSERTS", "TODAY_INSERTS",
                   "YESTERDAY_INSERTS", "PCT_OF_BASELINE", 
                   "HISTORY_DAYS", "AVG_DAILY_GROWTH", "MEDIAN_DAILY_GROWTH", "BASELINE_TOTAL_INSERTS",
                   "GROWTH_DAYS", "SHRINK_DAYS", "CHURN_DAYS", "QUIET_DAYS"]
    for col in volume_cols:
        if col in dq_df.columns:
            dq_df[col] = pd.to_numeric(dq_df[col], errors="coerce")
    
    # === FRESHNESS STATUS ===
    # Use per-table configured threshold (in hours) for freshness check
    threshold_hours = dq_df["CONFIG_FRESHNESS_HOURS"]
    recently_created = dq_df["HOURS_SINCE_CREATED"] < threshold_hours
    dq_df["IS_STALE"] = ~recently_created & (dq_df["LAST_MODIFIED"].isna() | (dq_df["HOURS_SINCE_WRITE"].fillna(np.inf) >= threshold_hours))
    
    dq_df["FRESHNESS_STATUS"] = np.where(recently_created, "üîµ Recreated",
                      np.where(dq_df["LAST_MODIFIED"].isna(), "‚ö™ No history",
                      np.where(dq_df["IS_STALE"], "üî¥ Stale", "‚úÖ Fresh")))
    
    # Freshness issues only (volume monitoring removed)
    dq_df["HAS_ANY_ISSUE"] = dq_df["IS_STALE"]
    
    dq_df["SIZE_GB"] = (dq_df["CURRENT_BYTES"].fillna(0) / 1024**3).round(2)
    
    # =========================================================================
    # METRICS OVERVIEW - Only count MONITORED tables for issues
    # =========================================================================
    
    table_cfg_map = get_table_config_map()
    
    # Filter to only monitored tables for issue counting
    monitored_df = dq_df[dq_df["IS_MONITORED"] == True]
    
    # Calculate issue counts
    total_issues = int((monitored_df["HAS_ANY_ISSUE"]).sum())
    
    # Summary counts
    total_tables = len(dq_df)
    monitored_count = len(monitored_df)
    healthy_count = int((monitored_df["HAS_ANY_ISSUE"] == False).sum())
    
    # ‚îÄ‚îÄ MONITORING OVERVIEW ‚îÄ‚îÄ
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        if total_issues > 0:
            st.metric("‚ö†Ô∏è Issues", total_issues, help="Tables with freshness issues")
        else:
            st.metric("‚ö†Ô∏è Issues", "‚úì", help="No issues detected")
    
    with col2:
        st.metric("‚úÖ Healthy", healthy_count, help="Monitored tables without issues")
    
    with col3:
        st.metric("üîî Monitored", monitored_count, help="Tables with monitoring enabled")
    
    with col4:
        st.metric("üìä Total Tables", total_tables, help="Total tables in selected schemas")
    
    # =========================================================================
    # TABS: MONITORING vs CONFIGURATION
    # =========================================================================
    st.markdown("---")
    
    # Use key to persist tab selection across reruns
    if "freshness_tab" not in st.session_state:
        st.session_state.freshness_tab = 0
    
    monitor_tab, config_tab = st.tabs(["üìä Monitoring", "‚öôÔ∏è Configuration"])
    
    with monitor_tab:
        # =========================================================================
        # STATUS DISTRIBUTION CHARTS
        # =========================================================================
        st.markdown("### üìà Status Distribution (All Tables)")
        
        chart_col1, chart_col2 = st.columns(2)
        
        with chart_col1:
            st.markdown("**Freshness Status**")
            freshness_counts = dq_df.groupby("FRESHNESS_STATUS").size().reset_index(name="Count")
            freshness_chart = alt.Chart(freshness_counts).mark_bar(cornerRadiusTopLeft=4, cornerRadiusTopRight=4).encode(
                x=alt.X("Count:Q", title="Tables"),
                y=alt.Y("FRESHNESS_STATUS:N", sort="-x", title=""),
                color=alt.Color("FRESHNESS_STATUS:N", legend=None, scale=alt.Scale(
                    domain=["‚úÖ Fresh", "üîµ Recreated", "üî¥ Stale", "‚ö™ No history"],
                    range=["#10B981", "#3B82F6", "#EF4444", "#9CA3AF"]))
            ).properties(height=150)
            st.altair_chart(freshness_chart, use_container_width=True)
        
        with chart_col2:
            st.markdown("**Ingestion Patterns**")
            if "INGEST_PATTERN" in dq_df.columns:
                pattern_counts = dq_df.groupby("INGEST_PATTERN").size().reset_index(name="Count")
                pattern_chart = alt.Chart(pattern_counts).mark_bar(cornerRadiusTopLeft=4, cornerRadiusTopRight=4).encode(
                    x=alt.X("Count:Q", title="Tables"),
                    y=alt.Y("INGEST_PATTERN:N", sort="-x", title=""),
                    color=alt.Color("INGEST_PATTERN:N", legend=None)
                ).properties(height=150)
                st.altair_chart(pattern_chart, use_container_width=True)
        
        # =========================================================================
        # TABLE DETAILS - Hierarchical view: Schema summary -> Table details
        # =========================================================================
        st.markdown("---")
        st.markdown("### üìã Schema & Table Details")
        
        # Filters row: Database + Schema filters only (status/monitor filters shown when schema selected)
        filter_col1, filter_col2 = st.columns([1, 1])
        
        with filter_col1:
            # Database filter first - include configured databases even if no data yet
            db_from_data = set(dq_df["DATABASE_NAME"].dropna().unique().tolist())
            db_from_config = set(available_filters.get("databases", []))
            table_db_options = ["All Databases"] + sorted(db_from_data | db_from_config)
            table_db_filter = st.selectbox(
                "üóÑÔ∏è Database",
                table_db_options,
                key="table_details_db_filter"
            )
        
        with filter_col2:
            # Schema filter - include configured schemas even if no data yet
            # Use db_schema_pairs to get correct schemas for selected database
            if table_db_filter != "All Databases":
                schemas_from_data = set(dq_df[dq_df["DATABASE_NAME"] == table_db_filter]["SCHEMA_NAME"].unique().tolist())
                schemas_from_config = set(sch for db, sch in available_filters.get("db_schema_pairs", []) if db == table_db_filter)
                schema_options = sorted(schemas_from_data | schemas_from_config)
            else:
                schemas_from_data = set(dq_df["SCHEMA_NAME"].unique().tolist())
                schemas_from_config = set(available_filters.get("schemas", []))
                schema_options = sorted(schemas_from_data | schemas_from_config)
            table_schema_filter = st.selectbox(
                "üìÅ Schema",
                ["All Schemas"] + schema_options,
                key="table_details_schema_filter",
                help="Select a schema to view table details"
            )
        
        # Apply database filter first
        if table_db_filter != "All Databases":
            filtered_df = dq_df[dq_df["DATABASE_NAME"] == table_db_filter].copy()
        else:
            filtered_df = dq_df.copy()
        
        # =========================================================================
        # SECTION 1: ALERTS TABLE (Warnings & Critical Issues)
        # =========================================================================
        st.markdown("### üö® Active Alerts")
        st.caption("Tables with freshness issues that need attention")
        
        # Get all monitored tables with issues
        alerts_df = dq_df[(dq_df["IS_MONITORED"] == True) & (dq_df["HAS_ANY_ISSUE"] == True)].copy()
        
        # Apply database filter
        if table_db_filter != "All Databases":
            alerts_df = alerts_df[alerts_df["DATABASE_NAME"] == table_db_filter]
        
        # Apply schema filter
        if table_schema_filter != "All Schemas":
            alerts_df = alerts_df[alerts_df["SCHEMA_NAME"] == table_schema_filter]
        
        if alerts_df.empty:
            st.success("‚úÖ No active alerts! All monitored tables are healthy.")
        else:
            # Sort by hours since update (most stale first)
            alerts_df = alerts_df.sort_values(by=["HOURS_SINCE_WRITE"], ascending=[False])
            
            # Build display dataframe
            alert_display = alerts_df[["DATABASE_NAME", "SCHEMA_NAME", "TABLE_NAME", 
                                       "HOURS_SINCE_WRITE", "CONFIG_FRESHNESS_HOURS", "FRESHNESS_STATUS"]].copy()
            
            # Format columns
            alert_display["HOURS_SINCE_WRITE"] = alert_display["HOURS_SINCE_WRITE"].apply(
                lambda x: f"{x:.1f}h" if pd.notna(x) else "‚Äî"
            )
            alert_display["CONFIG_FRESHNESS_HOURS"] = alert_display["CONFIG_FRESHNESS_HOURS"].apply(
                lambda x: f"{x:.0f}h" if pd.notna(x) else "‚Äî"
            )
            
            # Rename columns
            alert_display.columns = ["Database", "Schema", "Table", "Hours Ago", "Threshold", "Status"]
            
            st.metric("‚ö†Ô∏è Tables with issues", len(alerts_df))
            
            # Display alerts table
            st.dataframe(
                alert_display,
                use_container_width=True,
                hide_index=True,
                height=min(300, 50 + len(alert_display) * 35),
                column_config={
                    "Database": st.column_config.TextColumn("Database", width="medium"),
                    "Schema": st.column_config.TextColumn("Schema", width="medium"),
                    "Table": st.column_config.TextColumn("Table", width="large"),
                    "Hours Ago": st.column_config.TextColumn("Hours Ago", width="small"),
                    "Threshold": st.column_config.TextColumn("Threshold", width="small"),
                    "Status": st.column_config.TextColumn("Status", width="medium"),
                }
            )
        
        # =========================================================================
        # TABLE DRILLDOWN (Inside monitoring tab)
        # =========================================================================
        st.markdown("---")
        st.markdown("### üîç Table Activity History")
        
        table_options_m = ["Select a table..."] + [f"{r['DATABASE_NAME']}.{r['SCHEMA_NAME']}.{r['TABLE_NAME']}" for _, r in dq_df.iterrows()]
        selected_table_m = st.selectbox("Choose a table for detailed activity analysis", table_options_m, key="monitor_table_select")
        
        if selected_table_m != "Select a table...":
            parts_m = selected_table_m.split(".")
            if len(parts_m) == 3:
                m_db_name, m_schema_name, m_table_name = parts_m
                m_table_fqn = f"{m_db_name}.{m_schema_name}.{m_table_name}"
                
                m_table_info = dq_df[(dq_df["DATABASE_NAME"] == m_db_name) & 
                                   (dq_df["SCHEMA_NAME"] == m_schema_name) & 
                                   (dq_df["TABLE_NAME"] == m_table_name)]
                m_table_info = m_table_info.iloc[0] if len(m_table_info) > 0 else None
                
                if m_table_info is not None:
                    m_tbl_cfg = table_config.get(m_table_fqn, {})
                    m_is_monitored = m_table_info.get("IS_MONITORED", False)
                    m_schema_key = f"{m_db_name}.{m_schema_name}"
                    m_sch_cfg = schema_thresholds.get(m_schema_key, {})
                    
                    def fmt_mins_m(mins):
                        if mins is None: return "‚Äî"
                        mins = int(mins)
                        if mins >= 1440:
                            days = mins / 1440
                            return f"{days:.0f}d" if days == int(days) else f"{days:.1f}d"
                        return f"{mins // 60}h" if mins >= 60 else f"{mins}m"
                    
                    m_schema_warn_mins = m_sch_cfg.get("warn_threshold_minutes", 1440)
                    m_schema_alert_mins = m_sch_cfg.get("alert_threshold_minutes", 2880)
                    m_table_warn_mins = m_tbl_cfg.get("warn_threshold_minutes")
                    m_table_alert_mins = m_tbl_cfg.get("alert_threshold_minutes")
                    m_effective_warn = m_table_warn_mins if m_table_warn_mins else m_schema_warn_mins
                    m_effective_alert = m_table_alert_mins if m_table_alert_mins else m_schema_alert_mins
                    m_has_override = m_table_warn_mins is not None or m_table_alert_mins is not None
                    
                    m_status_parts = ["üîî Monitored" if m_is_monitored else "üîï Not monitored"]
                    m_override_text = " (table override)" if m_has_override else ""
                    st.markdown(f"""
                    <div style="background: #1F2937; padding: 0.75rem 1rem; border-radius: 8px; margin-bottom: 1rem;">
                        <span style="color: #E5E7EB;">{' ‚Ä¢ '.join(m_status_parts)}</span>
                        <span style="color: #6B7280; margin-left: 1rem;">‚îÇ</span>
                        <span style="color: #9CA3AF; margin-left: 1rem;">Thresholds{m_override_text}:</span>
                        <span style="color: #FBBF24; margin-left: 0.5rem;">‚ö†Ô∏è {fmt_mins_m(m_effective_warn)}</span>
                        <span style="color: #6B7280;"> ‚îÇ </span>
                        <span style="color: #F87171;">üö® {fmt_mins_m(m_effective_alert)}</span>
                    </div>
                    """, unsafe_allow_html=True)
                    
                    m_pattern = m_table_info.get("INGEST_PATTERN", "Unknown")
                    
                    if m_pattern in ["CTAS/NEW", "CTAS/SNAP", "EXTERNAL", "BATCH", "EMPTY"]:
                        st.markdown(f"**Pattern: {m_pattern}**")
                        mcol1, mcol2, mcol3, mcol4 = st.columns(4)
                        mcol1.metric("Current Rows", format_metric(m_table_info.get("CURRENT_ROWS")))
                        mcol2.metric("Size", f"{m_table_info['SIZE_GB']:.2f} GB" if pd.notna(m_table_info.get("SIZE_GB")) else "‚Äî")
                        m_created = m_table_info.get("TABLE_CREATED")
                        mcol3.metric("Created", pd.to_datetime(m_created).strftime("%Y-%m-%d") if pd.notna(m_created) else "‚Äî")
                        m_age_days = m_table_info.get("TABLE_AGE_DAYS")
                        mcol4.metric("Age", f"{int(m_age_days)} days" if pd.notna(m_age_days) else "‚Äî")
                    else:
                        mcol1, mcol2, mcol3, mcol4, mcol5 = st.columns(5)
                        mcol1.metric("Pattern", m_pattern)
                        mcol2.metric("Current Rows", format_metric(m_table_info.get("CURRENT_ROWS")))
                        mcol3.metric("Today Inserts", format_metric(m_table_info.get("TODAY_INSERTS"), 0) if pd.notna(m_table_info.get("TODAY_INSERTS")) else "0")
                        mcol4.metric("Avg/Day", format_metric(m_table_info.get("BASELINE_DAILY_INSERTS")))
                        mcol5.metric("% of Avg", format_pct(m_table_info.get("PCT_OF_BASELINE")))
                
                m_history_sql = f"""
                SELECT ACTIVITY_DATE, ROWS_INSERTED, ROWS_UPDATED, ROWS_DELETED,
                    NET_ROW_CHANGE AS NET_CHANGE, WRITE_OPERATIONS AS OPERATIONS, DATA_SOURCES AS LOAD_TYPES
                FROM {VOLUME_DAILY_TABLE_FQN}
                WHERE FQN = '{m_table_fqn.upper().replace("'", "''")}'
                ORDER BY ACTIVITY_DATE
                """
                
                m_hist_df = run_query(m_history_sql)
                
                if not m_hist_df.empty:
                    m_hist_df["ACTIVITY_DATE"] = pd.to_datetime(m_hist_df["ACTIVITY_DATE"])
                    for col in ["ROWS_INSERTED", "ROWS_UPDATED", "ROWS_DELETED", "NET_CHANGE", "OPERATIONS"]:
                        m_hist_df[col] = pd.to_numeric(m_hist_df[col], errors="coerce")
                    
                    m_all_sources = set()
                    for sources in m_hist_df["LOAD_TYPES"].dropna():
                        m_all_sources.update(s.strip() for s in str(sources).split(","))
                    if m_all_sources:
                        st.caption(f"üì° Data sources: {' '.join([f'`{s}`' for s in sorted(m_all_sources)])}")
                    
                    m_chart_col1, m_chart_col2 = st.columns(2)
                    
                    with m_chart_col1:
                        st.markdown("**üìä Daily Inserts Over Time**")
                        m_insert_chart = alt.Chart(m_hist_df).mark_bar(color="#3B82F6").encode(
                            x=alt.X("ACTIVITY_DATE:T", title="Date"),
                            y=alt.Y("ROWS_INSERTED:Q", title="Rows"),
                            tooltip=["ACTIVITY_DATE:T", "ROWS_INSERTED:Q"]
                        ).properties(height=200)
                        st.altair_chart(m_insert_chart, use_container_width=True)
                    
                    with m_chart_col2:
                        st.markdown("**üìà Net Row Change Over Time**")
                        m_net_chart = alt.Chart(m_hist_df).mark_line(color="#10B981", point=True).encode(
                            x=alt.X("ACTIVITY_DATE:T", title="Date"),
                            y=alt.Y("NET_CHANGE:Q", title="Net Change"),
                            tooltip=["ACTIVITY_DATE:T", "NET_CHANGE:Q"]
                        ).properties(height=200)
                        st.altair_chart(m_net_chart, use_container_width=True)
                    
                    with st.expander("üìÑ Raw Activity Data"):
                        st.dataframe(m_hist_df.sort_values("ACTIVITY_DATE", ascending=False), use_container_width=True, hide_index=True)
                else:
                    st.info("No activity history found for this table in the pre-calculated metrics.")
    
    with config_tab:
        # =========================================================================
        # SECTION 2: SCHEMA THRESHOLDS CONFIGURATION
        # =========================================================================
        st.markdown("### ‚öôÔ∏è Schema Thresholds Configuration")
        st.caption("Configure default thresholds per schema. Select a schema to configure table-level overrides.")
        
        # Get existing schema config
        existing_schema_config = get_schema_threshold_config()
        schema_list = dq_df[["DATABASE_NAME", "SCHEMA_NAME"]].drop_duplicates().values.tolist()
        
        # Build editable schema dataframe
        if not existing_schema_config.empty:
            schema_edit_df = existing_schema_config[["DATABASE_NAME", "SCHEMA_NAME", "WARN_THRESHOLD_MINUTES", 
                                                      "ALERT_THRESHOLD_MINUTES", "IS_MONITORED"]].copy()
            # Convert minutes to hours for display/edit
            schema_edit_df["WARN_HOURS"] = (schema_edit_df["WARN_THRESHOLD_MINUTES"] / 60).astype(int)
            schema_edit_df["ALERT_HOURS"] = (schema_edit_df["ALERT_THRESHOLD_MINUTES"] / 60).astype(int)
            schema_edit_df = schema_edit_df[["DATABASE_NAME", "SCHEMA_NAME", "WARN_HOURS", "ALERT_HOURS", "IS_MONITORED"]]
        else:
            # Create empty dataframe with all schemas using defaults
            schema_edit_df = pd.DataFrame(schema_list, columns=["DATABASE_NAME", "SCHEMA_NAME"])
            schema_edit_df["WARN_HOURS"] = 24
            schema_edit_df["ALERT_HOURS"] = 48
            schema_edit_df["IS_MONITORED"] = True
        
        # Rename for display
        schema_edit_df = schema_edit_df.rename(columns={
            "DATABASE_NAME": "Database",
            "SCHEMA_NAME": "Schema",
            "WARN_HOURS": "‚ö†Ô∏è Warn (h)",
            "ALERT_HOURS": "üö® Alert (h)",
            "IS_MONITORED": "Active"
        })
        
        SCHEMA_EDITOR_KEY = "schema_thresholds_editor"
        
        edited_schema_df = st.data_editor(
            schema_edit_df,
            column_config={
                "Database": st.column_config.TextColumn("Database", disabled=True),
                "Schema": st.column_config.TextColumn("Schema", disabled=True),
                "‚ö†Ô∏è Warn (h)": st.column_config.NumberColumn("‚ö†Ô∏è Warn (h)", min_value=1, max_value=720, step=1,
                                                              help="Hours before WARNING alert"),
                "üö® Alert (h)": st.column_config.NumberColumn("üö® Alert (h)", min_value=1, max_value=720, step=1,
                                                              help="Hours before CRITICAL alert"),
                "Active": st.column_config.CheckboxColumn("Active", help="Enable monitoring for this schema"),
            },
            disabled=["Database", "Schema"],
            use_container_width=True,
            hide_index=True,
            key=SCHEMA_EDITOR_KEY,
            num_rows="fixed"
        )
        
        # Save schema changes
        schema_editor_state = st.session_state.get(SCHEMA_EDITOR_KEY, {})
        schema_edited_rows = schema_editor_state.get("edited_rows", {})
        schema_pending = len(schema_edited_rows)
        
        sch_btn_col1, sch_btn_col2, sch_btn_col3 = st.columns([1, 1, 2])
        
        with sch_btn_col1:
            if st.button("üíæ Save Schema Config", type="primary", use_container_width=True, 
                         disabled=schema_pending == 0, key="save_schema_config"):
                saved = 0
                for row_idx_str, changes in schema_edited_rows.items():
                    row_idx = int(row_idx_str)
                    row = schema_edit_df.iloc[row_idx]
                    db_name = row["Database"]
                    schema_name = row["Schema"]
                    
                    # Get current or changed values
                    warn_h = changes.get("‚ö†Ô∏è Warn (h)", row["‚ö†Ô∏è Warn (h)"])
                    alert_h = changes.get("üö® Alert (h)", row["üö® Alert (h)"])
                    is_active = changes.get("Active", row["Active"])
                    
                    # Ensure alert >= warn
                    alert_h = max(alert_h, warn_h)
                    
                    upsert_schema_threshold(db_name, schema_name, warn_h * 60, alert_h * 60, 50, 200, is_active, "")
                    saved += 1
                
                if saved > 0:
                    st.success(f"‚úÖ Saved {saved} schema config(s)!")
                    st.cache_data.clear()
                    time.sleep(1)
                    st.rerun()
        
        with sch_btn_col2:
            if st.button("‚Ü©Ô∏è Discard", use_container_width=True, disabled=schema_pending == 0, key="discard_schema_config"):
                if SCHEMA_EDITOR_KEY in st.session_state:
                    del st.session_state[SCHEMA_EDITOR_KEY]
                st.rerun()
        
        with sch_btn_col3:
            if schema_pending > 0:
                st.warning(f"‚ö†Ô∏è {schema_pending} unsaved schema change(s)")
        
        # Add new schema button
        configured_keys = set(f"{r['Database']}.{r['Schema']}" for _, r in schema_edit_df.iterrows())
        unconfigured = [(db, sch) for db, sch in schema_list if f"{db}.{sch}" not in configured_keys]
        
        if unconfigured:
            st.markdown("---")
            add_col1, add_col2 = st.columns([3, 1])
            with add_col1:
                new_schema = st.selectbox("‚ûï Add schema:", ["Select..."] + [f"{db}.{sch}" for db, sch in unconfigured], 
                                          key="add_new_schema_select")
            with add_col2:
                st.markdown("<br>", unsafe_allow_html=True)
                if st.button("‚ûï Add", type="secondary", use_container_width=True, disabled=new_schema == "Select..."):
                    parts = new_schema.split(".", 1)
                    if len(parts) == 2:
                        upsert_schema_threshold(parts[0], parts[1], 1440, 2880, 50, 200, True, "")
                        st.success(f"‚úÖ Added {new_schema} with defaults (24h warn, 48h alert)")
                        st.cache_data.clear()
                        st.rerun()
        
        # =========================================================================
        # SECTION 3: TABLE THRESHOLDS (when schema selected)
        # =========================================================================
        # Note: table_schema_filter is from the monitoring tab, we need to add schema selector here too
        st.markdown("---")
        st.markdown("### üìã Table Configuration")
        
        # Schema selector for configuration tab
        config_schema_options = sorted(set(f"{r['Database']}.{r['Schema']}" for _, r in schema_edit_df.iterrows()))
        if config_schema_options:
            selected_config_schema = st.selectbox(
                "Select schema to configure tables:",
                ["Select a schema..."] + config_schema_options,
                key="config_schema_selector"
            )
            
            if selected_config_schema != "Select a schema...":
                # Parse selected schema
                config_db, config_schema = selected_config_schema.split(".", 1)
                
                st.caption("Configure monitoring and thresholds per table. Leave threshold blank to use schema defaults.")
                
                # Filter tables for selected schema
                table_filtered_df = dq_df[(dq_df["DATABASE_NAME"] == config_db) & 
                                           (dq_df["SCHEMA_NAME"] == config_schema)].copy()
                
                if table_filtered_df.empty:
                    st.warning(f"""
                        ‚ö†Ô∏è **Schema `{selected_config_schema}` has no data yet.**
                        
                        Click the button below to refresh data for this schema.
                    """)
                    
                    if st.button(f"üîÑ Refresh data for {selected_config_schema}", type="primary", key="config_refresh_btn"):
                        try:
                            with st.spinner("Refreshing..."):
                                config_json = json.dumps({config_db: [config_schema]})
                                run_ddl(f"CALL {CONFIG_DATABASE}.{CONFIG_SCHEMA}.REFRESH_DATA_FRESHNESS_TABLES('{config_json}', 30)")
                                st.success("‚úÖ Data refreshed! Reloading...")
                                st.cache_data.clear()
                                st.rerun()
                        except Exception as e:
                            st.error(f"Error refreshing data: {str(e)[:100]}")
                else:
                    # Get schema defaults for reference
                    schema_key = selected_config_schema
                    sch_cfg = schema_thresholds.get(schema_key, {})
                    schema_warn_h = sch_cfg.get("warn_threshold_minutes", 1440) // 60
                    schema_alert_h = sch_cfg.get("alert_threshold_minutes", 2880) // 60
                    
                    st.caption(f"üìå Schema defaults: ‚ö†Ô∏è Warn = **{schema_warn_h}h** | üö® Alert = **{schema_alert_h}h**")
                    
                    # Filter option
                    filter_col1, filter_col2 = st.columns([1, 3])
                    with filter_col1:
                        show_monitored_only_tables = st.checkbox("üîî Monitored only", value=False, key="table_config_monitored_filter")
                    
                    # Apply filter
                    if show_monitored_only_tables:
                        table_filtered_df = table_filtered_df[table_filtered_df["IS_MONITORED"] == True]
                    
                    if table_filtered_df.empty:
                        if show_monitored_only_tables:
                            st.info("No monitored tables in this schema. Uncheck the filter to see all tables.")
                    else:
                        # Build table config dataframe
                        def get_table_config_row(row):
                            fqn = row["FQN"]
                            tbl_cfg = table_config.get(fqn, {})
                            warn_mins = tbl_cfg.get("warn_threshold_minutes")
                            alert_mins = tbl_cfg.get("alert_threshold_minutes")
                            effective_warn_h = int(warn_mins / 60) if warn_mins else schema_warn_h
                            effective_alert_h = int(alert_mins / 60) if alert_mins else schema_alert_h
                            return pd.Series({
                                "FQN": fqn,
                                "Table": row["TABLE_NAME"],
                                "üîî Monitored": row["IS_MONITORED"],
                                "‚ö†Ô∏è Warn (h)": effective_warn_h,
                                "üö® Alert (h)": effective_alert_h,
                            })
                        
                        table_edit_df = table_filtered_df.apply(get_table_config_row, axis=1)
                        
                        TABLE_EDITOR_KEY = f"table_thresholds_editor_{selected_config_schema}"
                        
                        edited_table_df = st.data_editor(
                            table_edit_df[["Table", "üîî Monitored", "‚ö†Ô∏è Warn (h)", "üö® Alert (h)"]],
                            column_config={
                                "Table": st.column_config.TextColumn("Table", disabled=True),
                                "üîî Monitored": st.column_config.CheckboxColumn("üîî Monitored", help="Enable monitoring"),
                                "‚ö†Ô∏è Warn (h)": st.column_config.NumberColumn("‚ö†Ô∏è Warn (h)", min_value=1, max_value=720, step=1,
                                                                              help="Warn threshold in hours"),
                                "üö® Alert (h)": st.column_config.NumberColumn("üö® Alert (h)", min_value=1, max_value=720, step=1,
                                                                               help="Alert threshold in hours"),
                            },
                            disabled=["Table"],
                            use_container_width=True,
                            hide_index=True,
                            key=TABLE_EDITOR_KEY,
                            num_rows="fixed"
                        )
                        
                        # Save table changes
                        table_editor_state = st.session_state.get(TABLE_EDITOR_KEY, {})
                        table_edited_rows = table_editor_state.get("edited_rows", {})
                        table_pending = len(table_edited_rows)
                        
                        tbl_btn_col1, tbl_btn_col2, tbl_btn_col3 = st.columns([1, 1, 2])
                        
                        with tbl_btn_col1:
                            if st.button("üíæ Save Table Config", type="primary", use_container_width=True, 
                                         disabled=table_pending == 0, key="save_table_config"):
                                saved = 0
                                for row_idx_str, changes in table_edited_rows.items():
                                    row_idx = int(row_idx_str)
                                    fqn = table_edit_df.iloc[row_idx]["FQN"]
                                    orig_row = table_edit_df.iloc[row_idx]
                                    orig_data = table_filtered_df[table_filtered_df["FQN"] == fqn].iloc[0]
                                    
                                    is_mon = changes.get("üîî Monitored", orig_row["üîî Monitored"])
                                    warn_h = changes.get("‚ö†Ô∏è Warn (h)", orig_row["‚ö†Ô∏è Warn (h)"])
                                    alert_h = changes.get("üö® Alert (h)", orig_row["üö® Alert (h)"])
                                    
                                    warn_mins = int(warn_h * 60) if warn_h != schema_warn_h else None
                                    alert_mins = int(alert_h * 60) if alert_h != schema_alert_h else None
                                    
                                    upsert_table_monitor_config(
                                        fqn, orig_data["DATABASE_NAME"], orig_data["SCHEMA_NAME"], orig_data["TABLE_NAME"],
                                        bool(is_mon), warn_mins, alert_mins
                                    )
                                    saved += 1
                                
                                if saved > 0:
                                    st.success(f"‚úÖ Saved {saved} table config(s)!")
                                    st.cache_data.clear()
                                    time.sleep(1)
                                    st.rerun()
                        
                        with tbl_btn_col2:
                            if st.button("‚Ü©Ô∏è Discard", use_container_width=True, disabled=table_pending == 0, key="discard_table_config"):
                                if TABLE_EDITOR_KEY in st.session_state:
                                    del st.session_state[TABLE_EDITOR_KEY]
                                st.rerun()
                        
                        with tbl_btn_col3:
                            if table_pending > 0:
                                st.warning(f"‚ö†Ô∏è {table_pending} unsaved table change(s)")
        else:
            st.info("No schemas configured yet. Add a schema using the options above.")

# =============================================================================
# PIPE HEALTH PAGE - View Snowpipe ingestion status and anomalies
# =============================================================================
if page_choice == "üîß Pipelines":
    render_page_header(
        "üîß", 
        "Pipeline Monitor",
        "View all Snowpipes in your account, activate monitoring, and detect anomalies in ingestion"
    )

    # Initialize config table
    ensure_config_table_exists()
    
    # Get all pipes from the account
    all_account_pipes = list_all_pipes()
    configured_pipes_df = get_configured_pipes()
    
    # Check if materialized table exists (for health data)
    pipe_health_status = check_pipe_health_table_exists()
    has_health_data = pipe_health_status["exists"]

    if all_account_pipes.empty:
        st.warning("‚ö†Ô∏è No pipes found in your account. Ensure you have access to `SNOWFLAKE.ACCOUNT_USAGE.PIPES`.")
        st.stop()

    # Show last refresh time if health data exists
    if has_health_data and pipe_health_status["last_refresh"]:
        refresh_time = pd.to_datetime(pipe_health_status["last_refresh"])
        # Ensure refresh_time is timezone-naive for comparison
        if refresh_time.tzinfo is not None:
            refresh_time = refresh_time.tz_convert(None)
        hours_ago = (pd.Timestamp.utcnow().tz_localize(None) - refresh_time).total_seconds() / 3600
        if hours_ago > 24:
            st.warning(f"‚ö†Ô∏è Health data last refreshed **{hours_ago:.0f} hours ago** ({refresh_time:%Y-%m-%d %H:%M} UTC). Consider running the refresh task.")
        else:
            st.caption(f"üìä Health data refreshed {hours_ago:.1f} hours ago ({refresh_time:%Y-%m-%d %H:%M} UTC)")
    elif not has_health_data:
        st.info(f"‚ÑπÔ∏è Health metrics not yet available. Run `EXECUTE TASK {CONFIG_DATABASE}.{CONFIG_SCHEMA}.PIPE_HEALTH_ALERT_DAILY_TASK;` to populate health data.")

    # Info section
    with st.expander("‚ÑπÔ∏è **How It Works** - Understanding pipe health monitoring", expanded=False):
        st.markdown("""
        ### What This Page Shows
    
        This page monitors your Snowpipes for two types of issues:
    
        | Issue | Description |
        |-------|-------------|
        | **Missing Runs** | A pipe configured to run daily hasn't loaded data today |
        | **Volume Drops** | A pipe loaded significantly less data than expected |
    
        ### Data Sources
    
        | Source | Purpose |
        |--------|---------|
        | `PIPE_HEALTH_METRICS` | Pre-computed pipe health status (refreshed daily) |
        | `PIPE_HEALTH_DAILY` | Historical daily aggregates for trend charts |
        | Configuration Table | Per-pipe thresholds and settings |
    
        ### How Baselines Work
    
        - **Baseline Average**: Calculated from historical median (per weekday)
        - **History Window**: Configured during refresh (default 30 days)
        - **Lookback**: How far back data is scanned (default 45 days)
    
        ### Severity Scores
    
        | Score | Meaning |
        |-------|---------|
        | **5.0** | Critical - Missing or no activity |
        | **3-4** | High - Significant volume drop |
        | **1-2** | Medium - Moderate deviation |
        | **0** | OK - Within normal range |
    
        ### Status Indicators
    
        | Status | Meaning |
        |--------|---------|
        | ‚ö´ No activity | No loads in the lookback period |
        | üî¥ Missing | Expected today but hasn't run |
        | üü† Low volume | Below configured threshold |
        | ‚úÖ OK | Operating normally |
        """)

    st.markdown("---")

    # Build config map for quick lookup
    pipe_config_map = {}
    if not configured_pipes_df.empty:
        for _, row in configured_pipes_df.iterrows():
            pipe_config_map[row["PIPE_NAME"]] = row.to_dict()

    # Sidebar controls (similar to Data Freshness)
    with st.sidebar:
        st.markdown("### üîç Filters")
        
        # Search box for pipe name
        search_text = st.text_input("üîé Search pipes", placeholder="Filter by pipe name...", help="Filter pipes by name")
        
        # Database filter
        available_databases = sorted(all_account_pipes["DATABASE_NAME"].unique().tolist())
        selected_databases = st.multiselect(
            "üóÑÔ∏è Filter by Database",
            options=available_databases,
            default=[],
            help="Select databases to show (empty = all)",
            key="pipe_db_filter"
        )
        
        # Monitored only filter
        show_monitored_only = st.checkbox("Monitored pipes only", value=False, help="Show only pipes marked as monitored")
        
        st.markdown("---")
        st.markdown("### üìä Health Filters")
        
        anomalies_only = st.checkbox("Anomalies only", value=False, help="Show only pipes with issues")
        daily_only = st.checkbox("Daily pipes only", value=False, help="Show only pipes configured to run daily")
        min_severity = st.slider("Min severity", 0.0, 5.0, 0.0, 0.5, help="Filter by minimum severity score")
        show_advanced = st.checkbox("Advanced columns", value=False, help="Show additional metrics")

    # Query health data from materialized table (if available)
    health_df = pd.DataFrame()
    if has_health_data:
        # Query with proper column mapping to match app expectations
        health_sql = f"""
        SELECT 
            PIPE_NAME,
            DATABASE_NAME,
            SCHEMA_NAME,
            -- Date columns
            YESTERDAY_DATE,
            YESTERDAY_DATE AS LOAD_DATE,
            TODAY_DATE,
            -- Yesterday's metrics (mapped to LAST_ for compatibility)
            COALESCE(YESTERDAY_FILES, 0) AS YESTERDAY_FILES,
            COALESCE(YESTERDAY_FILES, 0) AS LAST_FILES,
            COALESCE(YESTERDAY_ROWS, 0) AS YESTERDAY_ROWS,
            COALESCE(YESTERDAY_ROWS, 0) AS LAST_ROWS,
            COALESCE(YESTERDAY_ERRORS, 0) AS YESTERDAY_ERRORS,
            COALESCE(YESTERDAY_ERRORS, 0) AS LAST_ERRORS,
            COALESCE(YESTERDAY_ROWS_PER_FILE, 0) AS YESTERDAY_RPF,
            YESTERDAY_IS_OUTLIER,
            -- Today's metrics
            COALESCE(TODAY_FILES, 0) AS TODAY_FILES,
            COALESCE(TODAY_ROWS, 0) AS TODAY_ROWS,
            CASE WHEN TODAY_FILES > 0 THEN ROUND(TODAY_ROWS / TODAY_FILES, 2) ELSE 0 END AS TODAY_RPF,
            -- Expected/baseline metrics
            COALESCE(EXPECTED_FILES, 0) AS EXPECTED_FILES,
            COALESCE(EXPECTED_ROWS, 0) AS EXPECTED_ROWS,
            COALESCE(EXPECTED_ROWS_PER_FILE, 0) AS EXPECTED_ROWS_PER_FILE,
            -- Stats
            COALESCE(HISTORY_DAYS, 0) AS HISTORY_DAYS,
            P95_LOAD_HOUR,
            -- Deviation percentages
            COALESCE(FILES_SHORT_PCT, 0) AS FILES_SHORT_PCT,
            COALESCE(ROWS_SHORT_PCT, 0) AS ROWS_SHORT_PCT,
            COALESCE(ROWS_PER_FILE_SHORT_PCT, 0) AS RPF_SHORT_PCT,
            REFRESHED_AT
        FROM {PIPE_HEALTH_METRICS_FQN}
        """
        health_df = run_query(health_sql)
        # Remove duplicates (keep first = most recent)
        if not health_df.empty:
            health_df = health_df.drop_duplicates(subset=["PIPE_NAME"], keep="first")

    # Merge all account pipes with health data
    df = all_account_pipes.copy()
    df = df.rename(columns={"FULL_PIPE_NAME": "FQN"})
    
    if not health_df.empty:
        # Merge health data with account pipes using FQN (full qualified name)
        # health_df.PIPE_NAME is fully qualified (DB.SCHEMA.PIPE)
        # df.FQN is also fully qualified (PIPE_CATALOG.PIPE_SCHEMA.PIPE_NAME)
        df = df.merge(health_df, left_on="FQN", right_on="PIPE_NAME", how="left", suffixes=('', '_HEALTH'))
        # Keep the simple PIPE_NAME from all_account_pipes
        if "PIPE_NAME_HEALTH" in df.columns:
            df = df.drop(columns=["PIPE_NAME_HEALTH"])
    
    # Ensure required columns exist with defaults
    required_cols_defaults = {
        "LAST_FILES": 0, "LAST_ROWS": 0, "LAST_ERRORS": 0,
        "YESTERDAY_FILES": 0, "YESTERDAY_ROWS": 0, "YESTERDAY_ERRORS": 0, "YESTERDAY_RPF": 0,
        "TODAY_FILES": 0, "TODAY_ROWS": 0, "TODAY_RPF": 0,
        "EXPECTED_FILES": 0, "EXPECTED_ROWS": 0, "EXPECTED_ROWS_PER_FILE": 0,
        "FILES_SHORT_PCT": 0, "ROWS_SHORT_PCT": 0, "RPF_SHORT_PCT": 0,
        "HISTORY_DAYS": 0, "LOAD_DATE": None, "P95_LOAD_HOUR": None,
        "YESTERDAY_DATE": None, "TODAY_DATE": None, "YESTERDAY_IS_OUTLIER": False
    }
    for col, default in required_cols_defaults.items():
        if col not in df.columns:
            df[col] = default

    # Add IS_MONITORED column from config
    df["IS_MONITORED"] = df["PIPE_NAME"].apply(lambda x: pipe_config_map.get(x, {}).get("IS_MONITORED", False))
    df["RUNS_DAILY"] = df["PIPE_NAME"].apply(lambda x: pipe_config_map.get(x, {}).get("RUNS_DAILY", True) if pipe_config_map.get(x) else True)
    
    # Apply filters
    if selected_databases:
        df = df[df["DATABASE_NAME"].isin(selected_databases)]
    
    if search_text:
        search_upper = search_text.upper()
        df = df[
            df["PIPE_NAME"].str.upper().str.contains(search_upper, na=False) |
            df["DATABASE_NAME"].str.upper().str.contains(search_upper, na=False) |
            df["SCHEMA_NAME"].str.upper().str.contains(search_upper, na=False)
        ]
    
    if show_monitored_only:
        df = df[df["IS_MONITORED"] == True]
    
    if df.empty:
        st.info("No pipes match the current filters. Try adjusting your filter criteria.")
        st.stop()

    # Apply per-pipe config
    # Uses YESTERDAY's data for health checks (complete day), not today's partial data
    def safe_int(val, default=0):
        """Safely convert to int, handling NaN/None values."""
        if val is None or pd.isna(val):
            return default
        try:
            return int(val)
        except (ValueError, TypeError):
            return default
    
    def safe_float(val, default=0.0):
        """Safely convert to float, handling NaN/None values."""
        if val is None or pd.isna(val):
            return default
        try:
            return float(val)
        except (ValueError, TypeError):
            return default
    
    def apply_config(row, config_map):
        cfg = config_map.get(row["PIPE_NAME"], {})
        runs_daily = cfg.get("RUNS_DAILY", True)
        if pd.isna(runs_daily): runs_daily = True
        threshold = safe_int(cfg.get("VOLUME_THRESHOLD_PCT", 50), 50)
    
        # Use YESTERDAY for health status (most recent COMPLETE 24h period)
        # With rolling windows, YESTERDAY_DATE = end date of the most recent complete period
        yesterday_date = row.get("YESTERDAY_DATE")
        has_yesterday_data = pd.notna(yesterday_date)
    
        # Stale check: YESTERDAY_DATE should be within the last 2 days
        # A pipe is stale if its last complete data is MORE than 1 day old
        # Using UTC for consistency with Snowflake timestamps
        if has_yesterday_data:
            yesterday_dt = pd.to_datetime(yesterday_date).date()
            today_utc = pd.Timestamp.utcnow().date()
            days_since_data = (today_utc - yesterday_dt).days
            # Stale if data is 2+ days old (yesterday_date is before yesterday)
            data_is_stale = days_since_data >= 2
        else:
            data_is_stale = True
    
        expected = 1 if runs_daily else 0
        missing = 1 if (runs_daily and data_is_stale and cfg.get("ALERT_ON_MISSING", True)) else 0
    
        # Check for MISSING_TODAY: Pipe ran yesterday but hasn't run today yet
        # This catches early-morning streams that should have loaded by now
        today_files = safe_int(row.get("TODAY_FILES"))
        exp_files = safe_float(row.get("EXPECTED_FILES"))
        p95_hour = row.get("P95_HOUR")
        history_days = safe_int(row.get("HISTORY_DAYS"))
        # Use UTC hour for consistency (Snowflake typically uses UTC)
        current_hour = pd.Timestamp.utcnow().hour
    
        missing_today = 0
        if runs_daily and cfg.get("ALERT_ON_MISSING", True) and has_yesterday_data and not data_is_stale:
            # Yesterday data exists and is not stale, but check if TODAY is missing
            if today_files == 0 and exp_files > 0:
                # Check if we're past the time when this pipe usually completes
                if p95_hour is not None and pd.notna(p95_hour):
                    p95_int = safe_int(p95_hour)
                    # If current hour > P95 hour + 1, pipe should have completed
                    if current_hour >= p95_int + 1:
                        missing_today = 1
                    # Or if pipe typically loads early (by 6 AM) and it's now 7+ AM
                    elif p95_int <= 6 and current_hour >= 7:
                        missing_today = 1
                elif history_days >= 7:
                    # No P95 data but has enough history, flag if no files today
                    missing_today = 1
    
        # Use yesterday's metrics for volume comparison
        exp_rpf = safe_float(row.get("EXPECTED_ROWS_PER_FILE"))
        act_files = safe_int(row.get("YESTERDAY_FILES"))
        act_rpf = safe_float(row.get("YESTERDAY_RPF"))
    
        vol_low = 0
        if cfg.get("ALERT_ON_VOLUME_DROP", True) and has_yesterday_data:
            # Use rows-per-file for volume check (more stable metric)
            if (exp_files > 0 and act_files <= threshold/100 * exp_files) or \
               (exp_rpf > 0 and act_rpf <= threshold/100 * exp_rpf):
                vol_low = 1
    
        # Check for errors
        errors = safe_int(row.get("YESTERDAY_ERRORS"))
        has_errors = 1 if errors > 0 else 0
    
        # Severity based on yesterday's deviation
        if not has_yesterday_data: 
            severity = 5.0
        elif missing: 
            severity = 5.0
        elif missing_today:
            severity = 5.0  # Missing today is critical
        elif has_errors:
            severity = 4.0  # Errors are high severity
        else: 
            files_short = safe_float(row.get("FILES_SHORT_PCT"))
            rpf_short = safe_float(row.get("RPF_SHORT_PCT"))
            severity = round(max(files_short, rpf_short) / 20, 2)
    
        anomaly = 1 if (missing or missing_today or vol_low or has_errors or not has_yesterday_data) else 0
    
        return pd.Series({"EXPECTED_TODAY": expected, "MISSING_FLAG": missing, "MISSING_TODAY_FLAG": missing_today,
                          "VOL_LOW_FLAG": vol_low, "ERRORS_FLAG": has_errors, "ERRORS_COUNT": errors,
                          "SEVERITY": severity, "ANOMALY": anomaly, "THRESHOLD_PCT": threshold})

    config_cols = df.apply(lambda r: apply_config(r, pipe_config_map), axis=1)
    df = pd.concat([df, config_cols], axis=1)

    # Process for display
    df["LOAD_DATE"] = pd.to_datetime(df["LOAD_DATE"], errors="coerce").dt.date
    df["YESTERDAY_DATE"] = pd.to_datetime(df["YESTERDAY_DATE"], errors="coerce").dt.date
    df["TODAY_DATE"] = pd.to_datetime(df["TODAY_DATE"], errors="coerce").dt.date

    # Status is based on YESTERDAY's data (most recent complete 24h period)
    # MISSING_TODAY catches pipes that ran yesterday but haven't run today yet
    df["STATUS"] = np.where(df["YESTERDAY_DATE"].isna(), "‚ö´ No data",
                  np.where(df["MISSING_FLAG"]==1, "üî¥ Stale data",
                  np.where(df["MISSING_TODAY_FLAG"]==1, "üü£ No run today",
                  np.where(df["ERRORS_FLAG"]==1, "üü° Errors",
                  np.where(df["VOL_LOW_FLAG"]==1, "üü† Low volume", "‚úÖ OK")))))

    # Separate monitored vs all pipes for metrics
    monitored_df = df[df["IS_MONITORED"] == True]
    
    # Calculate metrics for monitored pipes only
    no_activity_monitored = int(monitored_df["YESTERDAY_DATE"].isna().sum()) if not monitored_df.empty else 0
    stale_count_monitored = int((monitored_df["MISSING_FLAG"] == 1).sum()) if not monitored_df.empty else 0
    anomaly_count_monitored = int((monitored_df["ANOMALY"] == 1).sum()) if not monitored_df.empty else 0

    # Metrics
    render_section_header("üìä", "Monitoring Overview")
    
    # Summary counts
    total_pipes = len(df)
    monitored_count = len(monitored_df)
    
    # Quick summary message (based on monitored pipes)
    if monitored_count == 0:
        st.info("‚ÑπÔ∏è No pipes are being monitored yet. Enable monitoring using the checkboxes below.")
    elif anomaly_count_monitored == 0:
        render_alert("success", f"All <strong>{monitored_count} monitored pipelines</strong> are healthy! No issues detected.", "‚úÖ")
    elif anomaly_count_monitored <= 2:
        render_alert("warning", f"<strong>{anomaly_count_monitored}</strong> of {monitored_count} monitored pipeline(s) need attention.", "‚ö†Ô∏è")
    else:
        render_alert("error", f"<strong>{anomaly_count_monitored}</strong> of {monitored_count} monitored pipelines have issues!", "üö®")
    
    col1, col2, col3, col4, col5, col6 = st.columns(6)
    col1.metric("Total in Account", total_pipes, help="All pipes in your Snowflake account")
    col2.metric("üîî Monitored", monitored_count, help="Pipes with monitoring enabled")
    col3.metric("‚ö†Ô∏è Anomalies", anomaly_count_monitored, help="Monitored pipes with issues")
    col4.metric("‚ö´ No Activity", no_activity_monitored, help="Monitored pipes with no recent data")
    col5.metric("üî¥ Stale", stale_count_monitored, help="Monitored pipes with stale data")
    col6.metric("üü† Low Vol/Errors", int((monitored_df["VOL_LOW_FLAG"] == 1).sum()) + int((monitored_df["ERRORS_FLAG"] == 1).sum()) if not monitored_df.empty else 0, help="Volume or error issues")
    
    if has_health_data:
        st.caption("‚ÑπÔ∏è Health status is based on the most recent COMPLETE 24h period (rolling window)")

    # Apply filters
    _df = df.copy()
    _df["_DT"] = pd.to_datetime(_df["LOAD_DATE"], errors="coerce")

    valid_dates = _df["LOAD_DATE"].dropna()
    if not valid_dates.empty:
        dmin, dmax = valid_dates.min(), valid_dates.max()
    else:
        dmin, dmax = date.today(), date.today()

    # Filter in sidebar
    with st.sidebar:
        st.markdown("---")
        date_range = st.date_input("üìÖ Date range", value=(dmin, dmax))

    mask = _df["LOAD_DATE"].isna() | ((_df["_DT"] >= pd.to_datetime(date_range[0])) & (_df["_DT"] <= pd.to_datetime(date_range[1])))
    if anomalies_only: mask &= (_df["ANOMALY"] == 1)
    if daily_only: mask &= (_df["EXPECTED_TODAY"] == 1)
    if min_severity > 0: mask &= (_df["SEVERITY"].fillna(0) >= min_severity)

    fdf = _df.loc[mask].copy()

    # Table
    render_section_header("üìã", "Pipeline Details")
    
    # Inline filters (similar to Data Freshness)
    pipe_filter_col1, pipe_filter_col2, pipe_filter_col3, pipe_filter_col4 = st.columns([1, 1, 1, 1])
    
    with pipe_filter_col1:
        # Database filter first
        pipe_db_options = ["All Databases"] + sorted(fdf["DATABASE_NAME"].dropna().unique().tolist())
        pipe_db_filter = st.selectbox(
            "üóÑÔ∏è Database",
            pipe_db_options,
            key="pipe_details_db_filter"
        )
    
    with pipe_filter_col2:
        pipe_schema_filter = st.selectbox(
            "üìÅ Schema",
            ["All Schemas"] + sorted(fdf["SCHEMA_NAME"].dropna().unique().tolist()),
            key="pipe_details_schema_filter"
        )
    
    with pipe_filter_col3:
        # Status filter options
        pipe_status_options = [
            "All Statuses", 
            "‚ö†Ô∏è Issues Only",
            "‚ö´ No Data",
            "üî¥ Stale Data",
            "üü£ No Run Today",
            "üü† Low Volume",
            "üü° Errors",
            "‚úÖ OK Only"
        ]
        pipe_status_filter = st.selectbox(
            "üìä Status",
            pipe_status_options,
            key="pipe_details_status_filter"
        )
    
    with pipe_filter_col4:
        # Monitoring filter
        pipe_monitor_options = ["All Pipes", "üîî Monitored", "üîï Not Monitored"]
        pipe_monitor_filter = st.selectbox(
            "üîî Monitoring",
            pipe_monitor_options,
            key="pipe_details_monitor_filter"
        )
    
    # Apply inline filters to fdf
    if pipe_db_filter != "All Databases":
        fdf = fdf[fdf["DATABASE_NAME"] == pipe_db_filter]
    
    if pipe_schema_filter != "All Schemas":
        fdf = fdf[fdf["SCHEMA_NAME"] == pipe_schema_filter]
    
    if pipe_monitor_filter == "üîî Monitored":
        fdf = fdf[fdf["IS_MONITORED"] == True]
    elif pipe_monitor_filter == "üîï Not Monitored":
        fdf = fdf[fdf["IS_MONITORED"] == False]
    
    if pipe_status_filter == "‚ö†Ô∏è Issues Only":
        fdf = fdf[fdf["ANOMALY"] == 1]
    elif pipe_status_filter == "‚ö´ No Data":
        fdf = fdf[fdf["STATUS"] == "‚ö´ No data"]
    elif pipe_status_filter == "üî¥ Stale Data":
        fdf = fdf[fdf["STATUS"] == "üî¥ Stale data"]
    elif pipe_status_filter == "üü£ No Run Today":
        fdf = fdf[fdf["STATUS"] == "üü£ No run today"]
    elif pipe_status_filter == "üü† Low Volume":
        fdf = fdf[fdf["STATUS"] == "üü† Low volume"]
    elif pipe_status_filter == "üü° Errors":
        fdf = fdf[fdf["STATUS"] == "üü° Errors"]
    elif pipe_status_filter == "‚úÖ OK Only":
        fdf = fdf[fdf["STATUS"] == "‚úÖ OK"]

    # Glossary expander
    with st.expander("üìñ **Column Glossary** - What each column means", expanded=False):
        st.markdown("""
        ### Base Columns
    
        | Column | Description |
        |--------|-------------|
        | **‚ö†Ô∏è** | Anomaly flag: `1` = issue detected, `0` = normal |
        | **Severity** | Score from 0-5. Higher = more severe. `5.0` = critical (missing/stale/no run today), `4.0` = errors |
        | **Status** | Health status: ‚úÖ OK, üü† Low volume, üü° Errors, üü£ No run today, üî¥ Stale data, ‚ö´ No data |
        | **Pipe** | Full pipe name (DATABASE.SCHEMA.PIPE_NAME) |
        | **Last Load** | Timestamp of the most recent file load (displayed in US Eastern Time) |
        | **Daily** | `1` = pipe is configured to run daily, `0` = not daily |
        | **Y.Files** | Files loaded on the last complete day |
        | **Avg.Files** | Average files based on historical baseline (median for that day-of-week) |
        | **Files%** | Percentage: (Y.Files / Exp.Files) √ó 100. Below 100% = fewer files than expected |
        | **Y.R/F** | Rows per file on the last complete day (total rows √∑ files) |
        | **Avg.R/F** | Average rows per file based on historical baseline |
        | **R/F%** | Percentage: (Y.R/F / Avg.R/F) √ó 100. More stable than raw row counts |
        | **Errors** | Number of load errors on the last complete day. `> 0` triggers üü° Errors status |
        | **Thresh** | Configured alert threshold (%). Alert triggers if actual < threshold% of expected |
        | **Database** | Source database name |
        | **Schema** | Source schema name |
    
        ### Advanced Columns (toggle "Advanced columns" in sidebar)
    
        | Column | Description |
        |--------|-------------|
        | **Today** | Today's date (partial/incomplete data) |
        | **T.Files** | Files loaded today so far (partial - day not complete) |
        | **T.Rows** | Rows loaded today so far (partial) |
        | **T.R/F** | Today's rows per file (partial) |
        | **Y.Rows** | Total rows loaded on the last complete day |
        | **Y.Errors** | Error count on the last complete day |
        | **Files Z** | Z-score for files: how many standard deviations from mean. `|Z| > 2` = unusual |
        | **R/F Z** | Z-score for rows per file |
        | **P95 Hr** | 95th percentile load hour - when the pipe typically finishes loading |
        | **Hist** | Total history days available for baseline calculation |
        | **Clean** | History days excluding outliers (used for baseline) |
        | **Y.Outlier?** | Was the last complete day flagged as an outlier (reload/backfill)? |
    
        ### Understanding "Last Load" vs Health Status
    
        - **Last Load** shows the actual timestamp when files were last processed
        - Health status is based on whether the pipe has **complete daily data** for yesterday
        - A pipe can have a recent "Last Load" but still be flagged if yesterday's data is incomplete
    
        ### Understanding "üü£ No Run Today"
    
        This status catches pipes that:
        - Ran successfully **yesterday** (Last Complete Day is recent)
        - But have **0 files today** when they should have data by now
        - Based on **P95 Load Hour**: if the pipe usually completes by 5 AM and it's now 7 AM with no data, alert!
    
        **Use case:** Early-morning streaming/batch jobs that typically complete before 7 AM.
    
        ### Why Rows Per File (R/F)?
    
        Rows per file is a more **stable metric** than total rows because:
        - Total rows fluctuate with file count (more files = more rows)
        - R/F stays consistent even if file counts vary
        - Drops in R/F often indicate data quality issues (truncated files, schema changes)
        """)

    # Show outlier count if any
    outlier_count = int(fdf["YESTERDAY_IS_OUTLIER"].sum()) if "YESTERDAY_IS_OUTLIER" in fdf.columns else 0
    if outlier_count > 0:
        st.warning(f"‚ö†Ô∏è **{outlier_count} pipe(s)** yesterday were flagged as outliers (reload/backfill)")

    if fdf.empty:
        st.info("No pipes match the current filters.")
    else:
        # Calculate percentages using YESTERDAY's data
        fdf["FILES_PCT"] = np.where(fdf["EXPECTED_FILES"]>0, 100*fdf["YESTERDAY_FILES"]/fdf["EXPECTED_FILES"], np.nan)
        fdf["RPF_PCT"] = np.where(fdf["EXPECTED_ROWS_PER_FILE"]>0, 100*fdf["YESTERDAY_RPF"]/fdf["EXPECTED_ROWS_PER_FILE"], np.nan)
    
        # Format REFRESHED_AT timestamp for display (when data was last refreshed)
        if "REFRESHED_AT" in fdf.columns:
            fdf["LAST_TS_FMT"] = fdf["REFRESHED_AT"].apply(format_timestamp_est)
        else:
            fdf["LAST_TS_FMT"] = "‚Äî"
    
        # Remove duplicates based on PIPE_NAME (keep first occurrence, which has highest severity after sorting)
        fdf = fdf.drop_duplicates(subset=["PIPE_NAME"], keep="first")
    
        fdf = fdf.sort_values(["SEVERITY", "YESTERDAY_DATE"], ascending=[False, False], na_position="first")
        
        st.caption(f"Showing **{len(fdf)}** pipes ‚Ä¢ Toggle üîî to enable/disable monitoring")
    
        # Build editable dataframe with IS_MONITORED checkbox (like Data Freshness)
        visible_cols = ["IS_MONITORED", "STATUS", "PIPE_NAME", "DATABASE_NAME", "SCHEMA_NAME"]
        
        if has_health_data:
            visible_cols += ["LAST_TS_FMT", "YESTERDAY_FILES", "EXPECTED_FILES", "FILES_PCT", "ERRORS_COUNT"]
        
        # Advanced columns
        if show_advanced and has_health_data: 
            visible_cols += ["TODAY_FILES", "TODAY_ROWS", "HISTORY_DAYS"]
        
        all_cols = [c for c in visible_cols if c in fdf.columns]
        
        # Prepare editable dataframe
        edit_df = fdf[all_cols].copy()
        
        # Add internal tracking column (copy of PIPE_NAME for save logic)
        edit_df["_pipe_name_internal"] = fdf["PIPE_NAME"].values
        
        # Format numeric columns
        if "YESTERDAY_FILES" in edit_df.columns:
            edit_df["YESTERDAY_FILES"] = edit_df["YESTERDAY_FILES"].apply(lambda x: format_thousands(x) if pd.notna(x) else "‚Äî")
        if "EXPECTED_FILES" in edit_df.columns:
            edit_df["EXPECTED_FILES"] = edit_df["EXPECTED_FILES"].apply(lambda x: format_thousands(x) if pd.notna(x) else "‚Äî")
        if "FILES_PCT" in edit_df.columns:
            edit_df["FILES_PCT"] = edit_df["FILES_PCT"].apply(format_pct)
        if "ERRORS_COUNT" in edit_df.columns:
            edit_df["ERRORS_COUNT"] = edit_df["ERRORS_COUNT"].apply(lambda x: int(x) if pd.notna(x) else 0)
        if "TODAY_FILES" in edit_df.columns:
            edit_df["TODAY_FILES"] = edit_df["TODAY_FILES"].apply(lambda x: format_thousands(x) if pd.notna(x) else "‚Äî")
        if "TODAY_ROWS" in edit_df.columns:
            edit_df["TODAY_ROWS"] = edit_df["TODAY_ROWS"].apply(lambda x: format_thousands(x) if pd.notna(x) else "‚Äî")
        if "HISTORY_DAYS" in edit_df.columns:
            edit_df["HISTORY_DAYS"] = edit_df["HISTORY_DAYS"].apply(lambda x: int(x) if pd.notna(x) else 0)
        
        # Rename columns for display
        rename_map = {
            "PIPE_NAME": "Pipe",
            "IS_MONITORED": "üîî",
            "STATUS": "Status",
            "DATABASE_NAME": "Database",
            "SCHEMA_NAME": "Schema",
            "LAST_TS_FMT": "Last Load",
            "YESTERDAY_FILES": "Y.Files",
            "EXPECTED_FILES": "Exp.Files",
            "FILES_PCT": "Files%",
            "ERRORS_COUNT": "Errors",
            "TODAY_FILES": "T.Files",
            "TODAY_ROWS": "T.Rows",
            "HISTORY_DAYS": "History"
        }
        edit_df = edit_df.rename(columns=rename_map)
        
        # Use data_editor for interactive checkboxes
        if isinstance(edit_df, pd.DataFrame) and not edit_df.empty:
            # Remove internal column before display
            display_cols = [c for c in edit_df.columns if c != "_pipe_name_internal"]
            
            # Define which columns can be edited (only the checkbox)
            disabled_cols = [c for c in display_cols if c != "üîî"]
            
            edited_df = st.data_editor(
                edit_df[display_cols],
                disabled=disabled_cols,
                use_container_width=True,
                hide_index=True,
                key="pipe_details_editor"
            )
            # Add back the _pipe_name column for tracking
            edited_df["_pipe_name_internal"] = edit_df["_pipe_name_internal"].values
        else:
            st.warning("No data to display")
            edited_df = edit_df
        
        # Compare original vs edited to detect changes
        changes_made = []
        for idx in range(len(edited_df)):
            pipe_name = edit_df.iloc[idx]["_pipe_name_internal"]
            new_mon = edited_df.iloc[idx]["üîî"]
            orig_mon = edit_df.iloc[idx]["üîî"]
            if new_mon != orig_mon:
                # Get pipe info for adding to config
                pipe_row = fdf[fdf["PIPE_NAME"] == pipe_name].iloc[0]
                changes_made.append((pipe_name, new_mon, pipe_row["DATABASE_NAME"], pipe_row["SCHEMA_NAME"]))
        
        # Save button and download
        col1, col2, col3 = st.columns([1, 1, 2])
        with col1:
            if changes_made:
                if st.button(f"üíæ Save {len(changes_made)} Change(s)", type="primary", use_container_width=True):
                    for pipe_name, new_mon, db_name, schema_name in changes_made:
                        if new_mon:
                            # Add pipe to config if not exists, or update if exists
                            existing = pipe_config_map.get(pipe_name)
                            if existing:
                                thresh_val = existing.get("VOLUME_THRESHOLD_PCT", 50)
                                thresh_val = 50 if thresh_val is None or pd.isna(thresh_val) else int(thresh_val)
                                update_pipe_config(pipe_name, True, existing.get("RUNS_DAILY", True), 
                                                   existing.get("ALERT_ON_MISSING", True), 
                                                   existing.get("ALERT_ON_VOLUME_DROP", True),
                                                   thresh_val, 
                                                   existing.get("NOTES", "") or "")
                            else:
                                add_pipe_to_config(pipe_name, db_name, schema_name, "")
                        else:
                            # Update to not monitored
                            existing = pipe_config_map.get(pipe_name)
                            if existing:
                                thresh_val = existing.get("VOLUME_THRESHOLD_PCT", 50)
                                thresh_val = 50 if thresh_val is None or pd.isna(thresh_val) else int(thresh_val)
                                update_pipe_config(pipe_name, False, existing.get("RUNS_DAILY", True),
                                                   existing.get("ALERT_ON_MISSING", True),
                                                   existing.get("ALERT_ON_VOLUME_DROP", True),
                                                   thresh_val,
                                                   existing.get("NOTES", "") or "")
                            else:
                                # Add as not monitored
                                add_pipe_to_config(pipe_name, db_name, schema_name, "")
                                update_pipe_config(pipe_name, False, True, True, True, 50, "")
                    st.success(f"‚úÖ Updated {len(changes_made)} pipe(s)!")
                    st.cache_data.clear()
                    st.rerun()
            else:
                st.button("üíæ Save Changes", disabled=True, use_container_width=True)
        with col2:
            # Export - use original fdf for full data
            export_cols = ["PIPE_NAME", "DATABASE_NAME", "SCHEMA_NAME", "IS_MONITORED", "STATUS", 
                           "YESTERDAY_FILES", "EXPECTED_FILES", "ERRORS_COUNT"]
            export_cols = [c for c in export_cols if c in fdf.columns]
            st.download_button("üì• Download CSV", fdf[export_cols].to_csv(index=False).encode(), "pipelines_report.csv", "text/csv", use_container_width=True)

    # =========================================================================
    # PIPE CONFIGURATION (Collapsible)
    # =========================================================================
    st.markdown("---")
    
    with st.expander("‚öôÔ∏è **Configure Monitored Pipes** ‚Äî Set thresholds and alert settings", expanded=False):
        st.caption("Configure individual pipe settings. Changes apply to pipes already marked as monitored.")
        
        # Get list of monitored pipes
        monitored_pipes_list = df[df["IS_MONITORED"] == True]["PIPE_NAME"].tolist()
        
        if not monitored_pipes_list:
            st.info("üì≠ No pipes are being monitored yet. Enable monitoring using the üîî checkboxes in the table above.")
        else:
            # Show configured pipes summary
            st.markdown(f"**üìã Monitored Pipes: {len(monitored_pipes_list)}**")
            
            # Select pipe to configure
            config_pipe = st.selectbox(
                "Select pipe to configure:",
                ["Choose a pipe..."] + sorted(monitored_pipes_list),
                key="config_pipe_select"
            )
            
            if config_pipe != "Choose a pipe...":
                # Get current config
                current_cfg = pipe_config_map.get(config_pipe, {})
                pipe_row = df[df["PIPE_NAME"] == config_pipe].iloc[0]
                
                st.caption(f"üìÅ `{pipe_row['DATABASE_NAME']}.{pipe_row['SCHEMA_NAME']}`")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    st.markdown("**Monitoring Settings**")
                    cfg_runs_daily = st.checkbox(
                        "Runs daily", 
                        value=bool(current_cfg.get("RUNS_DAILY", True) if current_cfg.get("RUNS_DAILY") is not None else True),
                        key=f"cfg_daily_{config_pipe}",
                        help="Should this pipe run every day?"
                    )
                    cfg_alert_missing = st.checkbox(
                        "Alert if missing", 
                        value=bool(current_cfg.get("ALERT_ON_MISSING", True) if current_cfg.get("ALERT_ON_MISSING") is not None else True),
                        key=f"cfg_miss_{config_pipe}",
                        help="Alert when daily pipe hasn't run"
                    )
                
                with col2:
                    st.markdown("**Volume Settings**")
                    cfg_alert_volume = st.checkbox(
                        "Alert on low volume", 
                        value=bool(current_cfg.get("ALERT_ON_VOLUME_DROP", True) if current_cfg.get("ALERT_ON_VOLUME_DROP") is not None else True),
                        key=f"cfg_vol_{config_pipe}",
                        help="Alert when volume drops below threshold"
                    )
                    cfg_thresh_val = current_cfg.get("VOLUME_THRESHOLD_PCT", 50)
                    cfg_thresh_val = 50 if cfg_thresh_val is None or pd.isna(cfg_thresh_val) else int(cfg_thresh_val)
                    cfg_threshold = st.slider(
                        "Volume threshold %", 
                        10, 90, 
                        cfg_thresh_val,
                        key=f"cfg_thresh_{config_pipe}",
                        help="Flag if below this % of expected"
                    )
                
                cfg_notes = st.text_area(
                    "Notes", 
                    value=current_cfg.get("NOTES", "") or "",
                    key=f"cfg_notes_{config_pipe}",
                    height=68,
                    placeholder="Add notes about this pipe..."
                )
                
                btn_col1, btn_col2, btn_col3 = st.columns([1, 1, 2])
                with btn_col1:
                    if st.button("üíæ Save Config", key=f"cfg_save_{config_pipe}", type="primary", use_container_width=True):
                        update_pipe_config(config_pipe, True, cfg_runs_daily, cfg_alert_missing, cfg_alert_volume, cfg_threshold, cfg_notes)
                        st.success("‚úÖ Saved!")
                        st.cache_data.clear()
                        st.rerun()
                with btn_col2:
                    if st.button("üîï Disable Monitoring", key=f"cfg_disable_{config_pipe}", type="secondary", use_container_width=True):
                        update_pipe_config(config_pipe, False, cfg_runs_daily, cfg_alert_missing, cfg_alert_volume, cfg_threshold, cfg_notes)
                        st.success("‚úÖ Monitoring disabled")
                        st.cache_data.clear()
                        st.rerun()
            
            # Bulk actions
            st.markdown("---")
            st.markdown("**üîß Bulk Actions:**")
            bulk_col1, bulk_col2, bulk_col3 = st.columns(3)
            with bulk_col1:
                if configured_pipes_df is not None and not configured_pipes_df.empty:
                    st.download_button(
                        "üì• Export Config", 
                        configured_pipes_df.to_csv(index=False).encode(), 
                        "pipe_config.csv", 
                        "text/csv", 
                        use_container_width=True
                    )
            with bulk_col2:
                if st.button("üîÑ Refresh Data", use_container_width=True, key="bulk_refresh_pipes"):
                    st.cache_data.clear()
                    st.rerun()
            with bulk_col3:
                if st.button("üóëÔ∏è Remove All Config", type="secondary", use_container_width=True, key="bulk_remove_pipes"):
                    run_ddl(f"TRUNCATE TABLE {CONFIG_TABLE_FQN}")
                    st.success("‚úÖ All pipe configurations removed!")
                    st.cache_data.clear()
                    st.rerun()

    # Drilldown section
    st.markdown("---")
    render_section_header("üîç", "Deep Dive into a Pipeline")
    st.caption("Select a pipeline to see detailed history and performance metrics")

    pipe_options = ["Select a pipe..."] + sorted(df["PIPE_NAME"].unique().tolist())
    selected_pipe = st.selectbox("Choose a pipe for detailed history", pipe_options)

    if selected_pipe != "Select a pipe...":
        # Get pipe info from main dataframe (materialized data includes today)
        pipe_info = df[df["PIPE_NAME"] == selected_pipe]
        if not pipe_info.empty:
            pipe_info = pipe_info.iloc[0]
        
            # Show if yesterday was an outlier day
            if pipe_info.get("YESTERDAY_IS_OUTLIER", False):
                st.warning(f"‚ö†Ô∏è **Yesterday was an outlier** (possible reload/backfill) - excluded from baseline")
        
            # Show comparison: Today (partial) vs Yesterday (complete) vs Baseline Avg
            st.markdown("#### üìä Today vs Yesterday vs Baseline Avg")
        
            col1, col2, col3, col4 = st.columns(4)
            col1.markdown("**Metric**")
            col2.markdown("**Today** *(partial)*")
            col3.markdown("**Yesterday** *(complete)*")
            col4.markdown("**Baseline Avg**")
        
            col1, col2, col3, col4 = st.columns(4)
            col1.markdown("Files")
            col2.metric("", format_thousands(pipe_info.get("TODAY_FILES")), label_visibility="collapsed")
            col3.metric("", format_thousands(pipe_info.get("YESTERDAY_FILES")), label_visibility="collapsed")
            col4.metric("", format_thousands(pipe_info.get("EXPECTED_FILES")), label_visibility="collapsed")
        
            col1, col2, col3, col4 = st.columns(4)
            col1.markdown("Rows/File")
            col2.metric("", format_metric(pipe_info.get("TODAY_RPF")), label_visibility="collapsed")
            col3.metric("", format_metric(pipe_info.get("YESTERDAY_RPF")), label_visibility="collapsed")
            col4.metric("", format_metric(pipe_info.get("EXPECTED_ROWS_PER_FILE")), label_visibility="collapsed")
        
            # Status and health info
            st.markdown("---")
            col1, col2, col3, col4 = st.columns(4)
            col1.metric("Health Status", pipe_info.get("STATUS", "‚Äî"), help="Based on last complete day's data")
            # Show refreshed timestamp in EST - use shorter format to fit in metric card
            col2.metric("Data Updated", format_timestamp_est(pipe_info.get("REFRESHED_AT"), fmt="%m/%d %H:%M"))
            col3.metric("Files vs Avg", f"{pipe_info.get('FILES_SHORT_PCT', 0):.0f}% short" if pipe_info.get('FILES_SHORT_PCT', 0) > 0 else "OK")
            col4.metric("History Days", f"{int(pipe_info.get('HISTORY_DAYS', 0))} days" if pd.notna(pipe_info.get("HISTORY_DAYS")) else "‚Äî")
    
        # Query from history table
        hist_sql = f"""
        SELECT DISTINCT
            LOAD_DATE,
            FILES_LOADED AS FILES,
            ROWS_LOADED,
            COALESCE(AVG_ROWS_PER_FILE, 0) AS ROWS_PER_FILE,
            COALESCE(ERRORS, 0) AS ERRORS,
            1 AS LOAD_COUNT,
            FALSE AS IS_OUTLIER,
            NULL AS OUTLIER_REASON,
            CASE WHEN LOAD_DATE = CURRENT_DATE() THEN TRUE ELSE FALSE END AS IS_TODAY
        FROM {PIPE_HEALTH_DAILY_FQN}
        WHERE PIPE_NAME = '{selected_pipe.replace("'", "''")}'
        ORDER BY LOAD_DATE
        """
        hist_df = run_query(hist_sql)
    
        if not hist_df.empty:
            hist_df["LOAD_DATE"] = pd.to_datetime(hist_df["LOAD_DATE"])
        
            # Show outlier days count + today data note
            has_today = hist_df["IS_TODAY"].any()
            outlier_days = hist_df["IS_OUTLIER"].sum()
            caption_parts = [f"üìä {len(hist_df)} days of history"]
            if has_today:
                caption_parts.append("includes today üîÑ")
            if outlier_days > 0:
                caption_parts.append(f"{outlier_days} outlier day(s) excluded from baseline")
            st.caption(" ‚Ä¢ ".join(caption_parts))
        
            col1, col2 = st.columns(2)
            with col1:
                st.markdown("**üìÅ Files per Day**")
                st.line_chart(hist_df.set_index("LOAD_DATE")["FILES"])
            with col2:
                # Show Rows/File instead of total rows (more stable)
                st.markdown("**üìä Rows per File** *(more stable metric)*")
                st.line_chart(hist_df.set_index("LOAD_DATE")["ROWS_PER_FILE"])
        
            # Show daily stats table (includes today's partial data)
            st.markdown("**üìã Daily Load Summary** *(includes today)*")
            daily_display = hist_df.sort_values("LOAD_DATE", ascending=False).copy()
            # Mark today's row with indicator
            daily_display["Date"] = daily_display.apply(
                lambda r: r["LOAD_DATE"].strftime("%Y-%m-%d") + " üîÑ" if r.get("IS_TODAY", False) else r["LOAD_DATE"].strftime("%Y-%m-%d"), 
                axis=1
            )
            daily_display["ROWS_LOADED"] = daily_display["ROWS_LOADED"].apply(format_thousands)
            daily_display["FILES"] = daily_display["FILES"].apply(format_thousands)
            daily_display["ROWS_PER_FILE"] = daily_display["ROWS_PER_FILE"].apply(format_metric)
            # Add outlier indicator
            daily_display["Outlier"] = daily_display["IS_OUTLIER"].apply(lambda x: "‚ö†Ô∏è" if x else "")
            daily_display = daily_display.rename(columns={
                "ROWS_LOADED": "Rows",
                "FILES": "Files",
                "ROWS_PER_FILE": "Rows/File",
                "ERRORS": "Errors",
                "LOAD_COUNT": "Loads",
                "OUTLIER_REASON": "Outlier Reason"
            })
            # Select columns to display
            display_cols = ["Date", "Files", "Rows", "Rows/File", "Errors", "Loads", "Outlier", "Outlier Reason"]
            display_cols = [c for c in display_cols if c in daily_display.columns]
            st.dataframe(daily_display[display_cols], use_container_width=True, hide_index=True)
            st.caption("üîÑ = Today's data (partial, still loading)")
        
            st.download_button(
                "üì• Download Daily History",
                daily_display.to_csv(index=False).encode(),
                f"pipe_daily_{selected_pipe.replace('.', '_')}.csv",
                "text/csv"
            )
        
            # Note about detailed run history
            with st.expander("‚ÑπÔ∏è Need detailed file-level run history?"):
                st.markdown(f"""
                For detailed file-level information, query `SNOWFLAKE.ACCOUNT_USAGE.COPY_HISTORY` directly:
            
                ```sql
                SELECT 
                    PIPE_RECEIVED_TIME,
                    FILE_NAME,
                    ROW_COUNT,
                    ERROR_COUNT,
                    STATUS,
                    FIRST_ERROR_MESSAGE
                FROM SNOWFLAKE.ACCOUNT_USAGE.COPY_HISTORY
                WHERE PIPE_NAME = '{selected_pipe}'
                  AND PIPE_RECEIVED_TIME >= DATEADD('day', -30, CURRENT_TIMESTAMP())
                ORDER BY PIPE_RECEIVED_TIME DESC;
                ```
                """)
        else:
            st.info("No historical data available for this pipe in the materialized table.")

# -----------------------------------------------------------------------------
# KPI METRICS PAGE
# -----------------------------------------------------------------------------
if page_choice == "üìà KPI Metrics":
    render_page_header(
        "üìà",
        "KPI Metrics",
        "Track business metrics, detect anomalies, and configure monitoring"
    )
    
    # Tabs for Dashboard and Configuration
    if "kpi_page_tab" not in st.session_state:
        st.session_state.kpi_page_tab = "dashboard"
    
    kpi_tab_dashboard, kpi_tab_config = st.tabs(["üìä Dashboard", "‚öôÔ∏è Configuration"])
    
    # ==========================================================================
    # TAB 1: DASHBOARD
    # ==========================================================================
    with kpi_tab_dashboard:
        with st.expander("‚ÑπÔ∏è How anomaly detection works", expanded=False):
            st.markdown("""
            **Alert Levels:**
            | Status | Condition |
            |--------|-----------|
            | üü¢ **OK** | Value within normal range |
            | üü° **WARNING** | Deviation **25-49%** from baseline OR exceeds **2œÉ** |
            | üî¥ **CRITICAL** | Deviation **‚â•50%** from baseline OR exceeds **3œÉ** |
            
            *Baseline = average of the last 30 days. œÉ = standard deviation.*
            """)
        
        # Check if tables exist
        try:
            # Join with config to get EXPECTED_MODE, FIXED_EXPECTED_VALUE, DESCRIPTION, and DASHBOARD_URL
            summary_df = run_query(f"""
                SELECT 
                    s.KPI_NAME, s.LATEST_VALUE, s.EXPECTED_VALUE, s.DEVIATION_PCT,
                    s.DAY_OVER_DAY_PCT, s.THRESHOLD_PCT, s.HISTORY_DAYS, s.LATEST_DATE,
                    s.STATUS, s.IS_ANOMALY, s.SEVERITY, s.ANOMALY_REASON,
                    COALESCE(c.EXPECTED_MODE, 'THRESHOLD') AS EXPECTED_MODE,
                    c.FIXED_EXPECTED_VALUE AS CONFIG_FIXED_VALUE,
                    COALESCE(c.DATE_OFFSET, 0) AS DATE_OFFSET,
                    c.KPI_DESCRIPTION,
                    c.DASHBOARD_URL
                FROM {KPI_SUMMARY_FQN} s
                LEFT JOIN {KPI_CONFIG_FQN} c ON s.KPI_NAME = c.KPI_NAME
                ORDER BY s.SEVERITY DESC, s.KPI_NAME
            """)
        except:
            st.warning(f"""
            ‚ö†Ô∏è **KPI tables not found!**
            
            1. Run the setup script: `kpi_monitoring_setup.sql`
            2. Use the **Configuration** tab to add KPIs
            3. Run: `CALL REFRESH_KPI_METRICS();`
            """)
            summary_df = pd.DataFrame()
        
        if not summary_df.empty:
            # Summary metrics
            st.markdown("### üìä Overview")
            
            total_kpis = len(summary_df)
            healthy = (summary_df["STATUS"] == "OK").sum()
            warning = (summary_df["STATUS"] == "WARNING").sum()
            critical = (summary_df["STATUS"] == "CRITICAL").sum()
            no_data = (summary_df["STATUS"] == "NO_DATA").sum()
            
            col1, col2, col3, col4, col5 = st.columns(5)
            col1.metric("Total KPIs", total_kpis)
            col2.metric("‚úÖ Healthy", healthy)
            col3.metric("üü° Warning", warning)
            col4.metric("üî¥ Critical", critical)
            col5.metric("‚ö´ No Data", no_data)
            
            # Alert box if issues
            total_issues = warning + critical
            if total_issues > 0:
                if critical > 0:
                    st.error(f"üî¥ **{critical} CRITICAL** and **{warning} WARNING** KPI anomalies detected! Check the details below.")
                else:
                    st.warning(f"üü° **{warning} KPI warning(s)** detected. Check the details below.")
            
            st.markdown("---")
            
            # Glossary
            with st.expander("üìñ Column Glossary - What each column means", expanded=False):
                st.markdown("""
                | Column | Description |
                |--------|-------------|
                | **Status** | Health status: ‚úÖ OK, üü° Warning (25-49% deviation), üî¥ Critical (‚â•50% deviation), ‚ö´ No Data |
                | **KPI Name** | Name of the KPI being monitored |
                | **Description** | What this KPI measures |
                | **Latest Value** | Most recent measured value |
                | **Baseline Avg** | Average value based on historical baseline (last 30 days) |
                | **Deviation %** | How much the latest value deviates from baseline (negative = below) |
                | **DoD %** | Day-over-day change compared to previous day |
                | **Threshold** | Configured alert threshold % |
                | **History** | Number of days of historical data available |
                
                **Alert Thresholds:** üü° Warning = 25-49% deviation OR >2œÉ | üî¥ Critical = ‚â•50% deviation OR >3œÉ
                """)
            
            # Main KPI table
            st.markdown("### üìã KPI Details")
            
            # Add status column
            summary_df["STATUS_ICON"] = summary_df["STATUS"].map({
                "OK": "‚úÖ OK",
                "WARNING": "üü° Warning",
                "CRITICAL": "üî¥ Critical",
                "NO_DATA": "‚ö´ No Data",
                "MISSING": "‚ö´ Missing"
            }).fillna("‚ùì Unknown")
            
            # Format columns
            # Create threshold/fixed display column based on EXPECTED_MODE
            def format_threshold_display(row):
                mode = row.get("EXPECTED_MODE", "THRESHOLD")
                if mode == "FIXED":
                    fixed_val = row.get("CONFIG_FIXED_VALUE")
                    if pd.notna(fixed_val):
                        return f"‚â• {format_thousands(fixed_val)}"
                    return "Fixed"
                else:
                    thresh = row.get("THRESHOLD_PCT")
                    if pd.notna(thresh):
                        return f"-{thresh:.0f}%"
                    return "‚Äî"
            
            summary_df["THRESHOLD_DISPLAY"] = summary_df.apply(format_threshold_display, axis=1)
            
            # Keep actual URL for clickable links
            summary_df["Link"] = summary_df["DASHBOARD_URL"].apply(lambda x: str(x).strip() if pd.notna(x) and str(x).strip() else None)
            
            display_cols = ["STATUS_ICON", "KPI_NAME", "KPI_DESCRIPTION", "LATEST_VALUE", "EXPECTED_VALUE", "DEVIATION_PCT", 
                           "DAY_OVER_DAY_PCT", "THRESHOLD_DISPLAY", "HISTORY_DAYS", "LATEST_DATE", "Link"]
            
            display_df = summary_df[display_cols].copy()
            display_df["LATEST_VALUE"] = display_df["LATEST_VALUE"].apply(format_thousands)
            display_df["EXPECTED_VALUE"] = display_df["EXPECTED_VALUE"].apply(format_thousands)
            display_df["DEVIATION_PCT"] = display_df["DEVIATION_PCT"].apply(lambda x: f"{x:+.1f}%" if pd.notna(x) else "‚Äî")
            display_df["DAY_OVER_DAY_PCT"] = display_df["DAY_OVER_DAY_PCT"].apply(lambda x: f"{x:+.1f}%" if pd.notna(x) else "‚Äî")
            display_df["KPI_DESCRIPTION"] = display_df["KPI_DESCRIPTION"].fillna("‚Äî")
            
            display_df = display_df.rename(columns={
                "STATUS_ICON": "Status",
                "KPI_NAME": "KPI Name",
                "KPI_DESCRIPTION": "Description",
                "LATEST_VALUE": "Latest Value",
                "EXPECTED_VALUE": "Baseline Avg",
                "DEVIATION_PCT": "Deviation %",
                "DAY_OVER_DAY_PCT": "DoD %",
                "THRESHOLD_DISPLAY": "Threshold",
                "HISTORY_DAYS": "History",
                "LATEST_DATE": "Latest Date"
            })
            
            st.dataframe(
                display_df, 
                use_container_width=True, 
                hide_index=True,
                column_config={
                    "Link": st.column_config.LinkColumn(
                        "Link",
                        display_text="üîó",
                        help="Click to open BI dashboard"
                    )
                }
            )
            
            # Download button
            st.download_button("üì• Download CSV", display_df.to_csv(index=False).encode(), "kpi_health.csv", "text/csv")
            
            # Drilldown section
            st.markdown("---")
            st.markdown("### üîç KPI Drilldown")
            
            kpi_options = ["Select a KPI..."] + sorted(summary_df["KPI_NAME"].tolist())
            selected_kpi = st.selectbox("Choose a KPI for detailed history", kpi_options)
            
            if selected_kpi != "Select a KPI...":
                kpi_info = summary_df[summary_df["KPI_NAME"] == selected_kpi].iloc[0]
                
                # KPI info header
                col1, col2, col3, col4 = st.columns(4)
                col1.metric("Status", kpi_info["STATUS_ICON"])
                col2.metric("Latest Value", format_thousands(kpi_info["LATEST_VALUE"]))
                col3.metric("Baseline Avg", format_thousands(kpi_info["EXPECTED_VALUE"]))
                col4.metric("Deviation", f"{kpi_info['DEVIATION_PCT']:+.1f}%" if pd.notna(kpi_info['DEVIATION_PCT']) else "‚Äî")
                
                if kpi_info.get("KPI_DESCRIPTION"):
                    st.caption(f"üìù {kpi_info['KPI_DESCRIPTION']}")
                
                # Show dashboard link if available
                dashboard_url = kpi_info.get("DASHBOARD_URL")
                if dashboard_url and str(dashboard_url).strip():
                    st.markdown(f"üîó **[View in BI Dashboard]({dashboard_url})**")
                
                # Historical chart
                safe_kpi = selected_kpi.replace("'", "''")
                history_df = run_query(f"""
                    SELECT 
                        METRIC_DATE,
                        METRIC_VALUE,
                        EXPECTED_VALUE,
                        MEDIAN_VALUE,
                        DEVIATION_PCT,
                        IS_ANOMALY,
                        ANOMALY_REASON
                    FROM {KPI_DAILY_FQN}
                    WHERE KPI_NAME = '{safe_kpi}'
                    ORDER BY METRIC_DATE DESC
                    LIMIT 60
                """)
                
                if not history_df.empty:
                    history_df["METRIC_DATE"] = pd.to_datetime(history_df["METRIC_DATE"])
                    
                    # Line chart
                    st.markdown("**üìà Value Trend**")
                    
                    chart_df = history_df.sort_values("METRIC_DATE")
                    
                    # Melt for Altair
                    chart_melt = chart_df[["METRIC_DATE", "METRIC_VALUE", "EXPECTED_VALUE"]].melt(
                        id_vars=["METRIC_DATE"],
                        value_vars=["METRIC_VALUE", "EXPECTED_VALUE"],
                        var_name="Metric",
                        value_name="Value"
                    )
                    chart_melt["Metric"] = chart_melt["Metric"].map({
                        "METRIC_VALUE": "Actual",
                        "EXPECTED_VALUE": "Baseline Avg"
                    })
                    
                    chart = alt.Chart(chart_melt).mark_line(point=True).encode(
                        x=alt.X("METRIC_DATE:T", title="Date"),
                        y=alt.Y("Value:Q", title="Value"),
                        color=alt.Color("Metric:N", scale=alt.Scale(
                            domain=["Actual", "Baseline Avg"],
                            range=["#3B82F6", "#9CA3AF"]
                        )),
                        tooltip=["METRIC_DATE:T", "Metric:N", "Value:Q"]
                    ).properties(height=300)
                    
                    st.altair_chart(chart, use_container_width=True)
                    
                    # Show anomaly days
                    anomaly_days = history_df[history_df["IS_ANOMALY"] == True]
                    if not anomaly_days.empty:
                        st.markdown("**‚ö†Ô∏è Anomaly Days**")
                        anomaly_display = anomaly_days[["METRIC_DATE", "METRIC_VALUE", "EXPECTED_VALUE", "DEVIATION_PCT", "ANOMALY_REASON"]].copy()
                        anomaly_display["METRIC_DATE"] = anomaly_display["METRIC_DATE"].dt.strftime("%Y-%m-%d")
                        anomaly_display["METRIC_VALUE"] = anomaly_display["METRIC_VALUE"].apply(format_thousands)
                        anomaly_display["EXPECTED_VALUE"] = anomaly_display["EXPECTED_VALUE"].apply(format_thousands)
                        anomaly_display["DEVIATION_PCT"] = anomaly_display["DEVIATION_PCT"].apply(lambda x: f"{x:+.1f}%")
                        anomaly_display = anomaly_display.rename(columns={
                            "METRIC_DATE": "Date",
                            "METRIC_VALUE": "Actual",
                            "EXPECTED_VALUE": "Baseline Avg",
                            "DEVIATION_PCT": "Deviation",
                            "ANOMALY_REASON": "Reason"
                        })
                        st.dataframe(anomaly_display, use_container_width=True, hide_index=True)
                    
                    # Daily history table
                    st.markdown("**üìã Daily Values**")
                    daily_display = history_df.copy()
                    daily_display["METRIC_DATE"] = daily_display["METRIC_DATE"].dt.strftime("%Y-%m-%d")
                    daily_display["METRIC_VALUE"] = daily_display["METRIC_VALUE"].apply(format_thousands)
                    daily_display["EXPECTED_VALUE"] = daily_display["EXPECTED_VALUE"].apply(format_thousands)
                    daily_display["DEVIATION_PCT"] = daily_display["DEVIATION_PCT"].apply(lambda x: f"{x:+.1f}%" if pd.notna(x) else "‚Äî")
                    daily_display["IS_ANOMALY"] = daily_display["IS_ANOMALY"].apply(lambda x: "‚ö†Ô∏è" if x else "")
                    daily_display = daily_display[["METRIC_DATE", "METRIC_VALUE", "EXPECTED_VALUE", "DEVIATION_PCT", "IS_ANOMALY"]]
                    daily_display = daily_display.rename(columns={
                        "METRIC_DATE": "Date",
                        "METRIC_VALUE": "Value",
                        "EXPECTED_VALUE": "Baseline Avg",
                        "DEVIATION_PCT": "Deviation",
                        "IS_ANOMALY": "‚ö†Ô∏è"
                    })
                    st.dataframe(daily_display.head(30), use_container_width=True, hide_index=True)
                    
                    st.download_button(
                        "üì• Download History",
                        history_df.to_csv(index=False).encode(),
                        f"kpi_history_{selected_kpi.replace(' ', '_')}.csv",
                        "text/csv"
                    )
                else:
                    st.info("No historical data available. Run the refresh procedure to collect data.")
                
                # Show the SQL query
                with st.expander("üîç View Metric SQL"):
                    st.code(kpi_info.get("METRIC_SQL", "N/A"), language="sql")
        
        else:
            st.info("""
            **No KPI data yet!**
            
            1. Go to the **Configuration** tab to add your KPIs
            2. Run the refresh procedure to collect data
            3. Come back to the Dashboard to see results
            """)
    
    # ==========================================================================
    # TAB 2: CONFIGURATION
    # ==========================================================================
    with kpi_tab_config:
        # Check if table exists
        def ensure_kpi_config_exists():
            try:
                run_query(f"SELECT 1 FROM {KPI_CONFIG_FQN} LIMIT 1")
                return True
            except:
                return False
        
        if not ensure_kpi_config_exists():
            st.warning(f"""
            ‚ö†Ô∏è **KPI Configuration table not found!**
            
            Run the setup script first:
            ```sql
            CREATE TABLE IF NOT EXISTS {KPI_CONFIG_FQN} (
                KPI_NAME VARCHAR(255) PRIMARY KEY,
                ...
            );
            ```
            """)
        else:
            # Check/upgrade schema
            try:
                run_query(f"SELECT EXPECTED_MODE, DATE_OFFSET, DASHBOARD_URL FROM {KPI_CONFIG_FQN} LIMIT 1")
            except:
                st.info("üîÑ Upgrading table schema...")
                try:
                    run_ddl(f"ALTER TABLE {KPI_CONFIG_FQN} ADD COLUMN IF NOT EXISTS EXPECTED_MODE VARCHAR(20) DEFAULT 'THRESHOLD'")
                    run_ddl(f"ALTER TABLE {KPI_CONFIG_FQN} ADD COLUMN IF NOT EXISTS FIXED_EXPECTED_VALUE NUMBER(18,4)")
                    run_ddl(f"ALTER TABLE {KPI_CONFIG_FQN} ADD COLUMN IF NOT EXISTS DATE_OFFSET NUMBER DEFAULT 1")
                    run_ddl(f"ALTER TABLE {KPI_CONFIG_FQN} ADD COLUMN IF NOT EXISTS DASHBOARD_URL VARCHAR(1000)")
                    st.success("‚úÖ Schema upgraded! Please refresh the page.")
                    st.cache_data.clear()
                except Exception as e:
                    st.error(f"Schema upgrade failed: {str(e)}")
            
            # Get existing KPIs
            kpi_df = run_query(f"""
                SELECT KPI_NAME, KPI_DESCRIPTION, METRIC_SQL, 
                    COALESCE(EXPECTED_MODE, 'THRESHOLD') AS EXPECTED_MODE,
                    THRESHOLD_PCT, FIXED_EXPECTED_VALUE, BASELINE_DAYS,
                    COALESCE(DATE_OFFSET, 0) AS DATE_OFFSET,
                    DASHBOARD_URL, IS_MONITORED, CREATED_AT
                FROM {KPI_CONFIG_FQN}
                ORDER BY KPI_NAME
            """)
            kpi_count = len(kpi_df) if not kpi_df.empty else 0
            
            # Sub-tabs for configuration
            config_subtab1, config_subtab2, config_subtab3 = st.tabs(["üìã Manage KPIs", "‚ûï Add New", "üîÑ Refresh Data"])
            
            # ---------- MANAGE KPIs ----------
            with config_subtab1:
                if kpi_count == 0:
                    st.info("üöÄ No KPIs configured yet. Use the **Add New** tab to create your first KPI!")
                else:
                    st.success(f"üìä **{kpi_count}** KPI(s) configured")
                    
                    for idx, kpi_row in kpi_df.iterrows():
                        kpi_name = kpi_row["KPI_NAME"]
                        safe_name = kpi_name.replace("'", "''")
                        
                        # Status indicator
                        is_monitored = kpi_row["IS_MONITORED"]
                        status_icon = "‚úÖ" if is_monitored else "‚è∏Ô∏è"
                        
                        with st.expander(f"{status_icon} **{kpi_name}**", expanded=False):
                            st.caption(kpi_row["KPI_DESCRIPTION"] or "No description")
                            
                            # Edit SQL
                            current_sql = kpi_row["METRIC_SQL"] or ""
                            new_sql = st.text_area("SQL Query", value=current_sql, height=100, key=f"kpi_sql_{idx}")
                            
                            if new_sql != current_sql:
                                if st.button("üíæ Save SQL", key=f"save_sql_{idx}"):
                                    if "{DATE}" not in new_sql.upper():
                                        st.warning("‚ö†Ô∏è Query should contain {DATE} placeholder!")
                                    else:
                                        safe_sql = new_sql.replace("'", "''")
                                        run_ddl(f"UPDATE {KPI_CONFIG_FQN} SET METRIC_SQL = '{safe_sql}', UPDATED_AT = CURRENT_TIMESTAMP() WHERE KPI_NAME = '{safe_name}'")
                                        st.success("‚úÖ SQL updated!")
                                        st.cache_data.clear()
                            
                            # Settings
                            col1, col2 = st.columns(2)
                            with col1:
                                new_monitored = st.checkbox("Monitored", value=bool(is_monitored), key=f"mon_{idx}")
                                current_mode = kpi_row["EXPECTED_MODE"] or "THRESHOLD"
                                new_mode = st.radio("Mode", ["THRESHOLD", "FIXED"], index=0 if current_mode == "THRESHOLD" else 1, key=f"mode_{idx}", horizontal=True)
                            
                            with col2:
                                if new_mode == "THRESHOLD":
                                    new_threshold = st.slider("Threshold %", 5, 80, int(kpi_row["THRESHOLD_PCT"] or 20), key=f"thresh_{idx}")
                                else:
                                    current_fixed = float(kpi_row["FIXED_EXPECTED_VALUE"]) if pd.notna(kpi_row["FIXED_EXPECTED_VALUE"]) else 100
                                    st.number_input("Fixed Value", min_value=0, value=int(current_fixed), key=f"fixed_{idx}")
                                    new_threshold = int(kpi_row["THRESHOLD_PCT"] or 20)
                            
                            if st.button("üíæ Save Settings", key=f"save_settings_{idx}"):
                                run_ddl(f"UPDATE {KPI_CONFIG_FQN} SET IS_MONITORED = {new_monitored}, EXPECTED_MODE = '{new_mode}', THRESHOLD_PCT = {new_threshold}, UPDATED_AT = CURRENT_TIMESTAMP() WHERE KPI_NAME = '{safe_name}'")
                                st.success("‚úÖ Settings saved!")
                                st.cache_data.clear()
                            
                            # Delete option
                            if st.button("üóëÔ∏è Delete KPI", key=f"del_{idx}", type="secondary"):
                                run_ddl(f"DELETE FROM {KPI_CONFIG_FQN} WHERE KPI_NAME = '{safe_name}'")
                                run_ddl(f"DELETE FROM {KPI_DAILY_FQN} WHERE KPI_NAME = '{safe_name}'")
                                run_ddl(f"DELETE FROM {KPI_SUMMARY_FQN} WHERE KPI_NAME = '{safe_name}'")
                                st.success("‚úÖ KPI deleted!")
                                st.cache_data.clear()
                                st.rerun()
            
            # ---------- ADD NEW KPI ----------
            with config_subtab2:
                st.markdown("##### Create a new KPI metric")
                
                # Initialize session state
                if "query_test_result" not in st.session_state:
                    st.session_state.query_test_result = None
                if "query_test_valid" not in st.session_state:
                    st.session_state.query_test_valid = False
                
                kpi_name = st.text_input("KPI Name *", placeholder="e.g., Daily Active Users", key="new_kpi_name_tab")
                kpi_description = st.text_area("Description", placeholder="What does this KPI measure?", height=60, key="new_kpi_desc_tab")
                
                metric_sql = st.text_area(
                    "SQL Query *", 
                    placeholder="SELECT COUNT(*) FROM mydb.myschema.table WHERE date_column = '{DATE}'",
                    height=100,
                    help="Use {DATE} as placeholder for the date filter.",
                    key="new_kpi_sql_tab"
                )
                
                # Test Query
                col_test1, col_test2 = st.columns([1, 1])
                with col_test1:
                    test_date = st.date_input("Test date:", value=date.today() - timedelta(days=1), key="test_date_tab")
                with col_test2:
                    st.markdown("<div style='height: 28px'></div>", unsafe_allow_html=True)
                    if st.button("üß™ Test Query", use_container_width=True, key="btn_test_query_tab"):
                        if metric_sql:
                            test_sql = metric_sql.replace("{DATE}", str(test_date)).replace("{date}", str(test_date))
                            with st.spinner("Running query..."):
                                try:
                                    result_df = session.sql(test_sql).to_pandas()
                                    if result_df.empty:
                                        st.session_state.query_test_result = "‚ö†Ô∏è Query returned no rows"
                                        st.session_state.query_test_valid = False
                                    elif len(result_df.columns) != 1 or len(result_df) != 1:
                                        st.session_state.query_test_result = "‚ö†Ô∏è Query must return exactly 1 row with 1 column"
                                        st.session_state.query_test_valid = False
                                    else:
                                        value = result_df.iloc[0, 0]
                                        try:
                                            numeric_value = float(value)
                                            st.session_state.query_test_result = f"‚úÖ Valid! Result: **{format_thousands(numeric_value)}**"
                                            st.session_state.query_test_valid = True
                                        except:
                                            st.session_state.query_test_result = f"‚ö†Ô∏è Non-numeric value: {value}"
                                            st.session_state.query_test_valid = False
                                except Exception as e:
                                    st.session_state.query_test_result = f"‚ùå Error: {str(e)[:100]}"
                                    st.session_state.query_test_valid = False
                
                if st.session_state.query_test_result:
                    if st.session_state.query_test_valid:
                        st.success(st.session_state.query_test_result)
                    else:
                        st.warning(st.session_state.query_test_result)
                
                # Threshold settings
                col1, col2 = st.columns(2)
                with col1:
                    expected_mode = st.radio("Mode:", ["THRESHOLD", "FIXED"], key="new_kpi_mode_tab", horizontal=True)
                with col2:
                    if expected_mode == "THRESHOLD":
                        threshold_pct = st.slider("Threshold %", 5, 80, 20, key="new_kpi_threshold_tab")
                        baseline_days = 30
                        fixed_expected = None
                    else:
                        fixed_expected = st.number_input("Fixed value *", min_value=1, value=100, key="new_kpi_fixed_tab")
                        threshold_pct = 20
                        baseline_days = 30
                
                # Add button
                if not st.session_state.query_test_valid:
                    st.info("üí° Test your query first to enable the Add button")
                
                if st.button("‚ûï Add KPI", use_container_width=True, type="primary", disabled=not st.session_state.query_test_valid, key="btn_add_kpi_tab"):
                    if not kpi_name or not metric_sql:
                        st.error("KPI Name and SQL Query are required!")
                    else:
                        try:
                            safe_name = kpi_name.replace("'", "''")
                            safe_desc = (kpi_description or "").replace("'", "''")
                            safe_sql = metric_sql.replace("'", "''")
                            fixed_val_sql = f"{fixed_expected}" if fixed_expected else "NULL"
                            
                            insert_sql = f"""
                                INSERT INTO {KPI_CONFIG_FQN} (
                                    KPI_NAME, KPI_DESCRIPTION, METRIC_SQL, EXPECTED_MODE,
                                    THRESHOLD_PCT, FIXED_EXPECTED_VALUE, BASELINE_DAYS, IS_MONITORED
                                ) VALUES (
                                    '{safe_name}', '{safe_desc}', '{safe_sql}', '{expected_mode}',
                                    {threshold_pct}, {fixed_val_sql}, {baseline_days}, TRUE
                                )
                            """
                            run_ddl(insert_sql)
                            st.session_state.query_test_result = None
                            st.session_state.query_test_valid = False
                            st.cache_data.clear()
                            st.success(f"‚úÖ KPI '{kpi_name}' added! Go to **Manage KPIs** to view it.")
                        except Exception as e:
                            if "duplicate" in str(e).lower():
                                st.error("‚ùå A KPI with this name already exists!")
                            else:
                                st.error(f"‚ùå Error: {str(e)[:100]}")
            
            # ---------- REFRESH DATA ----------
            with config_subtab3:
                if kpi_count == 0:
                    st.warning("‚ö†Ô∏è No KPIs configured yet. Add KPIs first before refreshing data.")
                else:
                    st.info(f"üìä **{kpi_count} KPI(s)** will be refreshed")
                    
                    lookback_days = st.slider("Days to refresh", 1, 30, 7, help="Number of days of historical data to collect")
                    
                    if st.button("üîÑ Run Refresh", use_container_width=True, type="primary", key="btn_refresh_kpi_tab"):
                        with st.spinner("Refreshing KPI metrics..."):
                            try:
                                result = session.sql(f"CALL {CONFIG_DATABASE}.{CONFIG_SCHEMA}.REFRESH_KPI_METRICS('{CONFIG_DATABASE}', '{CONFIG_SCHEMA}', {lookback_days})").collect()
                                result_msg = result[0][0] if result else "Completed"
                                if "error" in result_msg.lower():
                                    st.warning(f"‚ö†Ô∏è {result_msg}")
                                else:
                                    st.success(f"‚úÖ {result_msg}")
                                st.cache_data.clear()
                            except Exception as e:
                                st.error(f"‚ùå Error: {str(e)}")
                    
                    st.markdown("---")
                    st.caption("""
                    **About refresh:**
                    - Executes each KPI's SQL query for the specified days
                    - Calculates baselines and detects anomalies
                    - Updates the Dashboard with latest health status
                    """)

# =============================================================================
# AI ASSISTANT PAGE - Natural language Q&A powered by Snowflake Cortex
# =============================================================================
if page_choice == "ü§ñ AI Assistant":
    render_page_header(
        "ü§ñ",
        "AI Data Assistant",
        "Ask questions in plain English and get instant insights about your data health"
    )
    
    # Initialize chat history in session state
    if "chat_messages" not in st.session_state:
        st.session_state.chat_messages = []
    
    # =========================================================================
    # HELPER FUNCTIONS FOR AI ASSISTANT
    # =========================================================================
    
    def get_current_health_context():
        """Gather current health status from all monitoring tables for AI context."""
        context_parts = []
        
        # Pipe Health Summary - ONLY for configured/monitored pipes
        try:
            pipe_df = run_query(f"""
                SELECT 
                    COUNT(*) AS total_pipes,
                    SUM(CASE WHEN m.YESTERDAY_FILES > 0 THEN 1 ELSE 0 END) AS active_pipes,
                    SUM(CASE WHEN m.YESTERDAY_FILES = 0 AND m.EXPECTED_FILES > 0 THEN 1 ELSE 0 END) AS missing_pipes,
                    SUM(CASE WHEN m.YESTERDAY_ERRORS > 0 THEN 1 ELSE 0 END) AS pipes_with_errors,
                    SUM(CASE WHEN m.FILES_SHORT_PCT > 50 THEN 1 ELSE 0 END) AS low_volume_pipes
                FROM {PIPE_HEALTH_METRICS_FQN} m
                INNER JOIN {CONFIG_TABLE_FQN} c 
                    ON m.PIPE_NAME = c.DATABASE_NAME || '.' || c.SCHEMA_NAME || '.' || c.PIPE_NAME
                WHERE c.IS_MONITORED = TRUE
            """)
            if not pipe_df.empty:
                row = pipe_df.iloc[0]
                context_parts.append(f"""
PIPE HEALTH STATUS (Configured Pipes Only):
- Total monitored pipes: {row['TOTAL_PIPES']}
- Active pipes (loaded yesterday): {row['ACTIVE_PIPES']}
- Missing/stale pipes: {row['MISSING_PIPES']}
- Pipes with errors: {row['PIPES_WITH_ERRORS']}
- Low volume pipes: {row['LOW_VOLUME_PIPES']}
""")
        except:
            context_parts.append("PIPE HEALTH: Data not available")
        
        # Pipe Details for issues - ONLY for configured/monitored pipes
        try:
            issues_df = run_query(f"""
                SELECT m.PIPE_NAME, m.YESTERDAY_FILES, m.EXPECTED_FILES, m.YESTERDAY_ERRORS,
                       ROUND(m.FILES_SHORT_PCT, 1) AS FILES_SHORT_PCT, m.HOURS_AGO,
                       c.NOTES AS PIPE_NOTES
                FROM {PIPE_HEALTH_METRICS_FQN} m
                INNER JOIN {CONFIG_TABLE_FQN} c 
                    ON m.PIPE_NAME = c.DATABASE_NAME || '.' || c.SCHEMA_NAME || '.' || c.PIPE_NAME
                WHERE c.IS_MONITORED = TRUE
                  AND (m.YESTERDAY_FILES = 0 OR m.YESTERDAY_ERRORS > 0 OR m.FILES_SHORT_PCT > 30)
                ORDER BY COALESCE(m.FILES_SHORT_PCT, 100) DESC
                LIMIT 10
            """)
            if not issues_df.empty:
                issues_text = "PIPE ISSUES DETAILS (Configured Pipes Only):\n"
                for _, r in issues_df.iterrows():
                    notes = f" ({r['PIPE_NOTES']})" if r.get('PIPE_NOTES') else ""
                    issues_text += f"- {r['PIPE_NAME']}{notes}: Files={r['YESTERDAY_FILES']}, Avg={r['EXPECTED_FILES']}, Errors={r['YESTERDAY_ERRORS']}, ShortPct={r['FILES_SHORT_PCT']}%\n"
                context_parts.append(issues_text)
        except:
            pass
        
        # List all configured pipes for context
        try:
            config_df = run_query(f"""
                SELECT PIPE_NAME, RUNS_DAILY, NOTES
                FROM {CONFIG_TABLE_FQN}
                WHERE IS_MONITORED = TRUE
                ORDER BY PIPE_NAME
            """)
            if not config_df.empty:
                config_text = "CONFIGURED PIPES LIST:\n"
                for _, r in config_df.iterrows():
                    daily = "Daily" if r.get('RUNS_DAILY', True) else "Not Daily"
                    notes = f" - {r['NOTES']}" if r.get('NOTES') else ""
                    config_text += f"- {r['PIPE_NAME']} ({daily}){notes}\n"
                context_parts.append(config_text)
        except:
            pass
        
        # Data Freshness Summary
        try:
            fresh_df = run_query(f"""
                SELECT 
                    COUNT(*) AS total_tables,
                    SUM(CASE WHEN HOURS_SINCE_WRITE < 24 THEN 1 ELSE 0 END) AS fresh_tables,
                    SUM(CASE WHEN HOURS_SINCE_WRITE >= 24 OR HOURS_SINCE_WRITE IS NULL THEN 1 ELSE 0 END) AS stale_tables,
                    SUM(CASE WHEN CURRENT_ROWS = 0 THEN 1 ELSE 0 END) AS empty_tables
                FROM {TABLE_METRICS_TABLE_FQN}
            """)
            if not fresh_df.empty:
                row = fresh_df.iloc[0]
                context_parts.append(f"""
DATA FRESHNESS STATUS:
- Total monitored tables: {row['TOTAL_TABLES']}
- Fresh tables (updated <24h): {row['FRESH_TABLES']}
- Stale tables: {row['STALE_TABLES']}
- Empty tables: {row['EMPTY_TABLES']}
""")
        except:
            context_parts.append("DATA FRESHNESS: Data not available")
        
        # Stale tables details
        try:
            stale_df = run_query(f"""
                SELECT TABLE_NAME, DATABASE_NAME, SCHEMA_NAME, 
                       ROUND(HOURS_SINCE_WRITE, 1) AS HOURS_SINCE_WRITE,
                       INGEST_PATTERN
                FROM {TABLE_METRICS_TABLE_FQN}
                WHERE HOURS_SINCE_WRITE >= 24 OR HOURS_SINCE_WRITE IS NULL
                ORDER BY HOURS_SINCE_WRITE DESC NULLS FIRST
                LIMIT 10
            """)
            if not stale_df.empty:
                stale_text = "STALE TABLES DETAILS:\n"
                for _, r in stale_df.iterrows():
                    hrs = r['HOURS_SINCE_WRITE'] if pd.notna(r['HOURS_SINCE_WRITE']) else 'Never'
                    stale_text += f"- {r['DATABASE_NAME']}.{r['SCHEMA_NAME']}.{r['TABLE_NAME']}: {hrs} hours ago, Pattern={r['INGEST_PATTERN']}\n"
                context_parts.append(stale_text)
        except:
            pass
        
        # KPI Summary
        try:
            kpi_df = run_query(f"""
                SELECT 
                    COUNT(*) AS total_kpis,
                    SUM(CASE WHEN STATUS = 'OK' THEN 1 ELSE 0 END) AS healthy_kpis,
                    SUM(CASE WHEN STATUS = 'LOW' THEN 1 ELSE 0 END) AS low_kpis,
                    SUM(CASE WHEN IS_ANOMALY = TRUE THEN 1 ELSE 0 END) AS anomaly_kpis
                FROM {KPI_SUMMARY_FQN}
            """)
            if not kpi_df.empty:
                row = kpi_df.iloc[0]
                context_parts.append(f"""
KPI HEALTH STATUS:
- Total KPIs: {row['TOTAL_KPIS']}
- Healthy KPIs: {row['HEALTHY_KPIS']}
- Low KPIs: {row['LOW_KPIS']}
- Anomaly KPIs: {row['ANOMALY_KPIS']}
""")
        except:
            context_parts.append("KPI HEALTH: Data not available")
        
        # KPI Details
        try:
            kpi_details = run_query(f"""
                SELECT KPI_NAME, LATEST_VALUE, EXPECTED_VALUE, 
                       ROUND(DEVIATION_PCT, 1) AS DEVIATION_PCT, STATUS
                FROM {KPI_SUMMARY_FQN}
                WHERE STATUS != 'OK' OR IS_ANOMALY = TRUE
                LIMIT 10
            """)
            if not kpi_details.empty:
                kpi_text = "KPI ISSUES DETAILS:\n"
                for _, r in kpi_details.iterrows():
                    kpi_text += f"- {r['KPI_NAME']}: Value={r['LATEST_VALUE']}, Baseline Avg={r['EXPECTED_VALUE']}, Deviation={r['DEVIATION_PCT']}%, Status={r['STATUS']}\n"
                context_parts.append(kpi_text)
        except:
            pass
        
        context_parts.append(f"\nCurrent timestamp: {pd.Timestamp.utcnow():%Y-%m-%d %H:%M} UTC")
        
        return "\n".join(context_parts)
    
    def get_schema_context():
        """Provide schema information for the AI to understand available data."""
        return """
AVAILABLE DATA TABLES FOR QUERYING:

1. PIPE_HEALTH_METRICS (pipe monitoring):
   - PIPE_NAME, DATABASE_NAME, SCHEMA_NAME
   - TODAY_FILES, TODAY_ROWS, YESTERDAY_FILES, YESTERDAY_ROWS
   - EXPECTED_FILES, EXPECTED_ROWS, FILES_SHORT_PCT, ROWS_SHORT_PCT
   - YESTERDAY_ERRORS, HOURS_AGO, HISTORY_DAYS
   
2. PIPE_HEALTH_DAILY (daily pipe history):
   - PIPE_NAME, LOAD_DATE, FILES_LOADED, ROWS_LOADED, ERROR_COUNT
   
3. DATA_FRESHNESS_TABLE_METRICS (table freshness):
   - DATABASE_NAME, SCHEMA_NAME, TABLE_NAME, FQN
   - CURRENT_ROWS, HOURS_SINCE_WRITE, LAST_MODIFIED_DATE
   - TODAY_INSERTS, BASELINE_DAILY_INSERTS, PCT_OF_BASELINE
   - INGEST_PATTERN (APPEND, CDC, BATCH, CTAS/SNAP, etc.)
   
4. KPI_HEALTH_SUMMARY (KPI monitoring):
   - KPI_NAME, LATEST_VALUE, EXPECTED_VALUE, DEVIATION_PCT
   - STATUS (OK, LOW, NO_DATA), IS_ANOMALY

5. KPI_DAILY_METRICS (KPI history):
   - KPI_NAME, METRIC_DATE, METRIC_VALUE, EXPECTED_VALUE, IS_ANOMALY
"""
    
    def ask_ai(user_question: str, include_data_context: bool = True) -> str:
        """Send question to Cortex AI with context about current data health."""
        
        # Build the system prompt
        system_prompt = """You are an expert Data Observability Assistant for a Snowflake data platform. 
Your role is to help users understand their data pipeline health, identify issues, and provide actionable recommendations.

You have access to real-time monitoring data about:
- Snowpipe ingestion (files loaded, rows, errors, volume trends)
- Table freshness (when tables were last updated, staleness)
- KPI metrics (business metrics and anomalies)

IMPORTANT RULES:
- ONLY discuss pipes that are listed in the "CONFIGURED PIPES LIST" section below
- Do NOT mention or analyze any pipes that are not in the configured list
- If asked about a pipe not in the config, say it's not being monitored
- Focus your analysis on the configured/monitored pipes only

Guidelines:
- Be concise but informative
- Use bullet points and structure for clarity
- When issues are found, suggest specific next steps
- Use emojis sparingly for visual clarity (‚úÖ ‚ö†Ô∏è üî¥ üìä)
- If asked to generate SQL, use the schema information provided
- Always ground your answers in the actual data provided
"""
        
        # Build context
        context = ""
        if include_data_context:
            context = f"""
{get_schema_context()}

CURRENT HEALTH STATUS:
{get_current_health_context()}
"""
        
        # Build the full prompt
        full_prompt = f"""{system_prompt}

{context}

User Question: {user_question}

Please provide a helpful, accurate response based on the data above."""
        
        try:
            # Escape single quotes for SQL
            escaped_prompt = full_prompt.replace("'", "''")
            
            # Call Cortex via SQL
            result_df = session.sql(f"""
                SELECT SNOWFLAKE.CORTEX.COMPLETE(
                    'mistral-large2',
                    '{escaped_prompt}'
                ) AS response
            """).to_pandas()
            
            if not result_df.empty:
                return result_df.iloc[0]['RESPONSE']
            else:
                return "No response received from AI."
        except Exception as e:
            error_msg = str(e)
            if "not authorized" in error_msg.lower() or "access" in error_msg.lower():
                return f"""‚ö†Ô∏è **Cortex Access Error**
                
It looks like Cortex isn't enabled or your role doesn't have access.

**To enable Cortex, run:**
```sql
-- Grant Cortex access to your role
GRANT DATABASE ROLE SNOWFLAKE.CORTEX_USER TO ROLE your_role_name;
```

Or contact your Snowflake administrator to enable Cortex functions.

Error: {error_msg}"""
            return f"Error calling AI: {error_msg}"
    
    def generate_health_summary() -> str:
        """Generate a comprehensive health summary using AI."""
        
        prompt = """Based on the current health status data provided, generate a comprehensive Daily Health Summary Report.

Format the report as follows:
1. **Executive Summary** (2-3 sentences on overall health)
2. **üîß Pipe Health** (status, any issues, recommendations)
3. **üìã Data Freshness** (status, stale tables, recommendations)  
4. **üìà KPI Health** (status, anomalies, recommendations)
5. **üéØ Priority Actions** (top 3 things to address today)

Be specific about which pipes/tables/KPIs have issues. Use the actual names and numbers from the data.
Keep it concise but actionable. Use emojis for visual scanning."""

        return ask_ai(prompt, include_data_context=True)
    
    # =========================================================================
    # UI LAYOUT
    # =========================================================================
    
    # Tabs for different features
    tab1, tab2 = st.tabs(["üí¨ Ask Questions", "üìã Health Summary"])
    
    # -------------------------------------------------------------------------
    # TAB 1: Natural Language Q&A
    # -------------------------------------------------------------------------
    with tab1:
        render_section_header("üí¨", "Ask Questions About Your Data")
        st.markdown("Type your question below in plain English ‚Äî no SQL needed!")
        
        # Example questions in a cleaner format
        with st.expander("üí° **Need inspiration? Try these example questions**", expanded=False):
            col1, col2 = st.columns(2)
            with col1:
                st.markdown("""
                **üîß Pipeline Questions:**
                - "Which pipes failed yesterday?"
                - "What pipes have errors?"
                - "Is my ORDERS_PIPE running normally?"
                - "Show me all unhealthy pipelines"
                """)
                st.markdown("""
                **üìã Freshness Questions:**
                - "Which tables are stale?"
                - "What hasn't been updated in 3 days?"
                - "Show me all staging table status"
                """)
            with col2:
                st.markdown("""
                **üìà KPI Questions:**
                - "Are there any KPI anomalies?"
                - "Which KPIs are below target?"
                - "How is revenue trending?"
                """)
                st.markdown("""
                **üéØ General Questions:**
                - "Give me a quick health summary"
                - "What should I fix first today?"
                - "Any critical issues right now?"
                """)
        
        # Chat container
        chat_container = st.container()
        
        # Display chat history
        with chat_container:
            for message in st.session_state.chat_messages:
                if message["role"] == "user":
                    st.chat_message("user").markdown(message["content"])
                else:
                    st.chat_message("assistant", avatar="ü§ñ").markdown(message["content"])
        
        # Chat input
        user_input = st.chat_input("Ask a question about your data health...")
        
        if user_input:
            # Add user message to history
            st.session_state.chat_messages.append({"role": "user", "content": user_input})
            
            # Display user message
            with chat_container:
                st.chat_message("user").markdown(user_input)
            
            # Get AI response
            with st.spinner("ü§ñ Thinking..."):
                ai_response = ask_ai(user_input)
            
            # Add AI response to history
            st.session_state.chat_messages.append({"role": "assistant", "content": ai_response})
            
            # Display AI response
            with chat_container:
                st.chat_message("assistant", avatar="ü§ñ").markdown(ai_response)
            
            st.rerun()
        
        # Clear chat button
        if st.session_state.chat_messages:
            if st.button("üóëÔ∏è Clear Chat", type="secondary"):
                st.session_state.chat_messages = []
                st.rerun()
    
    # -------------------------------------------------------------------------
    # TAB 2: AI-Generated Health Summary
    # -------------------------------------------------------------------------
    with tab2:
        render_section_header("üìã", "AI-Generated Health Report")
        st.markdown("Get a comprehensive executive summary of your entire data platform health with one click.")
        
        # Initialize edit mode state
        if "edit_mode" not in st.session_state:
            st.session_state.edit_mode = False
        if "edited_summary" not in st.session_state:
            st.session_state.edited_summary = ""
        
        col1, col2 = st.columns([1, 3])
        with col1:
            generate_btn = st.button("üöÄ Generate Summary", type="primary", use_container_width=True)
        with col2:
            st.caption("This will analyze all monitoring data and create an executive summary.")
        
        if generate_btn:
            with st.spinner("ü§ñ Analyzing your data platform health..."):
                summary = generate_health_summary()
            
            # Store in session state for persistence
            st.session_state.last_health_summary = summary
            st.session_state.last_summary_time = pd.Timestamp.utcnow()
            st.session_state.edited_summary = summary
            st.session_state.edit_mode = False
        
        # Show summary if exists
        if "last_health_summary" in st.session_state:
            st.markdown("---")
            st.caption(f"*Last generated: {st.session_state.last_summary_time:%Y-%m-%d %H:%M} UTC*")
            
            # Action buttons row
            col1, col2, col3, col4 = st.columns([1, 1, 1, 2])
            with col1:
                if st.button("‚úèÔ∏è Edit", use_container_width=True):
                    st.session_state.edit_mode = True
                    st.session_state.edited_summary = st.session_state.last_health_summary
                    st.rerun()
            with col2:
                if st.button("üìß Email", use_container_width=True):
                    st.session_state.show_email_form = True
                    st.rerun()
            with col3:
                if st.button("üìã Copy", use_container_width=True):
                    st.session_state.copy_text = st.session_state.get("edited_summary", st.session_state.last_health_summary)
                    st.toast("üìã Summary copied! Use Ctrl+C in the text area below.")
            
            # Edit Mode
            if st.session_state.get("edit_mode", False):
                st.markdown("#### ‚úèÔ∏è Edit Summary")
                edited_text = st.text_area(
                    "Edit the summary before sending:",
                    value=st.session_state.edited_summary,
                    height=400,
                    key="summary_editor"
                )
                st.session_state.edited_summary = edited_text
                
                col1, col2 = st.columns(2)
                with col1:
                    if st.button("üíæ Save Changes", type="primary", use_container_width=True):
                        st.session_state.last_health_summary = edited_text
                        st.session_state.edit_mode = False
                        st.success("‚úÖ Changes saved!")
                        st.rerun()
                with col2:
                    if st.button("‚ùå Cancel", use_container_width=True):
                        st.session_state.edit_mode = False
                        st.session_state.edited_summary = st.session_state.last_health_summary
                        st.rerun()
            
            # Email Form
            elif st.session_state.get("show_email_form", False):
                st.markdown("#### üìß Send Summary via Email")
                st.info("üí° This uses Snowflake's email notification service.")
                
                with st.form("email_form"):
                    email_to = st.text_input(
                        "To (email addresses)",
                        placeholder="team@company.com, manager@company.com",
                        help="Comma-separated email addresses"
                    )
                    email_subject = st.text_input(
                        "Subject",
                        value=f"üìä Data Health Summary - {pd.Timestamp.utcnow():%Y-%m-%d}"
                    )
                    email_body = st.text_area(
                        "Message Body",
                        value=st.session_state.get("edited_summary", st.session_state.last_health_summary),
                        height=300
                    )
                    
                    col1, col2 = st.columns(2)
                    with col1:
                        send_btn = st.form_submit_button("üì§ Send Email", type="primary", use_container_width=True)
                    with col2:
                        cancel_btn = st.form_submit_button("‚ùå Cancel", use_container_width=True)
                    
                    if send_btn:
                        if not email_to:
                            st.error("Please enter at least one email address.")
                        else:
                            try:
                                # Escape single quotes for SQL
                                safe_to = email_to.replace("'", "''")
                                safe_subject = email_subject.replace("'", "''")
                                safe_body = email_body.replace("'", "''")
                                
                                # Try to send email using Snowflake's notification system
                                # Method 1: Using SYSTEM$SEND_EMAIL (requires email integration)
                                session.sql(f"""
                                    CALL SYSTEM$SEND_EMAIL(
                                        'DATA_OBSERVABILITY_EMAIL',
                                        '{safe_to}',
                                        '{safe_subject}',
                                        '{safe_body}'
                                    )
                                """).collect()
                                
                                st.success(f"‚úÖ Email sent successfully to {email_to}!")
                                st.session_state.show_email_form = False
                                
                            except Exception as e:
                                error_msg = str(e)
                                if "does not exist" in error_msg.lower() or "invalid" in error_msg.lower():
                                    st.error(f"""
                                    ‚ùå **Email Integration Not Configured**
                                    
                                    To enable email sending, your Snowflake admin needs to set up an email integration:
                                    
                                    ```sql
                                    -- 1. Create notification integration
                                    CREATE OR REPLACE NOTIFICATION INTEGRATION DATA_OBSERVABILITY_EMAIL
                                        TYPE = EMAIL
                                        ENABLED = TRUE
                                        ALLOWED_RECIPIENTS = ('*@yourcompany.com');
                                    
                                    -- 2. Grant usage to your role
                                    GRANT USAGE ON INTEGRATION DATA_OBSERVABILITY_EMAIL TO ROLE your_role;
                                    ```
                                    
                                    Error: {error_msg}
                                    """)
                                else:
                                    st.error(f"‚ùå Error sending email: {error_msg}")
                    
                    if cancel_btn:
                        st.session_state.show_email_form = False
                        st.rerun()
            
            # Display Mode (not editing, not emailing)
            else:
                st.markdown(st.session_state.get("edited_summary", st.session_state.last_health_summary))
        
        # Quick stats preview
        st.markdown("---")
        render_section_header("üìä", "Current Health Snapshot")
        st.caption("Live data from your monitoring tables ‚Äî this is what the AI will analyze:")
        
        col1, col2, col3 = st.columns(3)
        
        # Pipe stats - ONLY configured pipes
        with col1:
            st.markdown("**üîß Pipes**")
            try:
                pipe_stats = run_query(f"""
                    SELECT 
                        COUNT(*) AS total,
                        SUM(CASE WHEN m.YESTERDAY_FILES > 0 THEN 1 ELSE 0 END) AS active,
                        SUM(CASE WHEN m.YESTERDAY_ERRORS > 0 THEN 1 ELSE 0 END) AS errors
                    FROM {PIPE_HEALTH_METRICS_FQN} m
                    INNER JOIN {CONFIG_TABLE_FQN} c 
                        ON m.PIPE_NAME = c.DATABASE_NAME || '.' || c.SCHEMA_NAME || '.' || c.PIPE_NAME
                    WHERE c.IS_MONITORED = TRUE
                """)
                if not pipe_stats.empty:
                    r = pipe_stats.iloc[0]
                    st.metric("Total", r['TOTAL'])
                    st.metric("Active", r['ACTIVE'], delta=None)
                    st.metric("With Errors", r['ERRORS'], delta=None, delta_color="inverse")
                else:
                    st.info("No pipe data")
            except:
                st.info("Pipe data not available")
        
        # Freshness stats
        with col2:
            st.markdown("**üìã Tables**")
            try:
                fresh_stats = run_query(f"""
                    SELECT 
                        COUNT(*) AS total,
                        SUM(CASE WHEN HOURS_SINCE_WRITE < 24 THEN 1 ELSE 0 END) AS fresh,
                        SUM(CASE WHEN HOURS_SINCE_WRITE >= 48 OR HOURS_SINCE_WRITE IS NULL THEN 1 ELSE 0 END) AS stale
                    FROM {TABLE_METRICS_TABLE_FQN}
                """)
                if not fresh_stats.empty:
                    r = fresh_stats.iloc[0]
                    st.metric("Total", r['TOTAL'])
                    st.metric("Fresh (<24h)", r['FRESH'])
                    st.metric("Stale (>48h)", r['STALE'], delta=None, delta_color="inverse")
                else:
                    st.info("No table data")
            except:
                st.info("Table data not available")
        
        # KPI stats
        with col3:
            st.markdown("**üìà KPIs**")
            try:
                kpi_stats = run_query(f"""
                    SELECT 
                        COUNT(*) AS total,
                        SUM(CASE WHEN STATUS = 'OK' THEN 1 ELSE 0 END) AS healthy,
                        SUM(CASE WHEN IS_ANOMALY = TRUE THEN 1 ELSE 0 END) AS anomalies
                    FROM {KPI_SUMMARY_FQN}
                """)
                if not kpi_stats.empty:
                    r = kpi_stats.iloc[0]
                    st.metric("Total", r['TOTAL'])
                    st.metric("Healthy", r['HEALTHY'])
                    st.metric("Anomalies", r['ANOMALIES'], delta=None, delta_color="inverse")
                else:
                    st.info("No KPI data")
            except:
                st.info("KPI data not available")

# =============================================================================
# TASK MANAGEMENT PAGE - View and manage Snowflake scheduled tasks
# =============================================================================
if page_choice == "‚è∞ Tasks":
    render_page_header(
        "‚è∞",
        "Monitoring Setup",
        "Set up automated monitoring for your data"
    )
    
    # Get current database and schema
    current_db = session.get_current_database()
    current_schema = session.get_current_schema()
    
    # Initialize session state
    if "admin_section" not in st.session_state:
        st.session_state.admin_section = "dashboard"
    if "wizard_step" not in st.session_state:
        st.session_state.wizard_step = 1
    if "wizard_data" not in st.session_state:
        st.session_state.wizard_data = {}
    
    # Sub-navigation
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        if st.button("üìä Dashboard", key="admin_dashboard", use_container_width=True,
                     type="primary" if st.session_state.admin_section == "dashboard" else "secondary"):
            st.session_state.admin_section = "dashboard"; st.rerun()
    with col2:
        if st.button("üßô‚Äç‚ôÇÔ∏è Setup Wizard", key="admin_wizard", use_container_width=True,
                     type="primary" if st.session_state.admin_section == "wizard" else "secondary"):
            st.session_state.admin_section = "wizard"
            st.session_state.wizard_step = 1
            st.session_state.wizard_data = {}
            st.rerun()
    with col3:
        if st.button("üîî Notifications", key="admin_slack", use_container_width=True,
                     type="primary" if st.session_state.admin_section == "slack" else "secondary"):
            st.session_state.admin_section = "slack"; st.rerun()
    with col4:
        if st.button("‚öôÔ∏è Advanced", key="admin_advanced", use_container_width=True,
                     type="primary" if st.session_state.admin_section == "advanced" else "secondary"):
            st.session_state.admin_section = "advanced"; st.rerun()
    
    st.markdown("---")
    
    # ========== DASHBOARD ==========
    if st.session_state.admin_section == "dashboard":
        st.markdown("### üìä Monitoring Status")
        st.caption("Overview of your automated monitoring jobs")
        
        # Query existing tasks
        tasks_df = pd.DataFrame()
        try:
            result = session.sql(f"SHOW TASKS IN SCHEMA {current_db}.{current_schema}").collect()
            if result:
                tasks_df = pd.DataFrame([r.as_dict() for r in result])
                tasks_df.columns = [c.upper() for c in tasks_df.columns]
        except:
            pass
        
        if tasks_df.empty:
            st.info("üöÄ **No monitoring jobs set up yet.** Click **Setup Wizard** to get started!")
            
            # Check for orphaned schema configs even when no jobs exist
            try:
                schema_configs = get_schema_threshold_config()
                configured_schemas = len(schema_configs) if not schema_configs.empty else 0
            except:
                configured_schemas = 0
            
            if configured_schemas > 0:
                st.warning(f"""
                    ‚ö†Ô∏è **Orphaned Configuration Detected**
                    
                    You have **{configured_schemas} schema(s)** configured for monitoring, but no monitoring jobs.
                    These schemas won't be monitored until you create a job.
                """)
                
                orphan_col1, orphan_col2 = st.columns(2)
                with orphan_col1:
                    if st.button("üßô‚Äç‚ôÇÔ∏è Create Monitoring Job", key="create_job_orphan_empty"):
                        st.session_state.admin_section = "wizard"
                        st.session_state.wizard_step = 1
                        st.session_state.wizard_data = {"type": "freshness"}
                        st.rerun()
                with orphan_col2:
                    if st.button("üóëÔ∏è Remove Schema Configs", key="remove_orphan_configs_empty"):
                        st.session_state.confirm_remove_orphan_empty = True
                
                if st.session_state.get("confirm_remove_orphan_empty", False):
                    st.warning("‚ö†Ô∏è This will remove all schema and table monitoring configurations. Are you sure?")
                    cc1, cc2 = st.columns(2)
                    with cc1:
                        if st.button("Yes, remove all", key="yes_remove_orphan_empty", type="primary"):
                            try:
                                run_ddl(f"DELETE FROM {SCHEMA_THRESHOLD_CONFIG_FQN}")
                                run_ddl(f"DELETE FROM {TABLE_MONITOR_CONFIG_FQN}")
                                st.success("‚úÖ All configurations removed!")
                                del st.session_state.confirm_remove_orphan_empty
                                st.cache_data.clear()
                                st.rerun()
                            except Exception as e:
                                st.error(f"Error: {str(e)[:50]}")
                    with cc2:
                        if st.button("Cancel", key="no_remove_orphan_empty"):
                            del st.session_state.confirm_remove_orphan_empty
                            st.rerun()
        else:
            # Summary
            total = len(tasks_df)
            running = (tasks_df["STATE"].str.lower() == "started").sum() if "STATE" in tasks_df.columns else 0
            paused = total - running
            
            m1, m2, m3 = st.columns(3)
            m1.metric("Total Jobs", total)
            m2.metric("üü¢ Active", running)
            m3.metric("‚è∏Ô∏è Paused", paused)
            
            # Check for orphaned schema configs (schemas configured but no active Data Freshness job)
            has_freshness_job = any("FRESHNESS" in str(t.get("NAME", "")).upper() and 
                                    str(t.get("STATE", "")).lower() == "started" 
                                    for _, t in tasks_df.iterrows())
            
            try:
                schema_configs = get_schema_threshold_config()
                configured_schemas = len(schema_configs) if not schema_configs.empty else 0
            except:
                configured_schemas = 0
            
            if configured_schemas > 0 and not has_freshness_job:
                st.warning(f"""
                    ‚ö†Ô∏è **Orphaned Configuration Detected**
                    
                    You have **{configured_schemas} schema(s)** configured for monitoring, but no active Data Freshness job.
                    These schemas won't be monitored until you create a job.
                """)
                
                orphan_col1, orphan_col2 = st.columns(2)
                with orphan_col1:
                    if st.button("üßô‚Äç‚ôÇÔ∏è Create Monitoring Job", key="create_job_orphan"):
                        st.session_state.admin_section = "wizard"
                        st.session_state.wizard_step = 1
                        st.session_state.wizard_data = {"type": "freshness"}
                        st.rerun()
                with orphan_col2:
                    if st.button("üóëÔ∏è Remove Schema Configs", key="remove_orphan_configs"):
                        st.session_state.confirm_remove_orphan = True
                
                if st.session_state.get("confirm_remove_orphan", False):
                    st.warning("‚ö†Ô∏è This will remove all schema and table monitoring configurations. Are you sure?")
                    cc1, cc2 = st.columns(2)
                    with cc1:
                        if st.button("Yes, remove all", key="yes_remove_orphan", type="primary"):
                            try:
                                run_ddl(f"DELETE FROM {SCHEMA_THRESHOLD_CONFIG_FQN}")
                                run_ddl(f"DELETE FROM {TABLE_MONITOR_CONFIG_FQN}")
                                st.success("‚úÖ All configurations removed!")
                                del st.session_state.confirm_remove_orphan
                                st.cache_data.clear()
                                st.rerun()
                            except Exception as e:
                                st.error(f"Error: {str(e)[:50]}")
                    with cc2:
                        if st.button("Cancel", key="no_remove_orphan"):
                            del st.session_state.confirm_remove_orphan
                            st.rerun()
            
            st.markdown("---")
            st.markdown("### üìã Your Monitoring Jobs")
            
            # Display tasks in a user-friendly way
            for idx, task in tasks_df.iterrows():
                task_name = str(task.get("NAME", f"Job {idx}"))
                state = str(task.get("STATE", "unknown")).lower()
                schedule = str(task.get("SCHEDULE", ""))
                definition = str(task.get("DEFINITION", ""))
                
                # Parse task name to determine type
                if "FRESHNESS" in task_name.upper():
                    job_type = "Data Freshness"
                    job_icon = "üìã"
                elif "KPI" in task_name.upper():
                    job_type = "KPI Monitoring"
                    job_icon = "üìà"
                elif "PIPE" in task_name.upper():
                    job_type = "Pipeline Health"
                    job_icon = "üîß"
                else:
                    job_type = "Custom Job"
                    job_icon = "‚öôÔ∏è"
                
                # Extract database/schemas from definition for Data Freshness jobs
                monitoring_target = ""
                if "FRESHNESS" in task_name.upper() and definition:
                    try:
                        # Parse the JSON config from the CALL statement
                        # Format: CALL ...REFRESH_DATA_FRESHNESS_TABLES('{"DB": ["SCHEMA1", "SCHEMA2"]}', ...)
                        import re
                        json_match = re.search(r"'(\{[^}]+\})'", definition)
                        if json_match:
                            config_str = json_match.group(1)
                            config = json.loads(config_str.replace("'", '"'))
                            targets = []
                            for db, schemas in config.items():
                                if isinstance(schemas, list):
                                    schema_str = ", ".join(schemas[:3])  # Show first 3
                                    if len(schemas) > 3:
                                        schema_str += f" (+{len(schemas)-3} more)"
                                    targets.append(f"{db}: {schema_str}")
                                else:
                                    targets.append(f"{db}")
                            monitoring_target = " | ".join(targets)
                    except:
                        monitoring_target = ""
                
                # Parse schedule to friendly text
                if "CRON" in schedule.upper():
                    # Extract time from CRON
                    try:
                        parts = schedule.replace("USING CRON ", "").split()
                        minute, hour = parts[0], parts[1]
                        friendly_schedule = f"Daily at {hour}:{minute.zfill(2)}"
                    except:
                        friendly_schedule = schedule
                elif "MINUTE" in schedule.upper():
                    try:
                        mins = int(schedule.split()[0])
                        if mins >= 60:
                            friendly_schedule = f"Every {mins // 60} hour(s)"
                        else:
                            friendly_schedule = f"Every {mins} minutes"
                    except:
                        friendly_schedule = schedule
                else:
                    friendly_schedule = schedule
                
                state_icon = "üü¢" if state == "started" else "‚è∏Ô∏è"
                state_text = "Active" if state == "started" else "Paused"
                
                # Card-like display
                with st.container():
                    card_col1, card_col2, card_col3, card_col4 = st.columns([3, 2, 2, 2])
                    
                    with card_col1:
                        st.markdown(f"**{job_icon} {job_type}**")
                        if monitoring_target:
                            st.caption(f"üìÅ {monitoring_target}")
                        st.caption(f"‚è∞ {friendly_schedule}")
                    
                    with card_col2:
                        st.markdown(f"{state_icon} **{state_text}**")
                    
                    with card_col3:
                        if state == "started":
                            if st.button("‚è∏Ô∏è Pause", key=f"dash_pause_{idx}", use_container_width=True):
                                try:
                                    run_ddl(f"ALTER TASK {current_db}.{current_schema}.{task_name} SUSPEND")
                                    st.success("Paused!")
                                    st.cache_data.clear()
                                    st.rerun()
                                except Exception as e:
                                    st.error(f"Error: {str(e)[:50]}")
                        else:
                            if st.button("‚ñ∂Ô∏è Start", key=f"dash_start_{idx}", use_container_width=True):
                                try:
                                    run_ddl(f"ALTER TASK {current_db}.{current_schema}.{task_name} RESUME")
                                    st.success("Started!")
                                    st.cache_data.clear()
                                    st.rerun()
                                except Exception as e:
                                    st.error(f"Error: {str(e)[:50]}")
                    
                    with card_col4:
                        if st.button("üóëÔ∏è Delete", key=f"dash_del_{idx}", use_container_width=True):
                            st.session_state[f"confirm_delete_{idx}"] = True
                    
                    # Confirm delete
                    if st.session_state.get(f"confirm_delete_{idx}", False):
                        is_freshness_job = "FRESHNESS" in task_name.upper()
                        
                        st.warning(f"‚ö†Ô∏è Are you sure you want to delete this job?")
                        
                        # For Data Freshness jobs, offer to also delete schema configs
                        also_delete_configs = False
                        if is_freshness_job:
                            also_delete_configs = st.checkbox(
                                "üóëÔ∏è Also remove schema configurations",
                                value=False,
                                key=f"also_del_configs_{idx}",
                                help="If checked, the schemas configured for monitoring will also be removed from SCHEMA_THRESHOLD_CONFIG"
                            )
                        
                        dc1, dc2 = st.columns(2)
                        with dc1:
                            if st.button("Yes, delete", key=f"yes_del_{idx}", type="primary"):
                                try:
                                    # Delete the task
                                    run_ddl(f"DROP TASK IF EXISTS {current_db}.{current_schema}.{task_name}")
                                    
                                    # If checkbox was checked, also delete schema configs
                                    if is_freshness_job and also_delete_configs:
                                        try:
                                            run_ddl(f"DELETE FROM {SCHEMA_THRESHOLD_CONFIG_FQN}")
                                            run_ddl(f"DELETE FROM {TABLE_MONITOR_CONFIG_FQN}")
                                            st.success("Deleted job and all monitoring configurations!")
                                        except Exception as cfg_err:
                                            st.warning(f"Job deleted, but could not remove configs: {str(cfg_err)[:50]}")
                                    else:
                                        st.success("Deleted!")
                                    
                                    del st.session_state[f"confirm_delete_{idx}"]
                                    st.cache_data.clear()
                                    st.rerun()
                                except Exception as e:
                                    st.error(f"Error: {str(e)[:50]}")
                        with dc2:
                            if st.button("Cancel", key=f"no_del_{idx}"):
                                del st.session_state[f"confirm_delete_{idx}"]
                                st.rerun()
                    
                    st.markdown("---")
    
    # ========== NOTIFICATION CONFIGURATION ==========
    elif st.session_state.admin_section == "slack":
        st.markdown("### üîî Notification Settings")
        st.caption("Manage webhook notification integrations and configure alert channels.")
        
        # Get available notification integrations
        available_integrations = get_notification_integrations()
        
        # Create tabs: Integrations Management + Alert Channels
        notif_main_tab1, notif_main_tab2 = st.tabs([
            "‚öôÔ∏è Manage Integrations",
            "üì¢ Alert Channels"
        ])
        
        # -----------------------------------------------------------------
        # TAB: MANAGE INTEGRATIONS
        # -----------------------------------------------------------------
        with notif_main_tab1:
            st.markdown("#### Webhook Notification Integrations")
            st.caption("View and manage your Snowflake notification integrations (webhook type only).")
            
            # Get detailed integration info
            integrations_df = get_all_notification_integrations_details()
            
            if integrations_df.empty:
                st.warning("‚ö†Ô∏è No webhook notification integrations found.")
                st.markdown("""
                **To create a webhook notification integration in Snowflake:**
                
                1. **Create a secret for the webhook URL:**
                ```sql
                CREATE SECRET my_slack_secret
                    TYPE = GENERIC_STRING
                    SECRET_STRING = 'https://hooks.slack.com/services/XXX/YYY/ZZZ';
                ```
                
                2. **Create the notification integration:**
                ```sql
                CREATE NOTIFICATION INTEGRATION my_slack_integration
                    TYPE = WEBHOOK
                    ENABLED = TRUE
                    WEBHOOK_URL = 'https://hooks.slack.com/services/XXX/YYY/ZZZ'
                    WEBHOOK_SECRET = snowflake.secrets.my_slack_secret
                    WEBHOOK_BODY_TEMPLATE = '<SNOWFLAKE_WEBHOOK_MESSAGE>'
                    WEBHOOK_HEADERS = ('Content-Type'='application/json');
                ```
                
                3. **Grant usage to the app:**
                ```sql
                GRANT USAGE ON INTEGRATION my_slack_integration TO APPLICATION <your_app>;
                ```
                """)
            else:
                st.success(f"‚úÖ Found **{len(integrations_df)}** webhook integration(s)")
                
                # Display integrations table
                for idx, row in integrations_df.iterrows():
                    int_name = row.get("name", "Unknown")
                    int_enabled = row.get("enabled", False)
                    int_comment = row.get("comment", "") or ""
                    
                    status_icon = "‚úÖ" if int_enabled else "‚ùå"
                    
                    with st.expander(f"{status_icon} **{int_name}**", expanded=False):
                        st.markdown(f"""
                        | Property | Value |
                        |----------|-------|
                        | **Name** | `{int_name}` |
                        | **Type** | WEBHOOK |
                        | **Enabled** | {status_icon} {'Yes' if int_enabled else 'No'} |
                        | **Comment** | {int_comment if int_comment else '‚Äî'} |
                        """)
                        
                        st.caption("üí° Use this integration name when configuring alert channels.")
                
                st.markdown("---")
                st.markdown("##### ‚ûï Create New Integration")
                st.caption("Run these SQL commands in a Snowflake worksheet to create a new webhook integration.")
                
                with st.expander("üìù Show SQL Template", expanded=False):
                    st.code("""
-- 1. Create a secret for your webhook URL (replace with your actual URL)
CREATE OR REPLACE SECRET my_new_webhook_secret
    TYPE = GENERIC_STRING
    SECRET_STRING = 'https://hooks.slack.com/services/YOUR/WEBHOOK/URL';

-- 2. Create the notification integration
CREATE OR REPLACE NOTIFICATION INTEGRATION my_new_webhook_integration
    TYPE = WEBHOOK
    ENABLED = TRUE
    WEBHOOK_URL = 'https://hooks.slack.com/services/YOUR/WEBHOOK/URL'
    WEBHOOK_SECRET = snowflake.secrets.my_new_webhook_secret
    WEBHOOK_BODY_TEMPLATE = '<SNOWFLAKE_WEBHOOK_MESSAGE>'
    WEBHOOK_HEADERS = ('Content-Type'='application/json');

-- 3. Grant usage to this app (if running in Snowflake Native App)
-- GRANT USAGE ON INTEGRATION my_new_webhook_integration TO APPLICATION <app_name>;
                    """, language="sql")
                
                if st.button("üîÑ Refresh Integrations List", key="refresh_integrations"):
                    st.cache_data.clear()
                    st.rerun()
        
        # -----------------------------------------------------------------
        # TAB: ALERT CHANNELS
        # -----------------------------------------------------------------
        with notif_main_tab2:
            if not available_integrations:
                st.warning("‚ö†Ô∏è No webhook integrations available. Create integrations first in the 'Manage Integrations' tab.")
            else:
                st.success(f"‚úÖ **{len(available_integrations)}** integration(s) available for alerts")
                
                # Create tabs for each monitoring type
                slack_tab1, slack_tab2, slack_tab3 = st.tabs([
                    "üìã Data Freshness", 
                    "üîß Pipeline Health",
                    "üìà KPI Monitoring"
                ])
                
                # -----------------------------------------------------------------
                # TAB 1: DATA FRESHNESS
                # -----------------------------------------------------------------
                with slack_tab1:
                    st.markdown("#### Configure notification channels per schema")
                    st.caption("Each schema can have its own critical/warning channels for Data Freshness alerts.")
                
                try:
                    existing_schema_config = get_schema_threshold_config()
                    if existing_schema_config.empty:
                        st.info("No schemas configured. Add schemas via the Setup Wizard first.")
                    else:
                        notif_schemas = existing_schema_config[existing_schema_config["IS_MONITORED"] == True].copy()
                        
                        if notif_schemas.empty:
                            st.info("No active schemas. Enable monitoring for schemas first.")
                        else:
                            integration_options = ["(default)"] + available_integrations
                            
                            for _, schema_row in notif_schemas.iterrows():
                                db_name = schema_row["DATABASE_NAME"]
                                schema_name = schema_row["SCHEMA_NAME"]
                                schema_key = f"{db_name}.{schema_name}"
                                
                                current_critical = schema_row.get("CRITICAL_INTEGRATION") or ""
                                current_warning = schema_row.get("WARNING_INTEGRATION") or ""
                                
                                # Ensure current values are in options list
                                local_options = integration_options.copy()
                                if current_critical and current_critical not in local_options:
                                    local_options.append(current_critical)
                                if current_warning and current_warning not in local_options:
                                    local_options.append(current_warning)
                                
                                with st.expander(f"üìÅ {schema_key}", expanded=False):
                                    notif_col1, notif_col2 = st.columns(2)
                                    
                                    with notif_col1:
                                        critical_idx = local_options.index(current_critical) if current_critical in local_options else 0
                                        new_critical = st.selectbox(
                                            "üö® Critical alerts",
                                            local_options,
                                            index=critical_idx,
                                            key=f"slack_notif_critical_{schema_key}",
                                            help="Channel for CRITICAL alerts (stale data)"
                                        )
                                    
                                    with notif_col2:
                                        warning_idx = local_options.index(current_warning) if current_warning in local_options else 0
                                        new_warning = st.selectbox(
                                            "‚ö†Ô∏è Warning alerts",
                                            local_options,
                                            index=warning_idx,
                                            key=f"slack_notif_warning_{schema_key}",
                                            help="Channel for WARNING alerts"
                                        )
                                    
                                    if st.button("üíæ Save", key=f"slack_save_notif_{schema_key}", use_container_width=True):
                                        new_critical_val = new_critical if new_critical != "(default)" else None
                                        new_warning_val = new_warning if new_warning != "(default)" else None
                                        
                                        upsert_schema_threshold(
                                            database_name=db_name,
                                            schema_name=schema_name,
                                            warn_threshold_minutes=int(schema_row.get("WARN_THRESHOLD_MINUTES", 1440)),
                                            alert_threshold_minutes=int(schema_row.get("ALERT_THRESHOLD_MINUTES", 2880)),
                                            is_monitored=True,
                                            critical_integration=new_critical_val,
                                            warning_integration=new_warning_val
                                        )
                                        st.success(f"‚úÖ Saved channels for {schema_key}")
                                        st.cache_data.clear()
                                        time.sleep(0.5)
                                        st.rerun()
                except Exception as e:
                    st.warning(f"Could not load schema config: {str(e)[:100]}")
            
            # -----------------------------------------------------------------
            # TAB 2: PIPELINE HEALTH
            # -----------------------------------------------------------------
            with slack_tab2:
                st.markdown("#### Configure notification channels for all pipelines")
                st.caption("These channels apply to all monitored Snowpipes.")
                
                # Get current Pipeline Health integration config
                pipe_health_config = get_alert_integration_config("PIPE_HEALTH")
                current_pipe_critical = pipe_health_config.get("critical", "")
                current_pipe_warning = pipe_health_config.get("warning", "")
                
                pipe_integration_options = ["(default)"] + available_integrations
                
                # Ensure current values are in options
                if current_pipe_critical and current_pipe_critical not in pipe_integration_options:
                    pipe_integration_options.append(current_pipe_critical)
                if current_pipe_warning and current_pipe_warning not in pipe_integration_options:
                    pipe_integration_options.append(current_pipe_warning)
                
                st.markdown("") # spacing
                pipe_col1, pipe_col2 = st.columns(2)
                
                with pipe_col1:
                    pipe_critical_idx = pipe_integration_options.index(current_pipe_critical) if current_pipe_critical in pipe_integration_options else 0
                    new_pipe_critical = st.selectbox(
                        "üö® Critical alerts",
                        pipe_integration_options,
                        index=pipe_critical_idx,
                        key="slack_pipe_health_critical",
                        help="NO_DATA, STALE_DATA, MISSING_TODAY"
                    )
                
                with pipe_col2:
                    pipe_warning_idx = pipe_integration_options.index(current_pipe_warning) if current_pipe_warning in pipe_integration_options else 0
                    new_pipe_warning = st.selectbox(
                        "‚ö†Ô∏è Warning alerts",
                        pipe_integration_options,
                        index=pipe_warning_idx,
                        key="slack_pipe_health_warning",
                        help="ERRORS, LOW_FILES"
                    )
                
                st.markdown("") # spacing
                if st.button("üíæ Save Pipeline Notification Channels", key="slack_save_pipe_health", use_container_width=True):
                    new_pipe_critical_val = new_pipe_critical if new_pipe_critical != "(default)" else None
                    new_pipe_warning_val = new_pipe_warning if new_pipe_warning != "(default)" else None
                    save_alert_integration_config("PIPE_HEALTH", new_pipe_critical_val, new_pipe_warning_val)
                    st.success("‚úÖ Pipeline Health channels saved!")
                    st.cache_data.clear()
                    time.sleep(0.5)
                    st.rerun()
                
                # Show current config
                if current_pipe_critical or current_pipe_warning:
                    st.markdown("---")
                    st.caption("**Current configuration:**")
                    cfg_text = []
                    if current_pipe_critical:
                        cfg_text.append(f"üö® Critical ‚Üí `{current_pipe_critical}`")
                    if current_pipe_warning:
                        cfg_text.append(f"‚ö†Ô∏è Warning ‚Üí `{current_pipe_warning}`")
                    st.markdown(" | ".join(cfg_text) if cfg_text else "Using defaults")
            
            # -----------------------------------------------------------------
            # TAB 3: KPI MONITORING
            # -----------------------------------------------------------------
            with slack_tab3:
                st.markdown("#### Configure notification channels for KPI alerts")
                st.caption("Set global defaults and/or per-KPI notification channels.")
                
                with st.expander("‚ÑπÔ∏è What triggers Critical vs Warning alerts?", expanded=False):
                    st.markdown("""
                    | Level | Trigger Condition |
                    |-------|-------------------|
                    | üö® **CRITICAL** | Value deviates **‚â•50%** from baseline **OR** exceeds **3 standard deviations** |
                    | ‚ö†Ô∏è **WARNING** | Value deviates **25-49%** from baseline **OR** exceeds **2 standard deviations** |
                    
                    *Baseline = average of the last 30 days (minimum 7 days of data required)*
                    """)
                
                # ---- GLOBAL DEFAULT SECTION ----
                st.markdown("##### üåê Global Defaults")
                st.caption("Channels used when a KPI doesn't have its own integration configured.")
                
                kpi_global_config = get_alert_integration_config("KPI")
                current_kpi_critical = kpi_global_config.get("critical", "")
                current_kpi_warning = kpi_global_config.get("warning", "")
                
                kpi_global_options = ["(none)"] + available_integrations
                
                if current_kpi_critical and current_kpi_critical not in kpi_global_options:
                    kpi_global_options.append(current_kpi_critical)
                if current_kpi_warning and current_kpi_warning not in kpi_global_options:
                    kpi_global_options.append(current_kpi_warning)
                
                kpi_global_col1, kpi_global_col2 = st.columns(2)
                
                with kpi_global_col1:
                    kpi_critical_idx = kpi_global_options.index(current_kpi_critical) if current_kpi_critical in kpi_global_options else 0
                    new_kpi_global_critical = st.selectbox(
                        "üö® Critical (global)",
                        kpi_global_options,
                        index=kpi_critical_idx,
                        key="slack_kpi_global_critical",
                        help="Default channel for critical anomalies (>50% deviation)"
                    )
                
                with kpi_global_col2:
                    kpi_warning_idx = kpi_global_options.index(current_kpi_warning) if current_kpi_warning in kpi_global_options else 0
                    new_kpi_global_warning = st.selectbox(
                        "‚ö†Ô∏è Warning (global)",
                        kpi_global_options,
                        index=kpi_warning_idx,
                        key="slack_kpi_global_warning",
                        help="Default channel for warnings (25-50% deviation)"
                    )
                
                if st.button("üíæ Save Global KPI Defaults", key="slack_save_kpi_global", use_container_width=True):
                    new_critical_val = new_kpi_global_critical if new_kpi_global_critical != "(none)" else None
                    new_warning_val = new_kpi_global_warning if new_kpi_global_warning != "(none)" else None
                    save_alert_integration_config("KPI", new_critical_val, new_warning_val)
                    st.success("‚úÖ Global KPI channels saved!")
                    st.cache_data.clear()
                    time.sleep(0.5)
                    st.rerun()
                
                # ---- PER-KPI SECTION ----
                st.markdown("---")
                st.markdown("##### üìä Per-KPI Channels")
                st.caption("Override global defaults for specific KPIs.")
                
                try:
                    kpi_list = run_query(f"""
                        SELECT KPI_NAME, DISPLAY_NAME, IS_MONITORED, ALERT_ON_ANOMALY,
                               CRITICAL_INTEGRATION, WARNING_INTEGRATION
                        FROM {KPI_CONFIG_FQN}
                        WHERE IS_ENABLED = TRUE
                        ORDER BY KPI_NAME
                    """)
                    
                    if kpi_list.empty:
                        st.info("No KPIs configured yet. Add KPIs from the KPI Config page first.")
                    else:
                        kpi_int_options = ["(use global default)"] + available_integrations
                        
                        for _, kpi_row in kpi_list.iterrows():
                            kpi_name = kpi_row["KPI_NAME"]
                            display_name = kpi_row.get("DISPLAY_NAME") or kpi_name
                            current_crit = kpi_row.get("CRITICAL_INTEGRATION") or ""
                            current_warn = kpi_row.get("WARNING_INTEGRATION") or ""
                            
                            # Ensure current values are in options
                            local_kpi_options = kpi_int_options.copy()
                            if current_crit and current_crit not in local_kpi_options:
                                local_kpi_options.append(current_crit)
                            if current_warn and current_warn not in local_kpi_options:
                                local_kpi_options.append(current_warn)
                            
                            safe_key = kpi_name.replace(" ", "_").replace(".", "_")
                            
                            with st.expander(f"üìà {display_name}", expanded=False):
                                kpi_int_col1, kpi_int_col2 = st.columns(2)
                                
                                with kpi_int_col1:
                                    crit_idx = local_kpi_options.index(current_crit) if current_crit in local_kpi_options else 0
                                    new_kpi_crit = st.selectbox(
                                        "üö® Critical alerts",
                                        local_kpi_options,
                                        index=crit_idx,
                                        key=f"slack_kpi_crit_{safe_key}",
                                        help="Override for critical anomalies"
                                    )
                                
                                with kpi_int_col2:
                                    warn_idx = local_kpi_options.index(current_warn) if current_warn in local_kpi_options else 0
                                    new_kpi_warn = st.selectbox(
                                        "‚ö†Ô∏è Warning alerts",
                                        local_kpi_options,
                                        index=warn_idx,
                                        key=f"slack_kpi_warn_{safe_key}",
                                        help="Override for warning anomalies"
                                    )
                                
                                if st.button("üíæ Save", key=f"slack_save_kpi_{safe_key}", use_container_width=True):
                                    new_crit_val = "NULL" if new_kpi_crit == "(use global default)" else f"'{new_kpi_crit}'"
                                    new_warn_val = "NULL" if new_kpi_warn == "(use global default)" else f"'{new_kpi_warn}'"
                                    
                                    safe_kpi_name = kpi_name.replace("'", "''")
                                    update_sql = f"""
                                        UPDATE {KPI_CONFIG_FQN}
                                        SET CRITICAL_INTEGRATION = {new_crit_val},
                                            WARNING_INTEGRATION = {new_warn_val},
                                            UPDATED_AT = CURRENT_TIMESTAMP()
                                        WHERE KPI_NAME = '{safe_kpi_name}'
                                    """
                                    run_ddl(update_sql)
                                    st.success(f"‚úÖ Saved channels for {display_name}")
                                    st.cache_data.clear()
                                    time.sleep(0.5)
                                    st.rerun()
                                
                                # Show current state
                                if current_crit or current_warn:
                                    current_cfg = []
                                    if current_crit:
                                        current_cfg.append(f"üö® `{current_crit}`")
                                    if current_warn:
                                        current_cfg.append(f"‚ö†Ô∏è `{current_warn}`")
                                    st.caption("Current: " + " | ".join(current_cfg))
                                else:
                                    st.caption("Using global defaults")
                except Exception as e:
                    st.warning(f"Could not load KPI config: {str(e)[:100]}")
    
    # ========== SETUP WIZARD ==========
    elif st.session_state.admin_section == "wizard":
        
        # Progress indicator - simplified to 4 steps
        steps = ["What to Monitor", "Configuration", "Schedule & Alerts", "Review & Create"]
        current_step = st.session_state.wizard_step
        
        # Progress bar
        progress_cols = st.columns(len(steps))
        for i, (col, step_name) in enumerate(zip(progress_cols, steps), 1):
            with col:
                if i < current_step:
                    st.markdown(f"‚úÖ **{step_name}**")
                elif i == current_step:
                    st.markdown(f"üîµ **{step_name}**")
                else:
                    st.markdown(f"‚ö™ {step_name}")
        
        st.markdown("---")
        
        # ===== STEP 1: What to Monitor =====
        if current_step == 1:
            st.markdown("### üéØ What would you like to monitor?")
            st.caption("Choose the type of monitoring you want to set up.")
            
            st.markdown("")
            
            # Option cards
            option_col1, option_col2, option_col3 = st.columns(3)
            
            with option_col1:
                st.markdown("""
                <div style="background: linear-gradient(135deg, #1E3A5F 0%, #2D5A87 100%); 
                            padding: 1.5rem; border-radius: 12px; text-align: center; height: 200px;">
                    <div style="font-size: 3rem; margin-bottom: 0.5rem;">üìã</div>
                    <div style="color: white; font-weight: 600; font-size: 1.1rem;">Data Freshness</div>
                    <div style="color: #93C5FD; font-size: 0.85rem; margin-top: 0.5rem;">
                        Monitor if your tables have recent data
                    </div>
                </div>
                """, unsafe_allow_html=True)
                if st.button("Select Data Freshness", key="select_freshness", use_container_width=True):
                    st.session_state.wizard_data["type"] = "freshness"
                    st.session_state.wizard_step = 2
                    st.rerun()
            
            with option_col2:
                st.markdown("""
                <div style="background: linear-gradient(135deg, #1E3A5F 0%, #2D5A87 100%); 
                            padding: 1.5rem; border-radius: 12px; text-align: center; height: 200px;">
                    <div style="font-size: 3rem; margin-bottom: 0.5rem;">üìà</div>
                    <div style="color: white; font-weight: 600; font-size: 1.1rem;">KPI Monitoring</div>
                    <div style="color: #93C5FD; font-size: 0.85rem; margin-top: 0.5rem;">
                        Track business metrics and detect anomalies
                    </div>
                </div>
                """, unsafe_allow_html=True)
                if st.button("Select KPI Monitoring", key="select_kpi", use_container_width=True):
                    st.session_state.wizard_data["type"] = "kpi"
                    st.session_state.wizard_step = 2
                    st.rerun()
            
            with option_col3:
                st.markdown("""
                <div style="background: linear-gradient(135deg, #1E3A5F 0%, #2D5A87 100%); 
                            padding: 1.5rem; border-radius: 12px; text-align: center; height: 200px;">
                    <div style="font-size: 3rem; margin-bottom: 0.5rem;">üîß</div>
                    <div style="color: white; font-weight: 600; font-size: 1.1rem;">Pipeline Health</div>
                    <div style="color: #93C5FD; font-size: 0.85rem; margin-top: 0.5rem;">
                        Monitor Snowpipe ingestion performance
                    </div>
                </div>
                """, unsafe_allow_html=True)
                if st.button("Select Pipeline Health", key="select_pipe", use_container_width=True):
                    st.session_state.wizard_data["type"] = "pipeline"
                    st.session_state.wizard_step = 2
                    st.rerun()
        
        # ===== STEP 2: Configuration =====
        elif current_step == 2:
            monitoring_type = st.session_state.wizard_data.get("type", "freshness")
            
            if monitoring_type == "freshness":
                st.markdown("### üìã Configure Data Freshness Monitoring")
                
                # Check if a Data Freshness task already exists
                existing_freshness_task = None
                try:
                    tasks = session.sql(f"SHOW TASKS IN SCHEMA {current_db}.{current_schema}").collect()
                    for t in tasks:
                        if "DATA_FRESHNESS" in t["name"].upper() or "FRESHNESS" in t["name"].upper():
                            existing_freshness_task = t["name"]
                            break
                except:
                    pass
                
                # Explanation box
                if existing_freshness_task:
                    st.info(f"""
                    ‚ÑπÔ∏è **A monitoring job already exists:** `{existing_freshness_task}`
                    
                    You can add new schemas below. The existing job schedule will not change.
                    """)
                    st.session_state.wizard_data["existing_task"] = existing_freshness_task
                else:
                    st.info("""
                    üí° **How it works:**
                    - One single task monitors **ALL configured schemas**
                    - You can add more schemas later without creating new tasks
                    - Each schema can have its own alert threshold
                    """)
                    st.session_state.wizard_data["existing_task"] = None
                
                st.markdown("---")
                st.caption("Select schemas to add to monitoring:")
                
                # Get available databases
                try:
                    db_result = session.sql("SHOW DATABASES").collect()
                    databases = [r["name"] for r in db_result if not r["name"].startswith("SNOWFLAKE")]
                except:
                    databases = [current_db]
                
                selected_db = st.selectbox("Select Database", databases, key="wiz_db")
                
                # Get schemas for selected database
                schema_error = None
                try:
                    schema_result = session.sql(f"SHOW SCHEMAS IN DATABASE {selected_db}").collect()
                    schemas = [r["name"] for r in schema_result if r["name"] not in ("INFORMATION_SCHEMA", "PUBLIC")]
                except Exception as e:
                    schemas = []
                    schema_error = str(e)
                
                # Show already monitored schemas
                try:
                    monitored = session.sql(f"""
                        SELECT DATABASE_NAME, SCHEMA_NAME 
                        FROM {current_db}.{current_schema}.SCHEMA_THRESHOLD_CONFIG 
                        WHERE IS_MONITORED = TRUE
                    """).collect()
                    monitored_schemas = set(f"{r['DATABASE_NAME']}.{r['SCHEMA_NAME']}" for r in monitored)
                    if monitored_schemas:
                        st.caption(f"üìã Already monitored: {', '.join(monitored_schemas)}")
                except:
                    monitored_schemas = set()
                
                if schemas:
                    # Filter out already monitored schemas from the selected database
                    available_schemas = [s for s in schemas if f"{selected_db}.{s}" not in monitored_schemas]
                    
                    if available_schemas:
                        selected_schemas = st.multiselect(
                            "Select Schemas to Add",
                            available_schemas,
                            default=[],
                            help="Select schemas to add to monitoring."
                        )
                    else:
                        selected_schemas = []
                        st.info(f"All schemas in {selected_db} are already monitored.")
                    
                    st.session_state.wizard_data["database"] = selected_db
                    st.session_state.wizard_data["schemas"] = selected_schemas
                    
                    # Threshold settings (only if schemas selected)
                    if selected_schemas:
                        st.markdown("---")
                        st.markdown("#### ‚è∞ Alert Thresholds")
                        st.caption("This threshold applies to the selected schemas. Customize per-table later in Data Freshness page.")
                        
                        threshold_option = st.radio(
                            "Default threshold for selected schemas",
                            ["24 hours (daily data)", "12 hours (twice daily)", "6 hours (frequent updates)", "48 hours (weekly batches)", "Custom"],
                            key="wiz_threshold"
                        )
                        
                        if threshold_option == "Custom":
                            custom_hours = st.number_input("Warn after (hours)", min_value=1, max_value=720, value=24)
                            st.session_state.wizard_data["threshold_hours"] = custom_hours
                        else:
                            hours_map = {
                                "24 hours (daily data)": 24, 
                                "12 hours (twice daily)": 12, 
                                "6 hours (frequent updates)": 6,
                                "48 hours (weekly batches)": 48
                            }
                            st.session_state.wizard_data["threshold_hours"] = hours_map.get(threshold_option, 24)
                        
                        warn_hours = st.session_state.wizard_data.get("threshold_hours", 24)
                        alert_hours = warn_hours * 2
                        st.caption(f"‚ö†Ô∏è **Warn** after {warn_hours}h without data ‚Ä¢ üö® **Alert** after {alert_hours}h")
                else:
                    st.warning(f"""
                    **No schemas found in `{selected_db}`**
                    
                    This is likely a **permissions issue**. The app's role doesn't have access to this database.
                    
                    **To fix this, run in Snowflake:**
                    ```sql
                    GRANT USAGE ON DATABASE {selected_db} TO ROLE <your_app_role>;
                    GRANT USAGE ON ALL SCHEMAS IN DATABASE {selected_db} TO ROLE <your_app_role>;
                    ```
                    
                    Or select a database you have access to (like `STITCH_DATABASE`).
                    """)
            
            elif monitoring_type == "kpi":
                st.markdown("### üìà Configure KPI Monitoring")
                st.caption("Track your business metrics automatically.")
                
                st.info("üí° KPI monitoring will track metrics you define in the KPI section. Make sure you have KPIs configured first.")
                
                lookback_days = st.slider("Historical data to analyze (days)", 7, 90, 30)
                st.session_state.wizard_data["lookback_days"] = lookback_days
            
            elif monitoring_type == "pipeline":
                st.markdown("### üîß Configure Pipeline Health Monitoring")
                st.caption("Monitor your Snowpipe ingestion performance.")
                
                st.info("üí° This will monitor all Snowpipes in your account for errors and performance issues.")
                
                history_days = st.slider("Days of history to keep", 7, 90, 30)
                st.session_state.wizard_data["history_days"] = history_days
            
            # Navigation buttons
            st.markdown("---")
            nav_col1, nav_col2, nav_col3 = st.columns([1, 2, 1])
            with nav_col1:
                if st.button("‚Üê Back", use_container_width=True):
                    st.session_state.wizard_step = 1
                    st.rerun()
            with nav_col3:
                can_proceed = True
                if monitoring_type == "freshness" and not st.session_state.wizard_data.get("schemas"):
                    can_proceed = False
                
                # If existing task for Data Freshness, skip schedule step (go directly to review)
                existing_task = st.session_state.wizard_data.get("existing_task")
                next_step = 4 if (monitoring_type == "freshness" and existing_task) else 3
                
                if st.button("Next ‚Üí", type="primary", use_container_width=True, disabled=not can_proceed):
                    st.session_state.wizard_step = next_step
                    st.rerun()
        
        # ===== STEP 3: Schedule =====
        elif current_step == 3:
            st.markdown("### ‚è∞ When should monitoring run?")
            st.caption("Choose how often to check your data.")
            
            schedule_option = st.radio(
                "Select schedule",
                [
                    "üåÖ Daily at 7:00 AM (recommended)",
                    "üåÖ Daily at 8:00 AM",
                    "üîÑ Every 6 hours",
                    "üîÑ Every hour",
                    "üïê Custom time"
                ],
                key="wiz_schedule"
            )
            
            if schedule_option == "üïê Custom time":
                time_col1, time_col2 = st.columns(2)
                with time_col1:
                    custom_hour = st.selectbox("Hour", list(range(24)), index=7)
                with time_col2:
                    custom_minute = st.selectbox("Minute", list(range(60)), index=0)
                
                st.session_state.wizard_data["schedule"] = f"USING CRON {custom_minute} {custom_hour} * * * America/New_York"
                st.session_state.wizard_data["schedule_friendly"] = f"Daily at {custom_hour}:{str(custom_minute).zfill(2)}"
            elif "7:00 AM" in schedule_option:
                st.session_state.wizard_data["schedule"] = "USING CRON 0 7 * * * America/New_York"
                st.session_state.wizard_data["schedule_friendly"] = "Daily at 7:00 AM"
            elif "8:00 AM" in schedule_option:
                st.session_state.wizard_data["schedule"] = "USING CRON 0 8 * * * America/New_York"
                st.session_state.wizard_data["schedule_friendly"] = "Daily at 8:00 AM"
            elif "6 hours" in schedule_option:
                st.session_state.wizard_data["schedule"] = "USING CRON 0 */6 * * * UTC"
                st.session_state.wizard_data["schedule_friendly"] = "Every 6 hours"
            elif "hour" in schedule_option:
                st.session_state.wizard_data["schedule"] = "USING CRON 0 * * * * UTC"
                st.session_state.wizard_data["schedule_friendly"] = "Every hour"
            
            # Timezone info
            st.caption("üìç Times are in America/New_York timezone (ET)")
            
            # Navigation buttons
            st.markdown("---")
            nav_col1, nav_col2, nav_col3 = st.columns([1, 2, 1])
            with nav_col1:
                if st.button("‚Üê Back", key="back_3", use_container_width=True):
                    st.session_state.wizard_step = 2
                    st.rerun()
            with nav_col3:
                if st.button("Next ‚Üí", key="next_3", type="primary", use_container_width=True):
                    st.session_state.wizard_step = 4
                    st.rerun()
        
        # ===== STEP 4: Review & Create =====
        elif current_step == 4:
            wizard_data = st.session_state.wizard_data
            monitoring_type = wizard_data.get("type", "freshness")
            existing_task = wizard_data.get("existing_task")
            
            # Different title based on action
            if existing_task:
                st.markdown("### ‚úÖ Review & Add Schemas")
                st.caption("Review the schemas you're adding to the existing monitoring job.")
            else:
                st.markdown("### ‚úÖ Review & Create")
                st.caption("Review your monitoring setup before creating.")
            
            # Summary card
            type_names = {"freshness": "üìã Data Freshness", "kpi": "üìà KPI Monitoring", "pipeline": "üîß Pipeline Health"}
            
            if monitoring_type == "freshness" and existing_task:
                # Simplified summary when adding to existing task
                schemas_str = ", ".join(wizard_data.get("schemas", []))
                st.markdown(f"""
                <div style="background: linear-gradient(135deg, #1E3A5F 0%, #2D5A87 100%); 
                            padding: 1.5rem; border-radius: 12px; margin-bottom: 1rem;">
                    <h4 style="color: white; margin: 0;">üìã Summary</h4>
                    <hr style="border-color: #60A5FA; margin: 0.5rem 0;">
                    <p style="color: #93C5FD; margin: 0.3rem 0;"><strong>Existing Task:</strong> {existing_task}</p>
                    <p style="color: #93C5FD; margin: 0.3rem 0;"><strong>Database:</strong> {wizard_data.get('database', current_db)}</p>
                    <p style="color: #93C5FD; margin: 0.3rem 0;"><strong>Schemas to add:</strong> {schemas_str if schemas_str else '(none)'}</p>
                    <p style="color: #93C5FD; margin: 0.3rem 0;"><strong>Alert Threshold:</strong> {wizard_data.get('threshold_hours', 24)} hours</p>
                </div>
                """, unsafe_allow_html=True)
                
                st.info("‚ÑπÔ∏è The existing task schedule will not change. Only the schema configuration will be updated.")
            else:
                # Full summary for new task creation
                st.markdown(f"""
                <div style="background: linear-gradient(135deg, #1E3A5F 0%, #2D5A87 100%); 
                            padding: 1.5rem; border-radius: 12px; margin-bottom: 1rem;">
                    <h4 style="color: white; margin: 0;">üìã Summary</h4>
                    <hr style="border-color: #60A5FA; margin: 0.5rem 0;">
                    <p style="color: #93C5FD; margin: 0.3rem 0;"><strong>Type:</strong> {type_names.get(monitoring_type, monitoring_type)}</p>
                    <p style="color: #93C5FD; margin: 0.3rem 0;"><strong>Schedule:</strong> {wizard_data.get('schedule_friendly', 'Daily at 7:00 AM')}</p>
                    <p style="color: #93C5FD; margin: 0.3rem 0;"><strong>Notifications:</strong> ‚úÖ Enabled</p>
                """, unsafe_allow_html=True)
                
                if monitoring_type == "freshness":
                    schemas_str = ", ".join(wizard_data.get("schemas", []))
                    st.markdown(f"""
                    <p style="color: #93C5FD; margin: 0.3rem 0;"><strong>Database:</strong> {wizard_data.get('database', current_db)}</p>
                    <p style="color: #93C5FD; margin: 0.3rem 0;"><strong>Schemas to add:</strong> {schemas_str if schemas_str else '(none)'}</p>
                    <p style="color: #93C5FD; margin: 0.3rem 0;"><strong>Alert Threshold:</strong> {wizard_data.get('threshold_hours', 24)} hours</p>
                    """, unsafe_allow_html=True)
                
                st.markdown("</div>", unsafe_allow_html=True)
            
            # Get warehouse (only needed for new task creation)
            if not existing_task:
                try:
                    wh_result = session.sql("SHOW WAREHOUSES").collect()
                    warehouses = [r["name"] for r in wh_result] if wh_result else ["COMPUTE_WH"]
                except:
                    warehouses = ["COMPUTE_WH"]
                
                selected_warehouse = st.selectbox("Warehouse to use", warehouses, key="wiz_warehouse")
            else:
                selected_warehouse = None  # Will not be used
            
            # Navigation buttons
            st.markdown("---")
            nav_col1, nav_col2, nav_col3 = st.columns([1, 2, 1])
            with nav_col1:
                # Go back to step 2 if we skipped step 3 (existing task)
                back_step = 2 if existing_task else 3
                if st.button("‚Üê Back", key="back_4", use_container_width=True):
                    st.session_state.wizard_step = back_step
                    st.rerun()
            with nav_col3:
                button_label = "‚ûï Add Schemas" if existing_task else "üöÄ Create Monitoring Job"
                if st.button(button_label, type="primary", use_container_width=True):
                    try:
                        # Generate task name based on type
                        timestamp = pd.Timestamp.now().strftime("%Y%m%d%H%M")
                        creation_messages = []
                        
                        # Build task SQL - uses default integrations configured in setup script
                        # SEND_*_ALERT procedures already call REFRESH internally
                        if monitoring_type == "freshness":
                            schemas = wizard_data.get("schemas", [])
                            db = wizard_data.get("database", current_db)
                            existing_task = wizard_data.get("existing_task")
                            
                            # Save schema thresholds for each selected schema
                            threshold_hours = wizard_data.get("threshold_hours", 24)
                            warn_mins = int(threshold_hours * 60)
                            alert_mins = warn_mins * 2  # Alert at 2x warn
                            
                            schemas_added = 0
                            for schema_name in schemas:
                                try:
                                    upsert_schema_threshold(
                                        database_name=db,
                                        schema_name=schema_name,
                                        warn_threshold_minutes=warn_mins,
                                        alert_threshold_minutes=alert_mins,
                                        is_monitored=True,
                                        notes=f"Created by Setup Wizard on {pd.Timestamp.now().strftime('%Y-%m-%d')}"
                                    )
                                    schemas_added += 1
                                except Exception as thresh_err:
                                    st.warning(f"Could not set threshold for {schema_name}: {str(thresh_err)[:50]}")
                            
                            if schemas_added > 0:
                                creation_messages.append(f"‚úÖ Added {schemas_added} schema(s) to monitoring")
                                
                                # Immediately refresh data so tables are visible
                                try:
                                    with st.spinner("üîÑ Refreshing data freshness metrics..."):
                                        # Build config for the selected schemas
                                        selected_db = wizard_data.get("database")
                                        selected_schemas = wizard_data.get("schemas", [])
                                        if selected_db and selected_schemas:
                                            config_dict = {selected_db: selected_schemas}
                                            config_json = json.dumps(config_dict)
                                            run_ddl(f"CALL {current_db}.{current_schema}.REFRESH_DATA_FRESHNESS_TABLES('{config_json}', 30)")
                                            creation_messages.append("‚úÖ Data refreshed - tables are now visible in Data Freshness")
                                except Exception as refresh_err:
                                    creation_messages.append(f"‚ö†Ô∏è Could not refresh data immediately: {str(refresh_err)[:50]}. Data will refresh when task runs.")
                            
                            # Only create task if none exists
                            if existing_task:
                                task_name = None  # Don't create new task
                                creation_messages.append(f"‚ÑπÔ∏è Using existing task: {existing_task}")
                            else:
                                task_name = f"ALERT_DATA_FRESHNESS_{timestamp}"
                                task_sql = f"CALL {current_db}.{current_schema}.SEND_DATA_FRESHNESS_ALERT()"
                        
                        elif monitoring_type == "kpi":
                            task_name = f"ALERT_KPI_METRICS_{timestamp}"
                            task_sql = f"CALL {current_db}.{current_schema}.SEND_KPI_ALERT()"
                        
                        elif monitoring_type == "pipeline":
                            task_name = f"ALERT_PIPE_HEALTH_{timestamp}"
                            task_sql = f"CALL {current_db}.{current_schema}.SEND_PIPE_HEALTH_ALERT()"
                        
                        schedule = wizard_data.get("schedule", "USING CRON 0 7 * * * America/New_York")
                        
                        # Create the task only if task_name is set (no existing task)
                        if task_name:
                            try:
                                run_ddl(f"""
                                    CREATE OR REPLACE TASK {current_db}.{current_schema}.{task_name}
                                        WAREHOUSE = {selected_warehouse}
                                        SCHEDULE = '{schedule}'
                                        COMMENT = 'Monitoring + Alert task - created by Wizard'
                                    AS
                                        {task_sql}
                                """)
                                run_ddl(f"ALTER TASK {current_db}.{current_schema}.{task_name} RESUME")
                                creation_messages.append(f"‚úÖ Monitoring task created: {task_name}")
                            except Exception as task_err:
                                creation_messages.append(f"‚ö†Ô∏è Could not create task: {str(task_err)[:50]}")
                        
                        # Show success messages
                        for msg in creation_messages:
                            if msg.startswith("‚úÖ"):
                                st.success(msg)
                            else:
                                st.warning(msg)
                        
                        st.balloons()
                        
                        # Reset wizard
                        st.session_state.wizard_step = 1
                        st.session_state.wizard_data = {}
                        st.session_state.admin_section = "dashboard"
                        st.cache_data.clear()
                        time.sleep(2)
                        st.rerun()
                        
                    except Exception as e:
                        st.error(f"‚ùå Error creating monitoring job: {str(e)}")
    
    # ========== ADVANCED (Original Task Management) ==========
    elif st.session_state.admin_section == "advanced":
        st.markdown("### ‚öôÔ∏è Advanced Task Management")
        st.caption("For technical users: direct task management")
        
        # Initialize task section
        if "task_section" not in st.session_state:
            st.session_state.task_section = "view"
        
        tcol1, tcol2 = st.columns(2)
        with tcol1:
            if st.button("üìã View All Tasks", key="adv_view", use_container_width=True,
                         type="primary" if st.session_state.task_section == "view" else "secondary"):
                st.session_state.task_section = "view"; st.rerun()
        with tcol2:
            if st.button("‚ûï Create Custom Task", key="adv_create", use_container_width=True,
                         type="primary" if st.session_state.task_section == "create" else "secondary"):
                st.session_state.task_section = "create"; st.rerun()
        
        st.markdown("---")
        
        # ========== VIEW TASKS (Advanced) ==========
        if st.session_state.task_section == "view":
            tasks_df = pd.DataFrame()
            try:
                result = session.sql(f"SHOW TASKS IN SCHEMA {current_db}.{current_schema}").collect()
                if result:
                    tasks_df = pd.DataFrame([r.as_dict() for r in result])
                    tasks_df.columns = [c.upper() for c in tasks_df.columns]
            except:
                pass
            
            if not tasks_df.empty:
                st.dataframe(tasks_df[["NAME", "STATE", "SCHEDULE", "WAREHOUSE"]], use_container_width=True, hide_index=True)
                
                # Task actions
                for idx, task in tasks_df.iterrows():
                    task_name = str(task["NAME"])
                    state = str(task.get("STATE", "")).lower()
                    schedule = str(task.get("SCHEDULE", ""))
                    warehouse = str(task.get("WAREHOUSE", ""))
                    definition = str(task.get("DEFINITION", ""))
                    task_key = f"{task_name}_{idx}"
                    
                    with st.expander(f"{'üü¢' if state == 'started' else '‚è∏Ô∏è'} {task_name}"):
                        st.code(definition, language="sql")
                        
                        bcol1, bcol2, bcol3, bcol4 = st.columns(4)
                        with bcol1:
                            if state == "started":
                                if st.button("‚è∏Ô∏è Suspend", key=f"adv_susp_{task_key}"):
                                    run_ddl(f"ALTER TASK {current_db}.{current_schema}.{task_name} SUSPEND")
                                    st.rerun()
                            else:
                                if st.button("‚ñ∂Ô∏è Resume", key=f"adv_res_{task_key}"):
                                    run_ddl(f"ALTER TASK {current_db}.{current_schema}.{task_name} RESUME")
                                    st.rerun()
                        with bcol2:
                            if st.button("‚ñ∂Ô∏è Run Now", key=f"adv_run_{task_key}"):
                                run_ddl(f"EXECUTE TASK {current_db}.{current_schema}.{task_name}")
                                st.success("Executed!")
                        with bcol3:
                            if st.button("‚úèÔ∏è Edit", key=f"adv_edit_{task_key}"):
                                st.session_state[f"editing_task_{task_key}"] = True
                        with bcol4:
                            if st.button("üóëÔ∏è Drop", key=f"adv_drop_{task_key}"):
                                st.session_state[f"confirm_drop_{task_key}"] = True
                        
                        # Edit form
                        if st.session_state.get(f"editing_task_{task_key}", False):
                            st.markdown("---")
                            new_schedule = st.text_input("Schedule", value=schedule, key=f"edit_sch_{task_key}")
                            new_sql = st.text_area("SQL", value=definition, key=f"edit_sql_{task_key}")
                            new_wh = st.text_input("Warehouse", value=warehouse, key=f"edit_wh_{task_key}")
                            
                            if st.button("üíæ Save", key=f"save_{task_key}", type="primary"):
                                if state == "started":
                                    run_ddl(f"ALTER TASK {current_db}.{current_schema}.{task_name} SUSPEND")
                                run_ddl(f"""
                                    CREATE OR REPLACE TASK {current_db}.{current_schema}.{task_name}
                                        WAREHOUSE = {new_wh}
                                        SCHEDULE = '{new_schedule}'
                                    AS {new_sql}
                                """)
                                if state == "started":
                                    run_ddl(f"ALTER TASK {current_db}.{current_schema}.{task_name} RESUME")
                                st.session_state[f"editing_task_{task_key}"] = False
                                st.rerun()
                        
                        # Confirm drop
                        if st.session_state.get(f"confirm_drop_{task_key}", False):
                            st.warning("‚ö†Ô∏è Confirm delete?")
                            if st.button("Yes, delete", key=f"yes_drop_{task_key}"):
                                run_ddl(f"DROP TASK IF EXISTS {current_db}.{current_schema}.{task_name}")
                                del st.session_state[f"confirm_drop_{task_key}"]
                                st.rerun()
            else:
                st.info("No tasks found.")
        
        # ========== CREATE CUSTOM TASK (Advanced) ==========
        elif st.session_state.task_section == "create":
            st.markdown("#### Create Custom Task")
            
            col1, col2 = st.columns(2)
            with col1:
                task_name = st.text_input("Task Name", placeholder="MY_CUSTOM_TASK")
            with col2:
                try:
                    wh_result = session.sql("SHOW WAREHOUSES").collect()
                    warehouses = [r["name"] for r in wh_result] if wh_result else ["COMPUTE_WH"]
                except:
                    warehouses = ["COMPUTE_WH"]
                warehouse = st.selectbox("Warehouse", warehouses)
            
            schedule = st.text_input("Schedule", value="USING CRON 0 7 * * * America/New_York")
            task_sql = st.text_area("SQL Statement", height=100, placeholder="CALL MY_PROCEDURE()")
            
            if st.button("üöÄ Create Task", type="primary"):
                if task_name and schedule and task_sql:
                    try:
                        run_ddl(f"""
                            CREATE OR REPLACE TASK {current_db}.{current_schema}.{task_name.upper()}
                                WAREHOUSE = {warehouse}
                                SCHEDULE = '{schedule}'
                            AS {task_sql}
                        """)
                        run_ddl(f"ALTER TASK {current_db}.{current_schema}.{task_name.upper()} RESUME")
                        st.success(f"‚úÖ Task {task_name.upper()} created!")
                        st.session_state.task_section = "view"
                        st.rerun()
                    except Exception as e:
                        st.error(f"Error: {str(e)}")
                else:
                    st.error("All fields required")
        
    
    st.stop()
